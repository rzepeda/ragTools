========================================
Test Results - 2025-12-15 08:41:57
========================================
Environment Configuration:
  EMBEDDING_MODEL_NAME: Xenova/all-MiniLM-L6-v2
  EMBEDDING_MODEL_PATH: models/embedding
  LM_STUDIO_BASE_URL: http://192.168.56.1:1234/v1
  LM_STUDIO_MODEL: phi-4-mini-instruct

Command: pytest tests/integration/test_config_integration.py tests/integration/services/test_embedding_integration.py tests/integration/services/test_onnx_embeddings_integration.py tests/integration/services/test_service_integration.py tests/integration/strategies/test_contextual_integration.py tests/integration/strategies/test_knowledge_graph_integration.py tests/integration/strategies/test_late_chunking_integration.py tests/integration/strategies/test_multi_query_integration.py tests/integration/test_factory_integration.py tests/integration/test_package_integration.py tests/integration/test_pipeline_integration.py
========================================

============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-9.0.1, pluggy-1.6.0 -- /mnt/MCPProyects/ragTools/venv/bin/python3
cachedir: .pytest_cache
rootdir: /mnt/MCPProyects/ragTools
configfile: pytest.ini (WARNING: ignoring pytest config in pyproject.toml!)
plugins: anyio-4.12.0, cov-7.0.0, asyncio-1.3.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 78 items

tests/integration/test_config_integration.py::test_load_validate_use_config PASSED [  1%]
tests/integration/test_config_integration.py::test_multi_environment_configuration PASSED [  2%]
tests/integration/test_config_integration.py::test_config_with_factory FAILED [  3%]
tests/integration/test_config_integration.py::test_config_with_pipeline PASSED [  5%]
tests/integration/test_config_integration.py::test_complete_real_world_scenario PASSED [  6%]
tests/integration/test_config_integration.py::test_config_export_and_reimport PASSED [  7%]
tests/integration/test_config_integration.py::test_json_configuration_integration PASSED [  8%]
tests/integration/test_config_integration.py::test_deeply_nested_configuration_access PASSED [ 10%]
tests/integration/services/test_embedding_integration.py::test_openai_full_workflow FAILED [ 11%]
tests/integration/services/test_embedding_integration.py::test_cohere_full_workflow SKIPPED [ 12%]
tests/integration/services/test_embedding_integration.py::test_local_embedding_provider FAILED [ 14%]
tests/integration/services/test_embedding_integration.py::test_large_batch_processing PASSED [ 15%]
tests/integration/services/test_embedding_integration.py::test_concurrent_embedding_requests PASSED [ 16%]
tests/integration/services/test_embedding_integration.py::test_cache_persistence_across_batches PASSED [ 17%]
tests/integration/services/test_embedding_integration.py::test_error_handling PASSED [ 19%]
tests/integration/services/test_embedding_integration.py::test_onnx_provider_compatibility FAILED [ 20%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_embed_single_document PASSED [ 21%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_embed_multiple_documents PASSED [ 23%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_semantic_similarity PASSED [ 24%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_batch_consistency PASSED [ 25%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_empty_input PASSED [ 26%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_long_text_handling PASSED [ 28%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_special_characters PASSED [ 29%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_performance_target PASSED [ 30%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_provider_metadata PASSED [ 32%]
tests/integration/services/test_onnx_embeddings_integration.py::TestONNXEmbeddingsIntegration::test_result_metadata PASSED [ 33%]
tests/integration/services/test_service_integration.py::test_rag_workflow ERROR [ 34%]
tests/integration/services/test_service_integration.py::test_embedding_database_consistency FAILED [ 35%]
tests/integration/strategies/test_contextual_integration.py::test_contextual_retrieval_complete_workflow PASSED [ 37%]
tests/integration/strategies/test_contextual_integration.py::test_cost_tracking_accuracy PASSED [ 38%]
tests/integration/strategies/test_contextual_integration.py::test_retrieval_with_different_formats PASSED [ 39%]
tests/integration/strategies/test_contextual_integration.py::test_error_recovery FAILED [ 41%]
tests/integration/strategies/test_contextual_integration.py::test_synchronous_indexing PASSED [ 42%]
tests/integration/strategies/test_contextual_integration.py::test_large_document_processing PASSED [ 43%]
tests/integration/strategies/test_knowledge_graph_integration.py::test_knowledge_graph_workflow PASSED [ 44%]
tests/integration/strategies/test_knowledge_graph_integration.py::test_hybrid_retrieval FAILED [ 46%]
tests/integration/strategies/test_knowledge_graph_integration.py::test_relationship_queries FAILED [ 47%]
tests/integration/strategies/test_knowledge_graph_integration.py::test_graph_statistics PASSED [ 48%]
tests/integration/strategies/test_late_chunking_integration.py::test_late_chunking_workflow FAILED [ 50%]
tests/integration/strategies/test_late_chunking_integration.py::test_fixed_size_chunking_integration FAILED [ 51%]
tests/integration/strategies/test_late_chunking_integration.py::test_adaptive_chunking_integration FAILED [ 52%]
tests/integration/strategies/test_late_chunking_integration.py::test_multiple_documents FAILED [ 53%]
tests/integration/strategies/test_late_chunking_integration.py::test_strategy_properties PASSED [ 55%]
tests/integration/strategies/test_late_chunking_integration.py::test_coherence_scores_computed FAILED [ 56%]
tests/integration/strategies/test_late_chunking_integration.py::test_short_document FAILED [ 57%]
tests/integration/strategies/test_late_chunking_integration.py::test_chunk_embeddings_valid FAILED [ 58%]
tests/integration/strategies/test_late_chunking_integration.py::test_embedding_quality FAILED [ 60%]
tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_multi_query_complete_workflow FAILED [ 61%]
tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_multi_query_async_workflow FAILED [ 62%]
tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_variant_diversity FAILED [ 64%]
tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_performance_requirements FAILED [ 65%]
tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_fallback_on_failure FAILED [ 66%]
tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_ranking_strategy_comparison FAILED [ 67%]
tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_deduplication_across_variants PASSED [ 69%]
tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_strategy_properties PASSED [ 70%]
tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryWithLMStudio::test_with_real_llm FAILED [ 71%]
tests/integration/test_factory_integration.py::test_register_create_use_strategy FAILED [ 73%]
tests/integration/test_factory_integration.py::test_create_multiple_different_strategies PASSED [ 74%]
tests/integration/test_factory_integration.py::test_create_multiple_instances_of_same_strategy FAILED [ 75%]
tests/integration/test_factory_integration.py::test_dependency_injection PASSED [ 76%]
tests/integration/test_factory_integration.py::test_config_file_with_yaml FAILED [ 78%]
tests/integration/test_factory_integration.py::test_config_file_with_json FAILED [ 79%]
tests/integration/test_factory_integration.py::test_factory_error_recovery FAILED [ 80%]
tests/integration/test_factory_integration.py::test_factory_state_after_failed_creation FAILED [ 82%]
tests/integration/test_factory_integration.py::test_decorator_integration PASSED [ 83%]
tests/integration/test_factory_integration.py::test_full_rag_workflow PASSED [ 84%]
tests/integration/test_factory_integration.py::test_async_retrieve_integration PASSED [ 85%]
tests/integration/test_package_integration.py::TestPackageInstallation::test_package_installable FAILED [ 87%]
tests/integration/test_package_integration.py::TestSmokeTest::test_basic_usage_smoke_test PASSED [ 88%]
tests/integration/test_package_integration.py::TestFullWorkflow::test_full_workflow_with_installed_package FAILED [ 89%]
tests/integration/test_package_integration.py::TestBuildAndDistribution::test_package_can_be_built SKIPPED [ 91%]
tests/integration/test_pipeline_integration.py::test_three_strategy_pipeline PASSED [ 92%]
tests/integration/test_pipeline_integration.py::test_parallel_faster_than_sequential PASSED [ 93%]
tests/integration/test_pipeline_integration.py::test_pipeline_continues_after_non_critical_failure FAILED [ 94%]
tests/integration/test_pipeline_integration.py::test_load_pipeline_from_yaml PASSED [ 96%]
tests/integration/test_pipeline_integration.py::test_async_sequential_execution PASSED [ 97%]
tests/integration/test_pipeline_integration.py::test_async_fallback_execution FAILED [ 98%]
tests/integration/test_pipeline_integration.py::test_parallel_execution_with_failures FAILED [100%]

==================================== ERRORS ====================================
_____________________ ERROR at setup of test_rag_workflow ______________________

    @pytest.fixture
    def llm_service():
        """Create LLM service."""
        config = LLMServiceConfig(
            provider="anthropic",
            model="claude-sonnet-4.5",
            enable_rate_limiting=False
        )
>       with patch("rag_factory.services.llm.service.AnthropicProvider") as mock_provider_cls:

tests/integration/services/test_service_integration.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7d9309757bf0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.services.llm.service' from '/mnt/MCPProyects/ragTools/rag_factory/services/llm/service.py'> does not have the attribute 'AnthropicProvider'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
=================================== FAILURES ===================================
___________________________ test_config_with_factory ___________________________

tmp_path = PosixPath('/tmp/pytest-of-admindevmac/pytest-132/test_config_with_factory0')

    @pytest.mark.integration
    def test_config_with_factory(tmp_path: Path) -> None:
        """Test configuration used with RAGFactory."""
        config_file = tmp_path / "config.yaml"
        config_file.write_text("""
    strategies:
      test_strategy:
        chunk_size: 1024
        top_k: 10
        strategy_name: test_strategy
    """)
    
        # Load config
        config = ConfigManager()
        config.load(str(config_file))
    
        # Register strategy
        RAGFactory.register_strategy(
            "test_strategy",
            TestIntegrationStrategy,
            override=True
        )
    
        # Get strategy config and create strategy
        strategy_config = config.get_strategy_config("test_strategy")
>       strategy = RAGFactory.create_strategy(
            "test_strategy",
            strategy_config.model_dump()
        )

tests/integration/test_config_integration.py:180: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = 'test_strategy'
name = {'chunk_overlap': 50, 'chunk_size': 1024, 'metadata': {}, 'strategy_name': 'test_strategy', ...}
config = None, override_deps = None

    def create_strategy(
        self,
        name: str,
        config: Optional[Dict[str, Any]] = None,
        override_deps: Optional[StrategyDependencies] = None,
    ) -> IRAGStrategy:
        """
        Create and initialize a strategy instance with dependency injection.
    
        Args:
            name: Name of the registered strategy to create
            config: Optional configuration dictionary for the strategy
            override_deps: Optional dependencies to override factory defaults
    
        Returns:
            IRAGStrategy: Initialized strategy instance
    
        Raises:
            StrategyNotFoundError: If strategy name not in registry
            ValueError: If required services are missing or config is invalid
    
        Example:
            >>> factory = RAGFactory(llm_service=my_llm)
            >>> config = {"chunk_size": 1024, "top_k": 10}
            >>> strategy = factory.create_strategy("my_strategy", config)
            >>>
            >>> # Or with override dependencies
            >>> override = StrategyDependencies(llm_service=different_llm)
            >>> strategy = factory.create_strategy("my_strategy", config, override)
        """
>       if name not in self._registry:
                       ^^^^^^^^^^^^^^
E       AttributeError: 'str' object has no attribute '_registry'

rag_factory/factory.py:202: AttributeError
__________________________ test_openai_full_workflow ___________________________

self = <Retrying object at 0x7d9309d82540 (stop=<tenacity.stop.stop_after_attempt object at 0x7d930d944e30>, wait=<tenacity.w...0x7d930ed06810>, before=<function before_nothing at 0x7d930e9c59e0>, after=<function after_nothing at 0x7d930e9c67a0>)>
fn = <function OpenAIProvider.get_embeddings at 0x7d930d3f6ac0>
args = (<rag_factory.services.embedding.providers.openai.OpenAIProvider object at 0x7d9309d82f00>, ['Hello, world!'])
kwargs = {}
retry_state = <RetryCallState 138070478184064: attempt #3; slept for 3.0; last result: failed (AuthenticationError Error code: 401 -...//platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})>
do = <tenacity.DoAttempt object at 0x7d9309b01f40>

    def __call__(
        self,
        fn: t.Callable[..., WrappedFnReturnT],
        *args: t.Any,
        **kwargs: t.Any,
    ) -> WrappedFnReturnT:
        self.begin()
    
        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
        while True:
            do = self.iter(retry_state=retry_state)
            if isinstance(do, DoAttempt):
                try:
>                   result = fn(*args, **kwargs)
                             ^^^^^^^^^^^^^^^^^^^

venv/lib/python3.12/site-packages/tenacity/__init__.py:480: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/services/embedding/providers/openai.py:88: in get_embeddings
    response = self.client.embeddings.create(
venv/lib/python3.12/site-packages/openai/resources/embeddings.py:132: in create
    return self._post(
venv/lib/python3.12/site-packages/openai/_base_client.py:1259: in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <openai.OpenAI object at 0x7d9309eec470>
cast_to = <class 'openai.types.create_embedding_response.CreateEmbeddingResponse'>
options = FinalRequestOptions(method='post', url='/embeddings', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT...son_data={'input': ['Hello, world!'], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}, extra_json=None)

    def request(
        self,
        cast_to: Type[ResponseT],
        options: FinalRequestOptions,
        *,
        stream: bool = False,
        stream_cls: type[_StreamT] | None = None,
    ) -> ResponseT | _StreamT:
        cast_to = self._maybe_override_cast_to(cast_to, options)
    
        # create a copy of the options we were given so that if the
        # options are mutated later & we then retry, the retries are
        # given the original options
        input_options = model_copy(options)
        if input_options.idempotency_key is None and input_options.method.lower() != "get":
            # ensure the idempotency key is reused between requests
            input_options.idempotency_key = self._idempotency_key()
    
        response: httpx.Response | None = None
        max_retries = input_options.get_max_retries(self.max_retries)
    
        retries_taken = 0
        for retries_taken in range(max_retries + 1):
            options = model_copy(input_options)
            options = self._prepare_options(options)
    
            remaining_retries = max_retries - retries_taken
            request = self._build_request(options, retries_taken=retries_taken)
            self._prepare_request(request)
    
            kwargs: HttpxSendArgs = {}
            if self.custom_auth is not None:
                kwargs["auth"] = self.custom_auth
    
            if options.follow_redirects is not None:
                kwargs["follow_redirects"] = options.follow_redirects
    
            log.debug("Sending HTTP Request: %s %s", request.method, request.url)
    
            response = None
            try:
                response = self._client.send(
                    request,
                    stream=stream or self._should_stream_response_body(request=request),
                    **kwargs,
                )
            except httpx.TimeoutException as err:
                log.debug("Encountered httpx.TimeoutException", exc_info=True)
    
                if remaining_retries > 0:
                    self._sleep_for_retry(
                        retries_taken=retries_taken,
                        max_retries=max_retries,
                        options=input_options,
                        response=None,
                    )
                    continue
    
                log.debug("Raising timeout error")
                raise APITimeoutError(request=request) from err
            except Exception as err:
                log.debug("Encountered Exception", exc_info=True)
    
                if remaining_retries > 0:
                    self._sleep_for_retry(
                        retries_taken=retries_taken,
                        max_retries=max_retries,
                        options=input_options,
                        response=None,
                    )
                    continue
    
                log.debug("Raising connection error")
                raise APIConnectionError(request=request) from err
    
            log.debug(
                'HTTP Response: %s %s "%i %s" %s',
                request.method,
                request.url,
                response.status_code,
                response.reason_phrase,
                response.headers,
            )
            log.debug("request_id: %s", response.headers.get("x-request-id"))
    
            try:
                response.raise_for_status()
            except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code
                log.debug("Encountered httpx.HTTPStatusError", exc_info=True)
    
                if remaining_retries > 0 and self._should_retry(err.response):
                    err.response.close()
                    self._sleep_for_retry(
                        retries_taken=retries_taken,
                        max_retries=max_retries,
                        options=input_options,
                        response=response,
                    )
                    continue
    
                # If the response is streamed then we need to explicitly read the response
                # to completion before attempting to access the response text.
                if not err.response.is_closed:
                    err.response.read()
    
                log.debug("Re-raising status error")
>               raise self._make_status_error_from_response(err.response) from None
E               openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: lm-studio. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

venv/lib/python3.12/site-packages/openai/_base_client.py:1047: AuthenticationError

The above exception was the direct cause of the following exception:

    @pytest.mark.integration
    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OPENAI_API_KEY environment variable not set"
    )
    def test_openai_full_workflow():
        """Test complete embedding workflow with real OpenAI API."""
        config = EmbeddingServiceConfig(
            provider="openai",
            model="text-embedding-3-small",
            provider_config={
                "api_key": os.getenv("OPENAI_API_KEY")
            },
            enable_cache=True,
            enable_rate_limiting=True,
            rate_limit_config={
                "requests_per_second": 2
            }
        )
    
        service = EmbeddingService(config)
    
        # Test single embedding
>       result1 = service.embed(["Hello, world!"])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/services/test_embedding_integration.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/services/embedding/service.py:162: in embed
    batch_result = self.provider.get_embeddings(batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/tenacity/__init__.py:338: in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/tenacity/__init__.py:477: in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/tenacity/__init__.py:378: in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

rs = <RetryCallState 138070478184064: attempt #3; slept for 3.0; last result: failed (AuthenticationError Error code: 401 -...//platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}})>

    def exc_check(rs: "RetryCallState") -> None:
        fut = t.cast(Future, rs.outcome)
        retry_exc = self.retry_error_cls(fut)
        if self.reraise:
            raise retry_exc.reraise()
>       raise retry_exc from fut.exception()
E       tenacity.RetryError: RetryError[<Future at 0x7d9309690500 state=finished raised AuthenticationError>]

venv/lib/python3.12/site-packages/tenacity/__init__.py:421: RetryError
------------------------------ Captured log call -------------------------------
ERROR    rag_factory.services.embedding.providers.openai:openai.py:107 OpenAI embedding error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: lm-studio. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    rag_factory.services.embedding.providers.openai:openai.py:107 OpenAI embedding error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: lm-studio. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    rag_factory.services.embedding.providers.openai:openai.py:107 OpenAI embedding error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: lm-studio. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    rag_factory.services.embedding.service:service.py:178 Error generating embeddings: RetryError[<Future at 0x7d9309690500 state=finished raised AuthenticationError>]
________________________ test_local_embedding_provider _________________________

    @pytest.mark.integration
    def test_local_embedding_provider():
        """Test ONNX local embedding provider."""
        try:
            config = EmbeddingServiceConfig(
                provider="onnx-local",
                model=EMBEDDING_MODEL,
                enable_cache=True
            )
    
            service = EmbeddingService(config)
    
            texts = ["This is a test", "Another test"]
            result = service.embed(texts)
    
            assert len(result.embeddings) == 2
            assert result.provider == "onnx-local"
            assert result.cost == 0.0  # Local models have no cost
>           assert len(result.embeddings[0]) == 768  # all-mpnet-base-v2 dimensions
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           assert 384 == 768
E            +  where 384 = len([0.023051774129271507, 0.12985804677009583, 0.03008302114903927, 0.05426175147294998, 0.040165700018405914, -0.022075537592172623, ...])

tests/integration/services/test_embedding_integration.py:101: AssertionError
_______________________ test_onnx_provider_compatibility _______________________

    @pytest.mark.integration
    def test_onnx_provider_compatibility():
        """Test that ONNX provider produces consistent results."""
        try:
            config = EmbeddingServiceConfig(
                provider="onnx-local",
                model=EMBEDDING_MODEL,
                enable_cache=False
            )
    
            service = EmbeddingService(config)
    
            # Test text
            text = "Machine learning is transforming technology"
    
            # Generate embedding twice
            result1 = service.embed([text])
            result2 = service.embed([text])
    
            # Should produce identical embeddings
            assert len(result1.embeddings) == 1
            assert len(result2.embeddings) == 1
            assert result1.embeddings[0] == result2.embeddings[0]
    
            # Verify metadata
            assert result1.provider == "onnx-local"
>           assert result1.dimensions == 768
E           AssertionError: assert 384 == 768
E            +  where 384 = EmbeddingResult(embeddings=[[0.009169669821858406, 0.10790054500102997, 0.055614352226257324, -0.009352089837193489, 0.017075322568416595, -0.005661534145474434, 0.017784669995307922, -0.03384237736463547, -0.02153019793331623, -0.035632189363241196, -0.08303147554397583, 0.02142961323261261, 0.021402090787887573, -0.05756121501326561, -0.00010207622835878283, 0.025315985083580017, 0.003863631747663021, 0.08728030323982239, 0.026107368990778923, 0.03593222424387932, -0.03347522020339966, -0.04096610099077225, 0.009780989959836006, -0.005716932471841574, 0.028958845883607864, 0.047344546765089035, -0.009005074389278889, -0.031476523727178574, 0.0007489648414775729, -0.014581914059817791, -0.04500942677259445, -0.049206189811229706, 0.05957046523690224, 0.03722509369254112, -0.05298028513789177, -0.038001205772161484, 0.023448796942830086, 0.011143304407596588, -0.00027080747531726956, -0.0005079850670881569, 0.015201549977064133, -0.0625193640589714, 0.06447001546621323, -0.053354308009147644, -0.028053592890501022, -0.022212062031030655, 0.018431471660733223, 0.0028718647081404924, 0.0637274757027626, 0.013170670717954636, -0.028647350147366524, -0.06545812636613846, -0.029545454...0.09691505879163742, 0.008526545949280262, -0.03046737238764763, -0.027000712230801582, -0.041392628103494644, 0.07175196707248688, -0.014601858332753181, 0.005366362165659666, -0.016283929347991943, -0.01535884104669094, -0.010170861147344112, -0.06489206105470657, 0.0931612029671669, 0.002049919916316867, 0.049107544124126434, -0.021203968673944473, 0.019493939355015755, -0.02454577200114727, 0.013964586891233921, 0.002276134677231312, 0.06958603113889694, 0.014445452019572258, 0.000661437981761992, 0.00933430902659893, 0.034703537821769714, -0.04443000629544258, 0.04225888475775719, -0.07536277174949646, 0.029021067544817924, -0.11214075982570648, -0.022681714966893196, -0.016538765281438828, 0.03096669912338257, -0.06484957039356232, 0.025818174704909325, 0.011914189904928207, 0.045037541538476944, -0.04556179419159889, 0.016516463831067085, 0.01109048631042242, 0.02429947257041931, -0.009558331221342087, -0.029329214245080948, -0.0045277755707502365, 0.09307458996772766, 0.057783208787441254, -0.014441781677305698, -0.0074713099747896194, -0.027757132425904274]], model='Xenova/all-MiniLM-L6-v2', dimensions=384, token_count=128, cost=0.0, provider='onnx-local', cached=[False]).dimensions

tests/integration/services/test_embedding_integration.py:247: AssertionError
_____________________ test_embedding_database_consistency ______________________

embedding_service = <rag_factory.services.embedding.service.EmbeddingService object at 0x7d93094f8170>
database_service = (<rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7d93099f6de0>, <AsyncMock name='create_pool().acquire().__aenter__()' id='138070469885584'>)

    @pytest.mark.asyncio
    async def test_embedding_database_consistency(embedding_service, database_service):
        """Test consistency between embedding dimensions and database storage."""
        db_service, db_conn = database_service
    
        # Get embedding dimension
        dim = embedding_service.provider.get_dimensions()
    
        # Create chunk with correct dimension
        embedding = [0.1] * dim
        chunk = {
            "chunk_id": "1",
            "text": "test",
            "embedding": embedding
        }
    
>       await db_service.store_chunks([chunk])

tests/integration/services/test_service_integration.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7d93099f6de0>
chunks = [{'chunk_id': '1', 'embedding': [0.1, 0.1, 0.1], 'text': 'test'}]

    async def store_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """Store document chunks.
    
        Args:
            chunks: List of chunk dictionaries. Each chunk should contain
                   at minimum: text content and embedding vector. Additional
                   fields like metadata, chunk_id, etc. are implementation-specific.
    
        Raises:
            Exception: If storage fails
        """
        if not chunks:
            return
    
        pool = await self._get_pool()
    
>       async with pool.acquire() as conn:
E       TypeError: 'coroutine' object does not support the asynchronous context manager protocol

rag_factory/services/database/postgres.py:157: TypeError
_____________________________ test_error_recovery ______________________________

mock_vector_store = <Mock id='138070470965392'>
mock_database = <Mock id='138070470958144'>
mock_llm_service = <Mock id='138070470965728'>
mock_embedding_service = <Mock id='138070470266176'>

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_error_recovery(
        mock_vector_store,
        mock_database,
        mock_llm_service,
        mock_embedding_service
    ):
        """Test error recovery with fallback."""
        from rag_factory.services.dependencies import StrategyDependencies
    
        # Make LLM fail for some chunks
        call_count = 0
    
        async def flaky_generate(prompt, temperature, max_tokens):
            nonlocal call_count
            call_count += 1
            if call_count % 3 == 0:
                raise Exception("LLM error")
            response = Mock()
            # Return longer text to pass context_length_min=10 (need ~40+ characters)
            response.text = "This chunk provides detailed contextual information about the content."
            return response
    
        mock_llm_service.agenerate = flaky_generate
    
        config = ContextualRetrievalConfig(
            fallback_to_no_context=True,
            batch_size=5,
            min_chunk_size_for_context=10,
            context_length_min=10,
            enable_parallel_batches=False  # Use sequential to ensure proper error handling
        )
    
        dependencies = StrategyDependencies(
            database_service=mock_database,
            llm_service=mock_llm_service,
            embedding_service=mock_embedding_service
        )
        # Add vector_store as additional attribute for testing
        dependencies.vector_store = mock_vector_store
    
        strategy = ContextualRetrievalStrategy(
            config=config.model_dump(),
            dependencies=dependencies
        )
    
        chunks = [{"chunk_id": f"c{i}", "text": f"Text {i} " * 10, "metadata": {}} for i in range(10)]
    
        # Should complete despite errors
        result = await strategy.aindex_document("doc", "doc_1", chunks)
    
>       assert result["total_chunks"] == 10
E       assert 7 == 10

tests/integration/strategies/test_contextual_integration.py:277: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    rag_factory.strategies.contextual.context_generator:context_generator.py:94 Error generating context for chunk c2: LLM error
WARNING  rag_factory.strategies.contextual.batch_processor:batch_processor.py:254 Chunk c2 returned without context (context was None)
ERROR    rag_factory.strategies.contextual.context_generator:context_generator.py:94 Error generating context for chunk c5: LLM error
WARNING  rag_factory.strategies.contextual.batch_processor:batch_processor.py:254 Chunk c5 returned without context (context was None)
ERROR    rag_factory.strategies.contextual.context_generator:context_generator.py:94 Error generating context for chunk c8: LLM error
WARNING  rag_factory.strategies.contextual.batch_processor:batch_processor.py:254 Chunk c8 returned without context (context was None)
____________________________ test_hybrid_retrieval _____________________________

mock_vector_store = <Mock id='138070469871280'>
mock_llm = <Mock id='138070469869696'>
mock_embedding_service = <Mock id='138070486777984'>
mock_graph_service = <Mock id='138070472348000'>
config = KnowledgeGraphConfig(entity_types=[<EntityType.PERSON: 'person'>, <EntityType.PLACE: 'place'>, <EntityType.ORGANIZATIO...ship_confidence=0.5, min_relationship_strength=0.3, batch_size=10, enable_entity_deduplication=True, neo4j_config=None)

    @pytest.mark.integration
    def test_hybrid_retrieval(mock_vector_store, mock_llm, mock_embedding_service, mock_graph_service, config):
        """Test hybrid retrieval combining vector and graph."""
        from rag_factory.services.dependencies import StrategyDependencies
    
        dependencies = StrategyDependencies(
            llm_service=mock_llm,
            embedding_service=mock_embedding_service,
            graph_service=mock_graph_service
        )
    
        strategy = KnowledgeGraphRAGStrategy(
            config=config.dict() if hasattr(config, 'dict') else config.__dict__,
            dependencies=dependencies
        )
    
        # Index document
        document = """Python is a popular programming language for Machine Learning.
    
    Machine Learning is a subset of Artificial Intelligence."""
    
        strategy.index_document(document, "ml_doc")
    
        # Retrieve with hybrid search
>       results = strategy.retrieve("What is Machine Learning?", top_k=3)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/strategies/test_knowledge_graph_integration.py:168: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/knowledge_graph/strategy.py:195: in retrieve
    results = self.hybrid_retriever.retrieve(query, top_k=top_k, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.strategies.knowledge_graph.hybrid_retriever.HybridRetriever object at 0x7d9309691970>
query = 'What is Machine Learning?', top_k = 3, kwargs = {}

    def retrieve(
        self,
        query: str,
        top_k: int = 5,
        **kwargs
    ) -> List[HybridSearchResult]:
        """
        Hybrid retrieval combining vector search and graph traversal.
    
        Args:
            query: Search query
            top_k: Number of results to return
            **kwargs: Additional parameters
    
        Returns:
            List of hybrid search results
        """
        logger.info(f"Hybrid retrieval for query: {query}")
    
        # Step 1: Vector search
>       vector_results = self.vector_store.search(query, top_k=top_k * 2)
                         ^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'search'

rag_factory/strategies/knowledge_graph/hybrid_retriever.py:64: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  rag_factory.strategies.knowledge_graph.strategy:strategy.py:118 No database service available for chunk indexing
ERROR    rag_factory.strategies.knowledge_graph.entity_extractor:entity_extractor.py:163 Failed to create entity from data {'source': 'Python', 'target': 'Machine Learning', 'type': 'related_to', 'description': 'Used for ML', 'strength': 0.9, 'confidence': 0.85}: 'name'
ERROR    rag_factory.strategies.knowledge_graph.entity_extractor:entity_extractor.py:163 Failed to create entity from data {'source': 'Machine Learning', 'target': 'Artificial Intelligence', 'type': 'is_part_of', 'description': 'Subset of AI', 'strength': 0.95, 'confidence': 0.9}: 'name'
ERROR    rag_factory.strategies.knowledge_graph.relationship_extractor:relationship_extractor.py:186 Failed to create relationship from data {'name': 'Python', 'type': 'concept', 'description': 'Programming language', 'confidence': 0.95}: 'source'
ERROR    rag_factory.strategies.knowledge_graph.relationship_extractor:relationship_extractor.py:186 Failed to create relationship from data {'name': 'Machine Learning', 'type': 'concept', 'description': 'AI technique', 'confidence': 0.9}: 'source'
ERROR    rag_factory.strategies.knowledge_graph.relationship_extractor:relationship_extractor.py:186 Failed to create relationship from data {'name': 'Artificial Intelligence', 'type': 'concept', 'description': 'AI field', 'confidence': 0.92}: 'source'
__________________________ test_relationship_queries ___________________________

mock_vector_store = <Mock id='138070469723824'>
mock_llm = <Mock id='138070469730880'>
mock_embedding_service = <Mock id='138070469736304'>
mock_graph_service = <Mock id='138070469736064'>
config = KnowledgeGraphConfig(entity_types=[<EntityType.PERSON: 'person'>, <EntityType.PLACE: 'place'>, <EntityType.ORGANIZATIO...ship_confidence=0.5, min_relationship_strength=0.3, batch_size=10, enable_entity_deduplication=True, neo4j_config=None)

    @pytest.mark.integration
    def test_relationship_queries(mock_vector_store, mock_llm, mock_embedding_service, mock_graph_service, config):
        """Test relationship-based queries."""
        from rag_factory.services.dependencies import StrategyDependencies
    
        dependencies = StrategyDependencies(
            llm_service=mock_llm,
            embedding_service=mock_embedding_service,
            graph_service=mock_graph_service
        )
    
        strategy = KnowledgeGraphRAGStrategy(
            config=config.dict() if hasattr(config, 'dict') else config.__dict__,
            dependencies=dependencies
        )
    
        document = """Climate change causes rising temperatures.
    
    Rising temperatures lead to glacier melting.
    
    Glacier melting results in sea level rise."""
    
        strategy.index_document(document, "climate_doc")
    
        # Query about causal relationships
>       results = strategy.retrieve("What causes sea level rise?", top_k=3)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/strategies/test_knowledge_graph_integration.py:202: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/knowledge_graph/strategy.py:195: in retrieve
    results = self.hybrid_retriever.retrieve(query, top_k=top_k, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.strategies.knowledge_graph.hybrid_retriever.HybridRetriever object at 0x7d93094d7530>
query = 'What causes sea level rise?', top_k = 3, kwargs = {}

    def retrieve(
        self,
        query: str,
        top_k: int = 5,
        **kwargs
    ) -> List[HybridSearchResult]:
        """
        Hybrid retrieval combining vector search and graph traversal.
    
        Args:
            query: Search query
            top_k: Number of results to return
            **kwargs: Additional parameters
    
        Returns:
            List of hybrid search results
        """
        logger.info(f"Hybrid retrieval for query: {query}")
    
        # Step 1: Vector search
>       vector_results = self.vector_store.search(query, top_k=top_k * 2)
                         ^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'search'

rag_factory/strategies/knowledge_graph/hybrid_retriever.py:64: AttributeError
------------------------------ Captured log call -------------------------------
WARNING  rag_factory.strategies.knowledge_graph.strategy:strategy.py:118 No database service available for chunk indexing
ERROR    rag_factory.strategies.knowledge_graph.entity_extractor:entity_extractor.py:163 Failed to create entity from data {'source': 'Python', 'target': 'Machine Learning', 'type': 'related_to', 'description': 'Used for ML', 'strength': 0.9, 'confidence': 0.85}: 'name'
ERROR    rag_factory.strategies.knowledge_graph.entity_extractor:entity_extractor.py:163 Failed to create entity from data {'source': 'Machine Learning', 'target': 'Artificial Intelligence', 'type': 'is_part_of', 'description': 'Subset of AI', 'strength': 0.95, 'confidence': 0.9}: 'name'
ERROR    rag_factory.strategies.knowledge_graph.relationship_extractor:relationship_extractor.py:186 Failed to create relationship from data {'name': 'Python', 'type': 'concept', 'description': 'Programming language', 'confidence': 0.95}: 'source'
ERROR    rag_factory.strategies.knowledge_graph.relationship_extractor:relationship_extractor.py:186 Failed to create relationship from data {'name': 'Machine Learning', 'type': 'concept', 'description': 'AI technique', 'confidence': 0.9}: 'source'
ERROR    rag_factory.strategies.knowledge_graph.relationship_extractor:relationship_extractor.py:186 Failed to create relationship from data {'name': 'Artificial Intelligence', 'type': 'concept', 'description': 'AI field', 'confidence': 0.92}: 'source'
_________________________ test_late_chunking_workflow __________________________

test_vector_store = <tests.integration.strategies.test_late_chunking_integration.MockVectorStore object at 0x7d9309600b60>

    @pytest.mark.integration
    def test_late_chunking_workflow(test_vector_store):
        """Test complete late chunking workflow."""
        config = {
            "model_name": EMBEDDING_MODEL,
            "chunking_method": EmbeddingChunkingMethod.SEMANTIC_BOUNDARY.value,
            "target_chunk_size": 128,
            "compute_coherence_scores": True,
    
        }
    
        strategy = LateChunkingRAGStrategy(
            vector_store_service=test_vector_store,
            config=config
        )
    
        # Index document
        document = """
        Machine learning is a subset of artificial intelligence that enables systems to learn from data.
        Deep learning is a type of machine learning that uses neural networks with many layers.
        Neural networks are inspired by biological neurons in the human brain.
        The training process involves adjusting network weights to minimize error.
        """
    
>       strategy.index_document(document, "ml_doc")

tests/integration/strategies/test_late_chunking_integration.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/late_chunking/strategy.py:71: in index_document
    doc_embedding = self.document_embedder.embed_document(document, document_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/strategies/late_chunking/document_embedder.py:116: in embed_document
    outputs = self.session.run(
venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:283: in run
    self._validate_input(list(input_feed.keys()))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7d9309601e80>
feed_input_names = ['input_ids', 'attention_mask']

    def _validate_input(self, feed_input_names):
        missing_input_names = []
        for input in self._inputs_meta:
            if input.name not in feed_input_names and not input.type.startswith("optional"):
                missing_input_names.append(input.name)
        if missing_input_names:
>           raise ValueError(
                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
            )
E           ValueError: Required inputs (['token_type_ids']) are missing from input feed (['input_ids', 'attention_mask']).

venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:265: ValueError
_____________________ test_fixed_size_chunking_integration _____________________

test_vector_store = <tests.integration.strategies.test_late_chunking_integration.MockVectorStore object at 0x7d930955bda0>

    @pytest.mark.integration
    def test_fixed_size_chunking_integration(test_vector_store):
        """Test integration with fixed-size chunking."""
        config = {
            "model_name": EMBEDDING_MODEL,
            "chunking_method": "fixed_size",
            "target_chunk_size": 50,
            "chunk_overlap_tokens": 10,
            "compute_coherence_scores": False,
    
        }
    
        strategy = LateChunkingRAGStrategy(test_vector_store, config)
    
        document = "This is a test document. " * 20
    
>       strategy.index_document(document, "test_doc")

tests/integration/strategies/test_late_chunking_integration.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/late_chunking/strategy.py:71: in index_document
    doc_embedding = self.document_embedder.embed_document(document, document_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/strategies/late_chunking/document_embedder.py:116: in embed_document
    outputs = self.session.run(
venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:283: in run
    self._validate_input(list(input_feed.keys()))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7d930955b4a0>
feed_input_names = ['input_ids', 'attention_mask']

    def _validate_input(self, feed_input_names):
        missing_input_names = []
        for input in self._inputs_meta:
            if input.name not in feed_input_names and not input.type.startswith("optional"):
                missing_input_names.append(input.name)
        if missing_input_names:
>           raise ValueError(
                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
            )
E           ValueError: Required inputs (['token_type_ids']) are missing from input feed (['input_ids', 'attention_mask']).

venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:265: ValueError
______________________ test_adaptive_chunking_integration ______________________

test_vector_store = <tests.integration.strategies.test_late_chunking_integration.MockVectorStore object at 0x7d9309dd9610>

    @pytest.mark.integration
    def test_adaptive_chunking_integration(test_vector_store):
        """Test integration with adaptive chunking."""
        config = {
            "model_name": EMBEDDING_MODEL,
            "chunking_method": "adaptive",
            "min_chunk_size": 20,
            "max_chunk_size": 100,
    
        }
    
        strategy = LateChunkingRAGStrategy(test_vector_store, config)
    
        document = """
        Python is a versatile programming language. It is widely used in data science and machine learning.
        Libraries like NumPy and Pandas make data manipulation easy. TensorFlow and PyTorch are popular ML frameworks.
        """
    
>       strategy.index_document(document, "python_doc")

tests/integration/strategies/test_late_chunking_integration.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/late_chunking/strategy.py:71: in index_document
    doc_embedding = self.document_embedder.embed_document(document, document_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/strategies/late_chunking/document_embedder.py:116: in embed_document
    outputs = self.session.run(
venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:283: in run
    self._validate_input(list(input_feed.keys()))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7d9309dd8d40>
feed_input_names = ['input_ids', 'attention_mask']

    def _validate_input(self, feed_input_names):
        missing_input_names = []
        for input in self._inputs_meta:
            if input.name not in feed_input_names and not input.type.startswith("optional"):
                missing_input_names.append(input.name)
        if missing_input_names:
>           raise ValueError(
                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
            )
E           ValueError: Required inputs (['token_type_ids']) are missing from input feed (['input_ids', 'attention_mask']).

venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:265: ValueError
___________________________ test_multiple_documents ____________________________

test_vector_store = <tests.integration.strategies.test_late_chunking_integration.MockVectorStore object at 0x7d93091a9a00>

    @pytest.mark.integration
    def test_multiple_documents(test_vector_store):
        """Test indexing multiple documents."""
        config = {
            "model_name": EMBEDDING_MODEL,
            "chunking_method": "semantic_boundary",
    
        }
    
        strategy = LateChunkingRAGStrategy(test_vector_store, config)
    
        documents = [
            ("First document about AI and machine learning.", "doc1"),
            ("Second document about neural networks and deep learning.", "doc2"),
            ("Third document about data science and analytics.", "doc3")
        ]
    
        for text, doc_id in documents:
>           strategy.index_document(text, doc_id)

tests/integration/strategies/test_late_chunking_integration.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/late_chunking/strategy.py:71: in index_document
    doc_embedding = self.document_embedder.embed_document(document, document_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/strategies/late_chunking/document_embedder.py:116: in embed_document
    outputs = self.session.run(
venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:283: in run
    self._validate_input(list(input_feed.keys()))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7d93091aa900>
feed_input_names = ['input_ids', 'attention_mask']

    def _validate_input(self, feed_input_names):
        missing_input_names = []
        for input in self._inputs_meta:
            if input.name not in feed_input_names and not input.type.startswith("optional"):
                missing_input_names.append(input.name)
        if missing_input_names:
>           raise ValueError(
                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
            )
E           ValueError: Required inputs (['token_type_ids']) are missing from input feed (['input_ids', 'attention_mask']).

venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:265: ValueError
________________________ test_coherence_scores_computed ________________________

test_vector_store = <tests.integration.strategies.test_late_chunking_integration.MockVectorStore object at 0x7d93093cc380>

    @pytest.mark.integration
    def test_coherence_scores_computed(test_vector_store):
        """Test that coherence scores are computed when enabled."""
        config = {
            "model_name": EMBEDDING_MODEL,
            "chunking_method": "semantic_boundary",
            "compute_coherence_scores": True,
    
        }
    
        strategy = LateChunkingRAGStrategy(test_vector_store, config)
    
        document = "Test document with multiple sentences. Each sentence adds more context."
    
>       strategy.index_document(document, "coherence_test")

tests/integration/strategies/test_late_chunking_integration.py:203: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/late_chunking/strategy.py:71: in index_document
    doc_embedding = self.document_embedder.embed_document(document, document_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/strategies/late_chunking/document_embedder.py:116: in embed_document
    outputs = self.session.run(
venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:283: in run
    self._validate_input(list(input_feed.keys()))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7d93093cc4a0>
feed_input_names = ['input_ids', 'attention_mask']

    def _validate_input(self, feed_input_names):
        missing_input_names = []
        for input in self._inputs_meta:
            if input.name not in feed_input_names and not input.type.startswith("optional"):
                missing_input_names.append(input.name)
        if missing_input_names:
>           raise ValueError(
                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
            )
E           ValueError: Required inputs (['token_type_ids']) are missing from input feed (['input_ids', 'attention_mask']).

venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:265: ValueError
_____________________________ test_short_document ______________________________

test_vector_store = <tests.integration.strategies.test_late_chunking_integration.MockVectorStore object at 0x7d93091723f0>

    @pytest.mark.integration
    def test_short_document(test_vector_store):
        """Test handling of very short documents."""
        config = {
            "model_name": EMBEDDING_MODEL,
    
        }
    
        strategy = LateChunkingRAGStrategy(test_vector_store, config)
    
        short_doc = "Short."
    
>       strategy.index_document(short_doc, "short_doc")

tests/integration/strategies/test_late_chunking_integration.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/late_chunking/strategy.py:71: in index_document
    doc_embedding = self.document_embedder.embed_document(document, document_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/strategies/late_chunking/document_embedder.py:116: in embed_document
    outputs = self.session.run(
venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:283: in run
    self._validate_input(list(input_feed.keys()))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7d9309172510>
feed_input_names = ['input_ids', 'attention_mask']

    def _validate_input(self, feed_input_names):
        missing_input_names = []
        for input in self._inputs_meta:
            if input.name not in feed_input_names and not input.type.startswith("optional"):
                missing_input_names.append(input.name)
        if missing_input_names:
>           raise ValueError(
                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
            )
E           ValueError: Required inputs (['token_type_ids']) are missing from input feed (['input_ids', 'attention_mask']).

venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:265: ValueError
_________________________ test_chunk_embeddings_valid __________________________

test_vector_store = <tests.integration.strategies.test_late_chunking_integration.MockVectorStore object at 0x7d9309322600>

    @pytest.mark.integration
    def test_chunk_embeddings_valid(test_vector_store):
        """Test that chunk embeddings are valid vectors."""
        config = {
            "model_name": EMBEDDING_MODEL,
    
        }
    
        strategy = LateChunkingRAGStrategy(test_vector_store, config)
    
        document = "Test document for embedding validation."
    
>       strategy.index_document(document, "embed_test")

tests/integration/strategies/test_late_chunking_integration.py:241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/late_chunking/strategy.py:71: in index_document
    doc_embedding = self.document_embedder.embed_document(document, document_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/strategies/late_chunking/document_embedder.py:116: in embed_document
    outputs = self.session.run(
venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:283: in run
    self._validate_input(list(input_feed.keys()))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7d9309322c90>
feed_input_names = ['input_ids', 'attention_mask']

    def _validate_input(self, feed_input_names):
        missing_input_names = []
        for input in self._inputs_meta:
            if input.name not in feed_input_names and not input.type.startswith("optional"):
                missing_input_names.append(input.name)
        if missing_input_names:
>           raise ValueError(
                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
            )
E           ValueError: Required inputs (['token_type_ids']) are missing from input feed (['input_ids', 'attention_mask']).

venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:265: ValueError
____________________________ test_embedding_quality ____________________________

test_vector_store = <tests.integration.strategies.test_late_chunking_integration.MockVectorStore object at 0x7d93091a15e0>

    @pytest.mark.integration
    def test_embedding_quality(test_vector_store):
        """Test that ONNX embeddings are of good quality."""
        import numpy as np
    
        config = {
            "model_name": EMBEDDING_MODEL,
    
        }
    
        strategy = LateChunkingRAGStrategy(test_vector_store, config)
    
        # Test with sample text
        text = "The quick brown fox jumps over the lazy dog."
>       strategy.index_document(text, "quality_test")

tests/integration/strategies/test_late_chunking_integration.py:264: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/late_chunking/strategy.py:71: in index_document
    doc_embedding = self.document_embedder.embed_document(document, document_id)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/strategies/late_chunking/document_embedder.py:116: in embed_document
    outputs = self.session.run(
venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:283: in run
    self._validate_input(list(input_feed.keys()))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7d93091a1af0>
feed_input_names = ['input_ids', 'attention_mask']

    def _validate_input(self, feed_input_names):
        missing_input_names = []
        for input in self._inputs_meta:
            if input.name not in feed_input_names and not input.type.startswith("optional"):
                missing_input_names.append(input.name)
        if missing_input_names:
>           raise ValueError(
                f"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names})."
            )
E           ValueError: Required inputs (['token_type_ids']) are missing from input feed (['input_ids', 'attention_mask']).

venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:265: ValueError
_________ TestMultiQueryIntegration.test_multi_query_complete_workflow _________

self = <tests.integration.strategies.test_multi_query_integration.TestMultiQueryIntegration object at 0x7d9309d819a0>
mock_vector_store = <Mock id='138070466669856'>
mock_llm_service = <Mock id='138070466661024'>
mock_embedding_service = <Mock id='138070466661936'>
mock_database_service = <Mock id='138070466668176'>

    def test_multi_query_complete_workflow(self, mock_vector_store, mock_llm_service, mock_embedding_service, mock_database_service):
        """Test complete multi-query retrieval workflow."""
        # Create strategy
        config = MultiQueryConfig(
            num_variants=3,
            ranking_strategy=RankingStrategy.RECIPROCAL_RANK_FUSION,
            final_top_k=5
        )
        strategy = MultiQueryRAGStrategy(
            config=config,
            dependencies=StrategyDependencies(
                llm_service=mock_llm_service,
                embedding_service=mock_embedding_service,
                database_service=mock_database_service
            )
        )
    
        # Retrieve with multi-query
        results = strategy.retrieve("What is machine learning?")
    
        # Verify results
>       assert len(results) > 0
E       assert 0 > 0
E        +  where 0 = len([])

tests/integration/strategies/test_multi_query_integration.py:99: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 0: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 1: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 2: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 0 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 1 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 2 failed: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.strategy:strategy.py:96 Multi-query retrieval failed: Only 0 queries succeeded, minimum required: 1
__________ TestMultiQueryIntegration.test_multi_query_async_workflow ___________

self = <tests.integration.strategies.test_multi_query_integration.TestMultiQueryIntegration object at 0x7d9309d81d30>
mock_vector_store = <Mock id='138070466664384'>
mock_llm_service = <Mock id='138070466664720'>
mock_embedding_service = <Mock id='138070466664816'>
mock_database_service = <Mock id='138070466662224'>

    @pytest.mark.asyncio
    async def test_multi_query_async_workflow(self, mock_vector_store, mock_llm_service, mock_embedding_service, mock_database_service):
        """Test async multi-query retrieval workflow."""
        config = MultiQueryConfig(
            num_variants=3,
            ranking_strategy=RankingStrategy.FREQUENCY_BOOST,
            final_top_k=5
        )
        strategy = MultiQueryRAGStrategy(
            config=config,
            dependencies=StrategyDependencies(
                llm_service=mock_llm_service,
                embedding_service=mock_embedding_service,
                database_service=mock_database_service
            )
        )
    
        # Async retrieve
        results = await strategy.aretrieve("What is machine learning?")
    
        # Verify results
>       assert len(results) > 0
E       assert 0 > 0
E        +  where 0 = len([])

tests/integration/strategies/test_multi_query_integration.py:129: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 0: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 1: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 2: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 0 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 1 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 2 failed: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.strategy:strategy.py:96 Multi-query retrieval failed: Only 0 queries succeeded, minimum required: 1
_______________ TestMultiQueryIntegration.test_variant_diversity _______________

self = <tests.integration.strategies.test_multi_query_integration.TestMultiQueryIntegration object at 0x7d9309d82150>
mock_vector_store = <Mock id='138070467950640'>
mock_llm_service = <Mock id='138070467948576'>
mock_embedding_service = <Mock id='138070467949104'>
mock_database_service = <Mock id='138070467946992'>

    def test_variant_diversity(self, mock_vector_store, mock_llm_service, mock_embedding_service, mock_database_service):
        """Test that generated variants are diverse."""
        config = MultiQueryConfig(num_variants=4, log_variants=True)
        strategy = MultiQueryRAGStrategy(
            config=config,
            dependencies=StrategyDependencies(
                llm_service=mock_llm_service,
                embedding_service=mock_embedding_service,
                database_service=mock_database_service
            )
        )
    
        # Retrieve
        results = strategy.retrieve("What is AI?")
    
        # Variants should have been generated
        # (We can't directly check variants without modifying the strategy,
        # but we can verify results came from multiple sources)
>       assert len(results) > 0
E       assert 0 > 0
E        +  where 0 = len([])

tests/integration/strategies/test_multi_query_integration.py:151: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 0: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 1: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 2: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 3: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 4: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 0 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 1 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 2 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 3 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 4 failed: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.strategy:strategy.py:96 Multi-query retrieval failed: Only 0 queries succeeded, minimum required: 1
___________ TestMultiQueryIntegration.test_performance_requirements ____________

self = <tests.integration.strategies.test_multi_query_integration.TestMultiQueryIntegration object at 0x7d9309d824e0>
mock_vector_store = <Mock id='138070467936480'>
mock_llm_service = <Mock id='138070467943104'>
mock_embedding_service = <Mock id='138070467944784'>
mock_database_service = <Mock id='138070467943392'>

    @pytest.mark.asyncio
    async def test_performance_requirements(self, mock_vector_store, mock_llm_service, mock_embedding_service, mock_database_service):
        """Test that multi-query retrieval completes within timeout."""
        import time
    
        config = MultiQueryConfig(
            num_variants=5,
            query_timeout=5.0,
            variant_generation_timeout=5.0
        )
        strategy = MultiQueryRAGStrategy(
            config=config,
            dependencies=StrategyDependencies(
                llm_service=mock_llm_service,
                embedding_service=mock_embedding_service,
                database_service=mock_database_service
            )
        )
    
        start = time.time()
        results = await strategy.aretrieve("test query")
        elapsed = time.time() - start
    
        # Should complete reasonably fast (< 3s as per requirements)
        # In practice with mocks, should be much faster
        assert elapsed < 3.0
>       assert len(results) > 0
E       assert 0 > 0
E        +  where 0 = len([])

tests/integration/strategies/test_multi_query_integration.py:179: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 0: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 1: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 2: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 3: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:134 Error executing variant 4: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 0 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 1 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 2 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 3 failed: Vector store must have 'asearch' or 'search' method
WARNING  rag_factory.strategies.multi_query.parallel_executor:parallel_executor.py:64 Query variant 4 failed: Vector store must have 'asearch' or 'search' method
ERROR    rag_factory.strategies.multi_query.strategy:strategy.py:96 Multi-query retrieval failed: Only 0 queries succeeded, minimum required: 1
______________ TestMultiQueryIntegration.test_fallback_on_failure ______________

self = <tests.integration.strategies.test_multi_query_integration.TestMultiQueryIntegration object at 0x7d9309d828d0>
mock_vector_store = <Mock id='138070468647808'>
mock_embedding_service = <Mock id='138070468656112'>
mock_database_service = <Mock id='138070468646944'>

    @pytest.mark.asyncio
    async def test_fallback_on_failure(self, mock_vector_store, mock_embedding_service, mock_database_service):
        """Test fallback to original query on variant generation failure."""
        # Mock LLM that fails
        llm_service = Mock()
        llm_service.agenerate = AsyncMock(side_effect=Exception("LLM failed"))
    
        config = MultiQueryConfig(fallback_to_original=True, final_top_k=3)
        strategy = MultiQueryRAGStrategy(
            config=config.dict() if hasattr(config, 'dict') else config.__dict__,
            dependencies=StrategyDependencies(
                llm_service=llm_service,
                embedding_service=mock_embedding_service,
                database_service=mock_database_service
            )
        )
    
        # Should fall back to original query
        results = await strategy.aretrieve("test query")
    
        # Should still get results from fallback
>       assert len(results) > 0
E       assert 0 > 0
E        +  where 0 = len([])

tests/integration/strategies/test_multi_query_integration.py:202: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    rag_factory.strategies.multi_query.variant_generator:variant_generator.py:72 Error generating variants: LLM failed
ERROR    rag_factory.strategies.multi_query.strategy:strategy.py:96 Multi-query retrieval failed: 'dict' object has no attribute 'log_variants'
__________ TestMultiQueryIntegration.test_ranking_strategy_comparison __________

self = <tests.integration.strategies.test_multi_query_integration.TestMultiQueryIntegration object at 0x7d9309d82cf0>
mock_vector_store = <Mock id='138070468641520'>
mock_llm_service = <Mock id='138070468645216'>
mock_embedding_service = <Mock id='138070468645792'>
mock_database_service = <Mock id='138070468644016'>

    def test_ranking_strategy_comparison(self, mock_vector_store, mock_llm_service, mock_embedding_service, mock_database_service):
        """Test different ranking strategies produce different results."""
        query = "What is machine learning?"
    
        # Test with MAX_SCORE
        config_max = MultiQueryConfig(
            num_variants=3,
            ranking_strategy=RankingStrategy.MAX_SCORE,
            final_top_k=5
        )
        strategy_max = MultiQueryRAGStrategy(
            config=config_max.dict() if hasattr(config_max, 'dict') else config_max.__dict__,
            dependencies=StrategyDependencies(
                llm_service=mock_llm_service,
                embedding_service=mock_embedding_service,
                database_service=mock_database_service
            )
        )
        results_max = strategy_max.retrieve(query)
    
        # Test with FREQUENCY_BOOST
        config_freq = MultiQueryConfig(
            num_variants=3,
            ranking_strategy=RankingStrategy.FREQUENCY_BOOST,
            final_top_k=5
        )
        strategy_freq = MultiQueryRAGStrategy(
            config=config_freq.dict() if hasattr(config_freq, 'dict') else config_freq.__dict__,
            dependencies=StrategyDependencies(
                llm_service=mock_llm_service,
                embedding_service=mock_embedding_service,
                database_service=mock_database_service
            )
        )
        results_freq = strategy_freq.retrieve(query)
    
        # Both should return results
>       assert len(results_max) > 0
E       assert 0 > 0
E        +  where 0 = len([])

tests/integration/strategies/test_multi_query_integration.py:241: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    rag_factory.strategies.multi_query.strategy:strategy.py:96 Multi-query retrieval failed: 'dict' object has no attribute 'log_variants'
ERROR    rag_factory.strategies.multi_query.strategy:strategy.py:96 Multi-query retrieval failed: 'dict' object has no attribute 'log_variants'
________________ TestMultiQueryWithLMStudio.test_with_real_llm _________________

self = <tests.integration.strategies.test_multi_query_integration.TestMultiQueryWithLMStudio object at 0x7d9309d83770>
llm_service_from_env = <rag_factory.services.llm.service.LLMService object at 0x7d93091aaf00>

    def test_with_real_llm(self, llm_service_from_env):
        """Test with real LLM service from environment (LM Studio)."""
        from unittest.mock import Mock
    
        # Mock vector store
        vector_store = Mock()
    
        async def mock_search(query, top_k):
            return [
                {"chunk_id": f"c{i}", "text": f"Result {i}", "score": 0.9 - i * 0.1}
                for i in range(top_k)
            ]
    
        vector_store.asearch = mock_search
    
        # Create strategy
        config = MultiQueryConfig(num_variants=3, final_top_k=5)
>       strategy = MultiQueryRAGStrategy(
            config=config,
            dependencies=StrategyDependencies(
                llm_service=llm_service_from_env
            )
        )

tests/integration/strategies/test_multi_query_integration.py:311: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/strategies/multi_query/strategy.py:43: in __init__
    super().__init__(config, dependencies)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.strategies.multi_query.strategy.MultiQueryRAGStrategy object at 0x7d93091a8830>
config = MultiQueryConfig(num_variants=3, variant_types=[<VariantType.PARAPHRASE: 'paraphrase'>, <VariantType.EXPAND: 'expand'>...2, rrf_k=60, final_top_k=5, fallback_to_original=True, min_successful_queries=1, log_variants=True, track_metrics=True)
dependencies = StrategyDependencies(llm_service=<rag_factory.services.llm.service.LLMService object at 0x7d93091aaf00>, embedding_service=None, graph_service=None, database_service=None, reranker_service=None)

    def __init__(
        self,
        config: Dict[str, Any],
        dependencies: StrategyDependencies
    ) -> None:
        """
        Initialize the strategy with configuration and dependencies.
    
        This method validates that all required services are present before
        allowing the strategy to be instantiated. Concrete strategies should
        call super().__init__() and then perform their own initialization.
    
        Args:
            config: Strategy-specific configuration parameters
            dependencies: Injected services required by the strategy
    
        Raises:
            ValueError: If required services are missing from dependencies
    
        Example:
            >>> from rag_factory.services.dependencies import StrategyDependencies, ServiceDependency
            >>> deps = StrategyDependencies(llm_service=my_llm, embedding_service=my_embedder)
            >>> config = {"chunk_size": 1024, "top_k": 10}
            >>> strategy = MyConcreteStrategy(config, deps)
        """
        self.config = config
        self.deps = dependencies
    
        # Validate dependencies
        required = self.requires_services()
        is_valid, missing = dependencies.validate_for_strategy(required)
    
        if not is_valid:
            service_names = [s.name for s in missing]
>           raise ValueError(
                f"{self.__class__.__name__} requires services: {', '.join(service_names)}"
            )
E           ValueError: MultiQueryRAGStrategy requires services: EMBEDDING, DATABASE

rag_factory/strategies/base.py:178: ValueError
______________________ test_register_create_use_strategy _______________________

factory = <rag_factory.factory.RAGFactory object at 0x7d93091abb00>

    @pytest.mark.integration
    def test_register_create_use_strategy(factory: RAGFactory) -> None:
        """Test complete workflow: register -> create -> use."""
        # Register
        factory.register_strategy("dummy", DummyStrategy)
    
        # Create with config
        config = {"chunk_size": 512, "top_k": 3}
        strategy = factory.create_strategy("dummy", config)
    
        # Verify initialization
>       assert strategy.initialized
               ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'DummyStrategy' object has no attribute 'initialized'. Did you mean: 'initialize'?

tests/integration/test_factory_integration.py:254: AttributeError
_______________ test_create_multiple_instances_of_same_strategy ________________

factory = <rag_factory.factory.RAGFactory object at 0x7d93091a9df0>

    @pytest.mark.integration
    def test_create_multiple_instances_of_same_strategy(
        factory: RAGFactory,
    ) -> None:
        """Test creating multiple instances of the same strategy."""
        factory.register_strategy("dummy", DummyStrategy)
    
        config1 = {"chunk_size": 256, "top_k": 3}
        config2 = {"chunk_size": 1024, "top_k": 10}
    
        strategy1 = factory.create_strategy("dummy", config1)
        strategy2 = factory.create_strategy("dummy", config2)
    
        # Different instances with different configs
        assert strategy1 is not strategy2
>       assert strategy1.config.chunk_size == 256
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'chunk_size'

tests/integration/test_factory_integration.py:318: AttributeError
__________________________ test_config_file_with_yaml __________________________

tmp_path = PosixPath('/tmp/pytest-of-admindevmac/pytest-132/test_config_file_with_yaml0')
factory = <rag_factory.factory.RAGFactory object at 0x7d93091a8e30>

    @pytest.mark.integration
    def test_config_file_with_yaml(tmp_path: Path, factory: RAGFactory) -> None:
        """Test loading configuration from YAML file."""
        factory.register_strategy("dummy", DummyStrategy)
    
        # Create YAML config file
        config_file = tmp_path / "config.yaml"
        config_file.write_text(
            """
    strategy_name: dummy
    chunk_size: 2048
    chunk_overlap: 100
    top_k: 15
    metadata:
      author: test
      version: 1.0
    """
        )
    
        # Create strategy from config file
        strategy = factory.create_from_config(str(config_file))
    
        assert isinstance(strategy, DummyStrategy)
>       assert strategy.config.chunk_size == 2048
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'chunk_size'

tests/integration/test_factory_integration.py:374: AttributeError
__________________________ test_config_file_with_json __________________________

tmp_path = PosixPath('/tmp/pytest-of-admindevmac/pytest-132/test_config_file_with_json0')
factory = <rag_factory.factory.RAGFactory object at 0x7d93091a92b0>

    @pytest.mark.integration
    def test_config_file_with_json(tmp_path: Path, factory: RAGFactory) -> None:
        """Test loading configuration from JSON file."""
        factory.register_strategy("dummy", DummyStrategy)
    
        # Create JSON config file
        config_file = tmp_path / "config.json"
        config_file.write_text(
            """{
        "strategy_name": "dummy",
        "chunk_size": 768,
        "top_k": 7
    }"""
        )
    
        # Create strategy from config file
        strategy = factory.create_from_config(str(config_file))
    
        assert isinstance(strategy, DummyStrategy)
>       assert strategy.config.chunk_size == 768
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'dict' object has no attribute 'chunk_size'

tests/integration/test_factory_integration.py:400: AttributeError
_________________________ test_factory_error_recovery __________________________

factory = <rag_factory.factory.RAGFactory object at 0x7d93091aaf00>

    @pytest.mark.integration
    def test_factory_error_recovery(factory: RAGFactory) -> None:
        """Test factory handles errors gracefully and maintains state."""
        factory.register_strategy("working", WorkingStrategy)
        factory.register_strategy("broken", BrokenStrategy)
    
        # Try to create broken strategy
>       with pytest.raises(RuntimeError, match="Initialization failed"):
E       Failed: DID NOT RAISE <class 'RuntimeError'>

tests/integration/test_factory_integration.py:414: Failed
___________________ test_factory_state_after_failed_creation ___________________

factory = <rag_factory.factory.RAGFactory object at 0x7d93091aa930>

    @pytest.mark.integration
    def test_factory_state_after_failed_creation(factory: RAGFactory) -> None:
        """Test factory state remains consistent after failed strategy creation."""
        factory.register_strategy("dummy", DummyStrategy)
    
        # Try to create with invalid config
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/integration/test_factory_integration.py:432: Failed
_______________ TestPackageInstallation.test_package_installable _______________

self = <tests.integration.test_package_integration.TestPackageInstallation object at 0x7d9309d82ab0>
tmp_path = PosixPath('/tmp/pytest-of-admindevmac/pytest-132/test_package_installable0')

    @pytest.mark.slow
    def test_package_installable(self, tmp_path: Path) -> None:
        """Test package can be installed in clean environment."""
        # Create virtual environment
        venv_dir = tmp_path / "venv"
        subprocess.run(
            [sys.executable, "-m", "venv", str(venv_dir)], check=True
        )
    
        # Determine pip and python executables based on OS
        if sys.platform == "win32":
            pip_executable = venv_dir / "Scripts" / "pip"
            python_executable = venv_dir / "Scripts" / "python"
        else:
            pip_executable = venv_dir / "bin" / "pip"
            python_executable = venv_dir / "bin" / "python"
    
        # Install package in editable mode
        project_root = Path(__file__).parent.parent.parent
        subprocess.run(
            [str(pip_executable), "install", "-e", str(project_root)],
            check=True,
        )
    
        # Test import in the venv
        result = subprocess.run(
            [
                str(python_executable),
                "-c",
                "import rag_factory; print(rag_factory.__version__)",
            ],
            capture_output=True,
            text=True,
            check=False,
        )
    
>       assert result.returncode == 0
E       assert 1 == 0
E        +  where 1 = CompletedProcess(args=['/tmp/pytest-of-admindevmac/pytest-132/test_package_installable0/venv/bin/python', '-c', 'import rag_factory; print(rag_factory.__version__)'], returncode=1, stdout='', stderr='Traceback (most recent call last):\n  File "<string>", line 1, in <module>\n  File "/mnt/MCPProyects/ragTools/rag_factory/__init__.py", line 12, in <module>\n    from rag_factory.factory import (\n  File "/mnt/MCPProyects/ragTools/rag_factory/factory.py", line 33, in <module>\n    from rag_factory.strategies.base import IRAGStrategy, StrategyConfig\n  File "/mnt/MCPProyects/ragTools/rag_factory/strategies/__init__.py", line 3, in <module>\n    from rag_factory.strategies.base import (\n  File "/mnt/MCPProyects/ragTools/rag_factory/strategies/base.py", line 34, in <module>\n    from rag_factory.services.dependencies import StrategyDependencies, ServiceDependency\n  File "/mnt/MCPProyects/ragTools/rag_factory/services/__init__.py", line 28, in <module>\n    from rag_factory.services.onnx import (\n  File "/mnt/MCPProyects/ragTools/rag_factory/services/onnx/__init__.py", line 7, in <module>\n    from rag_factory.services.onnx.embedding import ONNXEmbeddingService\n  File "/mnt/MCPProyects/ragTools/rag_factory/services/onnx/embedding.py", line 12, in <module>\n    from rag_factory.services.embedding.providers.onnx_local import ONNXLocalProvider\n  File "/mnt/MCPProyects/ragTools/rag_factory/services/embedding/__init__.py", line 3, in <module>\n    from .service import EmbeddingService\n  File "/mnt/MCPProyects/ragTools/rag_factory/services/embedding/service.py", line 11, in <module>\n    from .providers import OpenAIProvider, CohereProvider, LocalProvider, ONNXLocalProvider\n  File "/mnt/MCPProyects/ragTools/rag_factory/services/embedding/providers/__init__.py", line 6, in <module>\n    from .onnx_local import ONNXLocalProvider\n  File "/mnt/MCPProyects/ragTools/rag_factory/services/embedding/providers/onnx_local.py", line 12, in <module>\n    import numpy as np\nModuleNotFoundError: No module named \'numpy\'\n').returncode

tests/integration/test_package_integration.py:50: AssertionError
----------------------------- Captured stdout call -----------------------------
Obtaining file:///mnt/MCPProyects/ragTools
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Collecting pydantic>=2.0.0 (from rag-factory==0.1.0)
  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)
Collecting pyyaml>=6.0 (from rag-factory==0.1.0)
  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)
Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->rag-factory==0.1.0)
  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.41.5 (from pydantic>=2.0.0->rag-factory==0.1.0)
  Using cached pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)
Collecting typing-extensions>=4.14.1 (from pydantic>=2.0.0->rag-factory==0.1.0)
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting typing-inspection>=0.4.2 (from pydantic>=2.0.0->rag-factory==0.1.0)
  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)
Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)
Using cached pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)
Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)
Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)
Building wheels for collected packages: rag-factory
  Building editable for rag-factory (pyproject.toml): started
  Building editable for rag-factory (pyproject.toml): finished with status 'done'
  Created wheel for rag-factory: filename=rag_factory-0.1.0-0.editable-py3-none-any.whl size=6329 sha256=36c2a0a4b949c9659129f8299813ecdaf85caef6a78d86c4679e5311bffa5ce7
  Stored in directory: /tmp/pip-ephem-wheel-cache-yxzqxzx1/wheels/08/27/d0/02501a029db205cc4e40972294db0f3cd8ac5253539696b270
Successfully built rag-factory
Installing collected packages: typing-extensions, pyyaml, annotated-types, typing-inspection, pydantic-core, pydantic, rag-factory
Successfully installed annotated-types-0.7.0 pydantic-2.12.5 pydantic-core-2.41.5 pyyaml-6.0.3 rag-factory-0.1.0 typing-extensions-4.15.0 typing-inspection-0.4.2
__________ TestFullWorkflow.test_full_workflow_with_installed_package __________

self = <tests.integration.test_package_integration.TestFullWorkflow object at 0x7d9309db5ac0>

    def test_full_workflow_with_installed_package(self) -> None:
        """Test complete workflow using installed package."""
        from rag_factory import RAGFactory, StrategyPipeline
        from rag_factory.strategies import IRAGStrategy, Chunk
        from typing import List, Any, Optional
    
        # Define a test strategy
        class TestStrategy(IRAGStrategy):
            """Test strategy implementation."""
    
            def initialize(
                self, config: Optional[dict[str, Any]] = None
            ) -> None:
                """Initialize the strategy."""
                self.config = config
    
            def prepare_data(self, documents: List[str]) -> Any:
                """Prepare data for retrieval."""
                return {"prepared": True}
    
            def retrieve(self, query: str, top_k: int = 5) -> List[Chunk]:
                """Retrieve chunks."""
                return [
                    Chunk(
                        f"Result {i}",
                        {},
                        0.9,
                        f"doc{i}",
                        f"chunk{i}",
                    )
                    for i in range(top_k)
                ]
    
            async def aretrieve(
                self, query: str, top_k: int = 5
            ) -> List[Chunk]:
                """Asynchronously retrieve chunks."""
                return self.retrieve(query, top_k)
    
            def process_query(
                self, query: str, context: List[Chunk]
            ) -> str:
                """Process query with context."""
                return f"Processed: {query} with {len(context)} chunks"
    
        # Register strategy
        factory = RAGFactory()
        factory.register_strategy("test", TestStrategy)
    
        # Create strategy
>       strategy = factory.create_strategy("test")
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/test_package_integration.py:138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.factory.RAGFactory object at 0x7d93093ce1e0>, name = 'test'
config = None, override_deps = None

    def create_strategy(
        self,
        name: str,
        config: Optional[Dict[str, Any]] = None,
        override_deps: Optional[StrategyDependencies] = None,
    ) -> IRAGStrategy:
        """
        Create and initialize a strategy instance with dependency injection.
    
        Args:
            name: Name of the registered strategy to create
            config: Optional configuration dictionary for the strategy
            override_deps: Optional dependencies to override factory defaults
    
        Returns:
            IRAGStrategy: Initialized strategy instance
    
        Raises:
            StrategyNotFoundError: If strategy name not in registry
            ValueError: If required services are missing or config is invalid
    
        Example:
            >>> factory = RAGFactory(llm_service=my_llm)
            >>> config = {"chunk_size": 1024, "top_k": 10}
            >>> strategy = factory.create_strategy("my_strategy", config)
            >>>
            >>> # Or with override dependencies
            >>> override = StrategyDependencies(llm_service=different_llm)
            >>> strategy = factory.create_strategy("my_strategy", config, override)
        """
        if name not in self._registry:
            available = list(self._registry.keys())
            raise StrategyNotFoundError(
                f"Strategy '{name}' not found. "
                f"Available strategies: {available}"
            )
    
        strategy_class = self._registry[name]
        deps = override_deps or self.dependencies
    
        # Use config dict or empty dict if not provided
        strategy_config = config or {}
    
        try:
            # Strategy constructor will validate dependencies
>           strategy = strategy_class(config=strategy_config, dependencies=deps)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: Can't instantiate abstract class TestStrategy without an implementation for abstract method 'requires_services'

rag_factory/factory.py:217: TypeError
______________ test_pipeline_continues_after_non_critical_failure ______________

    @pytest.mark.integration
    def test_pipeline_continues_after_non_critical_failure() -> None:
        """Test pipeline continues when non-required strategy fails."""
        class OptionalStrategy(IRAGStrategy):
            """Strategy that always fails."""
    
            def requires_services(self):
                """Declare required services."""
                from rag_factory.services.dependencies import ServiceDependency
                return set()
    
            def initialize(self, config: Any) -> None:
                """Initialize."""
                pass
    
            def prepare_data(self, documents: List[Dict[str, Any]]) -> Any:
                """Prepare data."""
                return None
    
            def retrieve(self, query: str, top_k: int) -> List[Chunk]:
                """Retrieve - raises error."""
                raise RuntimeError("Optional failed")
    
            async def aretrieve(self, query: str, top_k: int) -> List[Chunk]:
                """Async retrieve."""
                return []
    
            def process_query(self, query: str, context: List[Chunk]) -> str:
                """Process query."""
                return ""
    
        required_strategy = TestStrategy("Required")
    
        pipeline = StrategyPipeline()
        # Add optional failing stage
        from rag_factory.pipeline import PipelineStage
        optional_stage = PipelineStage(
>           strategy=OptionalStrategy(),
                     ^^^^^^^^^^^^^^^^^^
            name="optional",
            required=False
        )
E       TypeError: IRAGStrategy.__init__() missing 2 required positional arguments: 'config' and 'dependencies'

tests/integration/test_pipeline_integration.py:178: TypeError
________________________ test_async_fallback_execution _________________________

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_async_fallback_execution() -> None:
        """Test fallback strategy in async mode."""
        class FailingAsyncStrategy(IRAGStrategy):
            """Strategy that always fails in async mode."""
    
            def requires_services(self):
                """Declare required services."""
                from rag_factory.services.dependencies import ServiceDependency
                return set()
    
            def initialize(self, config: Any) -> None:
                """Initialize."""
                pass
    
            def prepare_data(self, documents: List[Dict[str, Any]]) -> Any:
                """Prepare data."""
                return None
    
            def retrieve(self, query: str, top_k: int) -> List[Chunk]:
                """Retrieve."""
                return []
    
            async def aretrieve(self, query: str, top_k: int) -> List[Chunk]:
                """Async retrieve - raises error."""
                raise RuntimeError("Async primary failed")
    
            def process_query(self, query: str, context: List[Chunk]) -> str:
                """Process query."""
                return ""
    
        fallback_strategy = TestStrategy("Fallback")
>       primary_strategy = FailingAsyncStrategy()
                           ^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: IRAGStrategy.__init__() missing 2 required positional arguments: 'config' and 'dependencies'

tests/integration/test_pipeline_integration.py:353: TypeError
____________________ test_parallel_execution_with_failures _____________________

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_parallel_execution_with_failures() -> None:
        """Test parallel execution handles mixed success and failure."""
        class FailingParallelStrategy(IRAGStrategy):
            """Strategy that fails in parallel execution."""
    
            def requires_services(self):
                """Declare required services."""
                from rag_factory.services.dependencies import ServiceDependency
                return set()
    
            def initialize(self, config: Any) -> None:
                """Initialize."""
                pass
    
            def prepare_data(self, documents: List[Dict[str, Any]]) -> Any:
                """Prepare data."""
                return None
    
            def retrieve(self, query: str, top_k: int) -> List[Chunk]:
                """Retrieve."""
                return []
    
            async def aretrieve(self, query: str, top_k: int) -> List[Chunk]:
                """Async retrieve - raises error."""
                raise RuntimeError("Parallel strategy failed")
    
            def process_query(self, query: str, context: List[Chunk]) -> str:
                """Process query."""
                return ""
    
        working_strategy = TestStrategy("Working")
>       failing_strategy = FailingParallelStrategy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: IRAGStrategy.__init__() missing 2 required positional arguments: 'config' and 'dependencies'

tests/integration/test_pipeline_integration.py:399: TypeError
=============================== warnings summary ===============================
rag_factory/services/llm/config.py:8
  /mnt/MCPProyects/ragTools/rag_factory/services/llm/config.py:8: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class LLMServiceConfig(BaseModel):

rag_factory/database/config.py:12
  /mnt/MCPProyects/ragTools/rag_factory/database/config.py:12: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class DatabaseConfig(BaseSettings):

tests/integration/test_config_integration.py:34
  /mnt/MCPProyects/ragTools/tests/integration/test_config_integration.py:34: PytestCollectionWarning: cannot collect test class 'TestIntegrationStrategy' because it has a __init__ constructor (from: tests/integration/test_config_integration.py)
    class TestIntegrationStrategy(IRAGStrategy):

rag_factory/strategies/contextual/config.py:31
  /mnt/MCPProyects/ragTools/rag_factory/strategies/contextual/config.py:31: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class ContextualRetrievalConfig(BaseModel):

rag_factory/strategies/late_chunking/models.py:21
  /mnt/MCPProyects/ragTools/rag_factory/strategies/late_chunking/models.py:21: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class TokenEmbedding(BaseModel):

rag_factory/strategies/late_chunking/models.py:34
  /mnt/MCPProyects/ragTools/rag_factory/strategies/late_chunking/models.py:34: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class DocumentEmbedding(BaseModel):

rag_factory/strategies/late_chunking/models.py:49
  /mnt/MCPProyects/ragTools/rag_factory/strategies/late_chunking/models.py:49: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class EmbeddingChunk(BaseModel):

tests/integration/test_pipeline_integration.py:22
  /mnt/MCPProyects/ragTools/tests/integration/test_pipeline_integration.py:22: PytestCollectionWarning: cannot collect test class 'TestStrategy' because it has a __init__ constructor (from: tests/integration/test_pipeline_integration.py)
    class TestStrategy(IRAGStrategy):

tests/integration/services/test_service_integration.py::test_embedding_database_consistency
  /mnt/MCPProyects/ragTools/rag_factory/services/database/postgres.py:157: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async with pool.acquire() as conn:
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/integration/strategies/test_knowledge_graph_integration.py::test_knowledge_graph_workflow
  /mnt/MCPProyects/ragTools/tests/integration/strategies/test_knowledge_graph_integration.py:118: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    config=config.dict() if hasattr(config, 'dict') else config.__dict__,

tests/integration/strategies/test_knowledge_graph_integration.py::test_hybrid_retrieval
  /mnt/MCPProyects/ragTools/tests/integration/strategies/test_knowledge_graph_integration.py:156: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    config=config.dict() if hasattr(config, 'dict') else config.__dict__,

tests/integration/strategies/test_knowledge_graph_integration.py::test_relationship_queries
  /mnt/MCPProyects/ragTools/tests/integration/strategies/test_knowledge_graph_integration.py:189: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    config=config.dict() if hasattr(config, 'dict') else config.__dict__,

tests/integration/strategies/test_knowledge_graph_integration.py::test_graph_statistics
  /mnt/MCPProyects/ragTools/tests/integration/strategies/test_knowledge_graph_integration.py:220: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    config=config.dict() if hasattr(config, 'dict') else config.__dict__,

tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_multi_query_complete_workflow
  /mnt/MCPProyects/ragTools/rag_factory/strategies/multi_query/strategy.py:140: RuntimeWarning: coroutine 'MultiQueryRAGStrategy.aretrieve' was never awaited
    return asyncio.run(self.aretrieve(query, **kwargs))
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_fallback_on_failure
  /mnt/MCPProyects/ragTools/tests/integration/strategies/test_multi_query_integration.py:190: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    config=config.dict() if hasattr(config, 'dict') else config.__dict__,

tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_ranking_strategy_comparison
  /mnt/MCPProyects/ragTools/tests/integration/strategies/test_multi_query_integration.py:215: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    config=config_max.dict() if hasattr(config_max, 'dict') else config_max.__dict__,

tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_ranking_strategy_comparison
  /mnt/MCPProyects/ragTools/tests/integration/strategies/test_multi_query_integration.py:231: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    config=config_freq.dict() if hasattr(config_freq, 'dict') else config_freq.__dict__,

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_______________ coverage: platform linux, python 3.12.3-final-0 ________________

Name                                                               Stmts   Miss  Cover   Missing
------------------------------------------------------------------------------------------------
rag_factory/__init__.py                                               37      9    76%   100-101, 107-109, 121-122, 130-131
rag_factory/__version__.py                                             1      0   100%
rag_factory/cli/__init__.py                                            2      0   100%
rag_factory/cli/commands/__init__.py                                   1      0   100%
rag_factory/cli/commands/benchmark.py                                 96     81    16%   40-53, 64-84, 148-263
rag_factory/cli/commands/check_consistency.py                         43     33    23%   64-120
rag_factory/cli/commands/config.py                                    78     68    13%   26-64, 102-188
rag_factory/cli/commands/index.py                                     65     52    20%   29-39, 93-185
rag_factory/cli/commands/query.py                                     47     36    23%   73-149
rag_factory/cli/commands/repl.py                                     152    130    14%   29-56, 63-108, 112-120, 124-133, 137-155, 159-172, 176-186, 190-207, 211-220, 253-260
rag_factory/cli/commands/strategies.py                                45     36    20%   48-120
rag_factory/cli/commands/validate_pipeline.py                         70     57    19%   57-156
rag_factory/cli/formatters/__init__.py                                 4      0   100%
rag_factory/cli/formatters/consistency.py                             61     55    10%   21-69, 79-103, 108-138
rag_factory/cli/formatters/output.py                                  20      9    55%   23-24, 43-44, 63-64, 74, 79, 84
rag_factory/cli/formatters/results.py                                 71     62    13%   28-55, 72-99, 116-144, 157-181
rag_factory/cli/formatters/validation.py                              63     53    16%   25-43, 48-56, 65-77, 86-98, 103-129, 134-137
rag_factory/cli/main.py                                               30     10    67%   25-28, 48-49, 74-76, 80
rag_factory/cli/utils/__init__.py                                      3      0   100%
rag_factory/cli/utils/progress.py                                     14      8    43%   34-47, 73-76
rag_factory/cli/utils/validation.py                                   53     42    21%   30-41, 57-77, 93-118, 135
rag_factory/config.py                                                155     50    68%   136, 222, 231-232, 248, 259-266, 275, 358, 367, 389, 395, 416-440, 450-451, 464-469, 479, 497, 506-508, 521-526, 536-542
rag_factory/core/__init__.py                                           4      4     0%   7-23
rag_factory/core/capabilities.py                                      61     16    74%   129, 155, 173-174, 239-253
rag_factory/core/indexing_interface.py                                54     35    35%   55-57, 118-123, 144, 164, 208, 260, 272, 292-313, 341-364
rag_factory/core/pipeline.py                                          49     35    29%   22-24, 33-40, 55-77, 95-96, 105-108, 117-120, 137-146
rag_factory/core/retrieval_interface.py                               38     20    47%   56-58, 112-117, 138, 158, 196, 247, 259, 285-309
rag_factory/database/__init__.py                                       4      0   100%
rag_factory/database/config.py                                        17      0   100%
rag_factory/database/connection.py                                    80     54    32%   40-48, 61-86, 96, 107, 126-137, 150-157, 165-170, 178-183, 196-197, 209-214, 218, 222
rag_factory/database/env_validator.py                                 38     38     0%   7-139
rag_factory/database/models.py                                        85     32    62%   31-34, 37-45, 48-54, 66-69, 72-77, 80-85, 181, 323-325
rag_factory/database/vector_indexing.py                               38     38     0%   7-114
rag_factory/evaluation/__init__.py                                     4      4     0%   29-41
rag_factory/evaluation/analysis/__init__.py                            3      3     0%   8-11
rag_factory/evaluation/analysis/comparison.py                        135    135     0%   8-323
rag_factory/evaluation/analysis/statistics.py                         81     81     0%   8-320
rag_factory/evaluation/benchmarks/__init__.py                          3      3     0%   8-14
rag_factory/evaluation/benchmarks/config.py                           25     25     0%   7-64
rag_factory/evaluation/benchmarks/runner.py                          155    155     0%   8-423
rag_factory/evaluation/datasets/__init__.py                            3      3     0%   8-14
rag_factory/evaluation/datasets/loader.py                             88     88     0%   8-281
rag_factory/evaluation/datasets/schema.py                             52     52     0%   8-160
rag_factory/evaluation/datasets/statistics.py                         65     65     0%   8-226
rag_factory/evaluation/exporters/__init__.py                           4      4     0%   8-12
rag_factory/evaluation/exporters/csv_exporter.py                      52     52     0%   3-124
rag_factory/evaluation/exporters/html_exporter.py                     47     47     0%   3-293
rag_factory/evaluation/exporters/json_exporter.py                     19     19     0%   3-68
rag_factory/evaluation/metrics/__init__.py                             3      3     0%   12-25
rag_factory/evaluation/metrics/base.py                                31     31     0%   8-115
rag_factory/evaluation/metrics/cost.py                                65     65     0%   8-313
rag_factory/evaluation/metrics/performance.py                         42     42     0%   8-202
rag_factory/evaluation/metrics/quality.py                             84     84     0%   8-377
rag_factory/evaluation/metrics/retrieval.py                           77     77     0%   9-401
rag_factory/exceptions.py                                             13      0   100%
rag_factory/factory.py                                               187    110    41%   144, 165-170, 203-204, 226, 256-270, 303, 313, 319, 347-348, 418, 435, 452-454, 458-460, 480-502, 528-545, 568-611, 644-699, 703-715
rag_factory/models/__init__.py                                         6      6     0%   3-14
rag_factory/models/embedding/__init__.py                               4      4     0%   3-12
rag_factory/models/embedding/loader.py                               179    179     0%   3-367
rag_factory/models/embedding/models.py                                47     47     0%   3-106
rag_factory/models/embedding/registry.py                             128    128     0%   3-282
rag_factory/models/evaluation/__init__.py                              3      3     0%   3-6
rag_factory/models/evaluation/ab_testing.py                           98     98     0%   3-281
rag_factory/models/evaluation/models.py                               29     29     0%   3-74
rag_factory/observability/__init__.py                                  3      0   100%
rag_factory/observability/integrations/__init__.py                     1      1     0%   3
rag_factory/observability/integrations/prometheus.py                  47     47     0%   3-229
rag_factory/observability/logging/__init__.py                          2      0   100%
rag_factory/observability/logging/config.py                           16     16     0%   3-37
rag_factory/observability/logging/filters.py                          41     41     0%   3-103
rag_factory/observability/logging/logger.py                           86     46    47%   49, 99-103, 108-115, 140-162, 190-229, 250, 270, 290-291, 306, 315, 324, 333, 344-373, 394-396
rag_factory/observability/metrics/__init__.py                          2      0   100%
rag_factory/observability/metrics/collector.py                       143     96    33%   66-68, 77-79, 88-90, 99, 108, 117, 128-143, 193-198, 221-266, 284-290, 304, 335-340, 352-364, 372-404, 442-443, 457-459
rag_factory/observability/metrics/cost.py                             39     39     0%   3-221
rag_factory/observability/metrics/performance.py                      64     64     0%   3-196
rag_factory/observability/monitoring/__init__.py                       1      1     0%   3
rag_factory/observability/monitoring/api.py                          101    101     0%   3-328
rag_factory/pipeline.py                                              156     41    74%   186-188, 209, 236-249, 291-307, 352-367, 444-450
rag_factory/repositories/__init__.py                                   5      5     0%   20-31
rag_factory/repositories/base.py                                      42     42     0%   7-164
rag_factory/repositories/chunk.py                                    206    206     0%   7-670
rag_factory/repositories/document.py                                  93     93     0%   7-312
rag_factory/repositories/exceptions.py                                18     18     0%   7-77
rag_factory/services/__init__.py                                       8      0   100%
rag_factory/services/api/__init__.py                                   4      0   100%
rag_factory/services/api/anthropic.py                                 27     18    33%   46-52, 76-94, 118-141
rag_factory/services/api/cohere.py                                    30     19    37%   14-15, 49-59, 82-113
rag_factory/services/api/openai.py                                    46     31    33%   45-51, 75-93, 117-140, 168-174, 189-199, 216-227, 235
rag_factory/services/consistency.py                                   32     24    25%   80-113, 144-170
rag_factory/services/database/__init__.py                              3      0   100%
rag_factory/services/database/neo4j.py                                54     36    33%   14, 60-71, 79-84, 103-114, 134-150, 172-181, 188-191, 195, 199
rag_factory/services/database/postgres.py                             91     56    38%   17-18, 73, 98-109, 115-139, 153, 159-185, 209-242, 256-271, 286-301, 333-351, 358-361, 365, 369
rag_factory/services/dependencies.py                                  41     12    71%   99, 103, 107, 134-140, 176-180
rag_factory/services/embedding/__init__.py                             4      0   100%
rag_factory/services/embedding/base.py                                31      6    81%   44, 59, 68, 77, 86, 98
rag_factory/services/embedding/cache.py                               43     12    72%   54-55, 70-72, 80-82, 90-94
rag_factory/services/embedding/config.py                              29      4    86%   43, 57, 63, 80
rag_factory/services/embedding/providers/__init__.py                   5      0   100%
rag_factory/services/embedding/providers/cohere.py                    53     36    32%   10-20, 55-75, 90-117, 125, 133, 141, 152-153
rag_factory/services/embedding/providers/local.py                     43     27    37%   8-9, 46-67, 83-109, 117, 125, 133, 146
rag_factory/services/embedding/providers/onnx_local.py               121     35    71%   18-19, 24-25, 92, 99, 121, 145, 150-154, 165, 177-182, 255-257, 280, 296-315
rag_factory/services/embedding/providers/openai.py                    51     20    61%   10-20, 54, 64, 70, 93-97, 116, 143-144
rag_factory/services/embedding/rate_limiter.py                        20      2    90%   50-51
rag_factory/services/embedding/service.py                            100     12    88%   88, 248-266, 273-275
rag_factory/services/interfaces.py                                    35      0   100%
rag_factory/services/llm/__init__.py                                   5      0   100%
rag_factory/services/llm/base.py                                      43      0   100%
rag_factory/services/llm/config.py                                    23      4    83%   48, 51-53
rag_factory/services/llm/prompt_template.py                           43     24    44%   47-82, 97-100, 111-115
rag_factory/services/llm/providers/__init__.py                        12     10    17%   10-22
rag_factory/services/llm/providers/anthropic.py                       57     42    26%   44-50, 64-97, 120-147, 171-172, 184-189, 197, 205
rag_factory/services/llm/providers/ollama.py                          55     42    24%   22-24, 37-67, 93-122, 142-143, 157, 165, 174, 185-196
rag_factory/services/llm/providers/openai.py                          57     35    39%   56, 62, 76-110, 133-158, 173, 186-194, 202, 210
rag_factory/services/llm/service.py                                   66     40    39%   68, 93-129, 154-175, 186, 194-198, 215-216
rag_factory/services/llm/token_counter.py                             27     17    37%   22-41, 56-57, 71-72
rag_factory/services/local/__init__.py                                 2      0   100%
rag_factory/services/local/reranker.py                                32     22    31%   13-14, 45-52, 78-116
rag_factory/services/onnx/__init__.py                                  2      0   100%
rag_factory/services/onnx/embedding.py                                29     19    34%   54-68, 83-93, 110-121, 129
rag_factory/services/utils/__init__.py                                 2      0   100%
rag_factory/services/utils/model_converter.py                        104    104     0%   8-291
rag_factory/services/utils/onnx_utils.py                             173     79    54%   22-23, 28-29, 41, 48, 94, 100-101, 106-107, 127-197, 217-238, 267, 281-282, 286, 290-293, 308-310, 350, 357-360, 453-454, 468, 484-496
rag_factory/services/utils/reranker_selector.py                       68     68     0%   8-249
rag_factory/strategies/__init__.py                                     2      0   100%
rag_factory/strategies/agentic/__init__.py                             7      7     0%   8-20
rag_factory/strategies/agentic/agent.py                              151    151     0%   8-420
rag_factory/strategies/agentic/config.py                              12     12     0%   5-31
rag_factory/strategies/agentic/frameworks/__init__.py                  1      1     0%   9
rag_factory/strategies/agentic/query_analyzer.py                      89     89     0%   8-278
rag_factory/strategies/agentic/strategy.py                            85     85     0%   8-282
rag_factory/strategies/agentic/tool_implementations.py               135    135     0%   11-511
rag_factory/strategies/agentic/tools.py                               40     40     0%   8-142
rag_factory/strategies/base.py                                        55      3    95%   81, 83, 85
rag_factory/strategies/chunking/__init__.py                           15      4    73%   45-48
rag_factory/strategies/chunking/base.py                               91     35    62%   115-128, 146, 159, 171, 182-196, 207-217, 228-256
rag_factory/strategies/chunking/docling_chunker.py                    45     26    42%   18, 61-73, 89-101, 112, 131-136, 147-152, 163-168, 186-188
rag_factory/strategies/chunking/fixed_size_chunker.py                 74     61    18%   8-9, 33-46, 58-111, 122, 145-159, 171-185, 196-204
rag_factory/strategies/chunking/hybrid_chunker.py                     70     58    17%   36-48, 63-132, 143, 157-160, 173-192
rag_factory/strategies/chunking/semantic_chunker.py                  195    171    12%   9-10, 14-15, 45-71, 83-128, 139, 155-156, 172-177, 193-206, 218-227, 245-277, 293-328, 340-386, 397-423, 434-441, 459-473, 485-534
rag_factory/strategies/chunking/structural_chunker.py                147    128    13%   9-10, 37-50, 62-69, 80, 95, 107-144, 155-201, 222-288, 299-305, 317-366, 377-385, 399-425
rag_factory/strategies/chunking/utils.py                              89     89     0%   3-264
rag_factory/strategies/contextual/__init__.py                          7      0   100%
rag_factory/strategies/contextual/batch_processor.py                  91     17    81%   125-126, 154-160, 191-196, 258-266
rag_factory/strategies/contextual/config.py                           46      0   100%
rag_factory/strategies/contextual/context_generator.py                84     31    63%   60, 64-65, 83-84, 87, 99, 113, 120-121, 125-127, 131-133, 162-165, 168, 171-172, 224-234
rag_factory/strategies/contextual/cost_tracker.py                     32      2    94%   82, 126
rag_factory/strategies/contextual/prompts.py                          25      9    64%   88, 90, 93-94, 98-101, 104
rag_factory/strategies/contextual/storage.py                          44      8    82%   107-114
rag_factory/strategies/contextual/strategy.py                         73     12    84%   159, 172, 193-201, 225-226, 235, 264, 270, 275
rag_factory/strategies/fine_tuned/__init__.py                          4      4     0%   1-5
rag_factory/strategies/fine_tuned/ab_testing.py                       78     78     0%   1-134
rag_factory/strategies/fine_tuned/config.py                           11     11     0%   1-30
rag_factory/strategies/fine_tuned/custom_loader.py                    40     40     0%   1-88
rag_factory/strategies/fine_tuned/model_registry.py                  126    126     0%   1-303
rag_factory/strategies/hierarchical/__init__.py                        5      5     0%   8-21
rag_factory/strategies/hierarchical/hierarchy_builder.py              79     79     0%   8-335
rag_factory/strategies/hierarchical/models.py                         58     58     0%   8-137
rag_factory/strategies/hierarchical/parent_retriever.py               89     89     0%   8-278
rag_factory/strategies/hierarchical/strategy.py                       71     71     0%   9-292
rag_factory/strategies/indexing/__init__.py                            5      0   100%
rag_factory/strategies/indexing/context_aware.py                      97     79    19%   29, 40, 60-120, 141-142, 146-156, 168-174, 178-184, 194-223, 227-228
rag_factory/strategies/indexing/hierarchical.py                       83     68    18%   48, 60, 84-121, 143-172, 188-223, 240-265, 279-280
rag_factory/strategies/indexing/in_memory.py                          41     25    39%   54, 65, 93-121, 143, 160, 173, 194-200
rag_factory/strategies/indexing/keyword_indexing.py                   40     40     0%   8-155
rag_factory/strategies/indexing/vector_embedding.py                   29     21    28%   21, 30, 53-91
rag_factory/strategies/knowledge_graph/__init__.py                     4      0   100%
rag_factory/strategies/knowledge_graph/config.py                      21      2    90%   128-129
rag_factory/strategies/knowledge_graph/entity_extractor.py            70      7    90%   89, 130-131, 135-137, 179
rag_factory/strategies/knowledge_graph/graph_store.py                 31      9    71%   25, 35, 48, 66, 86, 99, 109, 114, 124
rag_factory/strategies/knowledge_graph/hybrid_retriever.py            56     37    34%   65-97, 101-108, 117-164
rag_factory/strategies/knowledge_graph/memory_graph_store.py          97     68    30%   60, 68-87, 96-152, 161-168, 172-182, 186-189
rag_factory/strategies/knowledge_graph/models.py                      53      0   100%
rag_factory/strategies/knowledge_graph/relationship_extractor.py      62      8    87%   57, 137-138, 142-144, 157-161
rag_factory/strategies/knowledge_graph/strategy.py                    78     15    81%   59, 101-112, 198-214, 237, 241, 245, 250, 255
rag_factory/strategies/late_chunking/__init__.py                       6      0   100%
rag_factory/strategies/late_chunking/coherence_analyzer.py            27     15    44%   36-45, 57-65, 82-96
rag_factory/strategies/late_chunking/document_embedder.py             99     58    41%   35, 56, 62-63, 72, 111, 126-177, 195-208, 220-228, 249-272, 289-296, 311
rag_factory/strategies/late_chunking/embedding_chunker.py            112     96    14%   41-61, 68-98, 105-141, 148-168, 177-178, 185-240, 251-275, 279-286
rag_factory/strategies/late_chunking/models.py                        62      0   100%
rag_factory/strategies/late_chunking/strategy.py                      49     18    63%   48, 72-105, 124-133
rag_factory/strategies/multi_query/__init__.py                         7      0   100%
rag_factory/strategies/multi_query/config.py                          36      0   100%
rag_factory/strategies/multi_query/deduplicator.py                    87     75    14%   41-100, 116-176, 188-197
rag_factory/strategies/multi_query/parallel_executor.py               52     11    79%   74-75, 84-86, 116-118, 128-132, 148, 154-155
rag_factory/strategies/multi_query/prompts.py                         10      2    80%   76-80
rag_factory/strategies/multi_query/ranker.py                          80     67    16%   35-70, 81-84, 98-103, 116-126, 141-177, 189-201
rag_factory/strategies/multi_query/strategy.py                        66     14    79%   86-93, 103, 107, 111, 130-134, 159-172
rag_factory/strategies/multi_query/variant_generator.py               62      9    85%   79, 115-129, 177, 182
rag_factory/strategies/query_expansion/__init__.py                     8      0   100%
rag_factory/strategies/query_expansion/base.py                        66     10    85%   90, 103, 114-118, 130-133
rag_factory/strategies/query_expansion/cache.py                       52     40    23%   20-22, 38-55, 64-66, 70-73, 81-97, 105-112
rag_factory/strategies/query_expansion/expander_service.py            92     71    23%   36-40, 55-58, 75-148, 159-180, 204-215, 233-234, 242-246, 254, 265-273, 283-284
rag_factory/strategies/query_expansion/hyde_expander.py               19     12    37%   25-27, 41-65
rag_factory/strategies/query_expansion/llm_expander.py                25     16    36%   23-25, 39-71, 99
rag_factory/strategies/query_expansion/metrics.py                     66     30    55%   44-45, 53-57, 76-109, 130, 134, 146-152, 160
rag_factory/strategies/query_expansion/prompts.py                     13      7    46%   16, 28-56, 68-107
rag_factory/strategies/reranking/__init__.py                           7      0   100%
rag_factory/strategies/reranking/base.py                              68     16    76%   81, 101, 106, 110-119, 123-130
rag_factory/strategies/reranking/bge_reranker.py                      51     37    27%   14, 53-81, 103-147, 151
rag_factory/strategies/reranking/cache.py                             48     34    29%   32-34, 51-66, 76-77, 81, 90-101, 110-113, 121, 136-138
rag_factory/strategies/reranking/cohere_reranker.py                   34     20    41%   15-16, 55-71, 94-115, 119
rag_factory/strategies/reranking/cosine_reranker.py                   62     47    24%   54-72, 97-139, 158-171, 188, 209-215, 227-230, 242-245, 249
rag_factory/strategies/reranking/cross_encoder_reranker.py            40     27    32%   13-14, 53-76, 98-120, 124
rag_factory/strategies/reranking/metrics.py                           67     67     0%   8-264
rag_factory/strategies/reranking/reranker_service.py                 115     88    23%   56-67, 71-96, 113-225, 235-255, 269, 282-283, 287-292, 296-300, 308-310
rag_factory/strategies/self_reflective/__init__.py                     6      6     0%   3-16
rag_factory/strategies/self_reflective/config.py                      20     20     0%   3-42
rag_factory/strategies/self_reflective/grader.py                      80     80     0%   3-266
rag_factory/strategies/self_reflective/models.py                      48     48     0%   3-109
rag_factory/strategies/self_reflective/refiner.py                     69     69     0%   3-228
rag_factory/strategies/self_reflective/strategy.py                    89     89     0%   3-264
rag_factory/utils/__init__.py                                          3      0   100%
rag_factory/utils/token_counter.py                                    45     30    33%   36-40, 52-62, 74-82, 86, 90, 105-106, 124-135, 153-158
rag_factory/utils/tokenization.py                                     95     73    23%   36-40, 53-64, 87-114, 126-129, 141-144, 156, 175-190, 209-228, 241-243, 251-252, 270-271, 292-293, 314-315
------------------------------------------------------------------------------------------------
TOTAL                                                              11124   7834    30%
=========================== short test summary info ============================
FAILED tests/integration/test_config_integration.py::test_config_with_factory
FAILED tests/integration/services/test_embedding_integration.py::test_openai_full_workflow
FAILED tests/integration/services/test_embedding_integration.py::test_local_embedding_provider
FAILED tests/integration/services/test_embedding_integration.py::test_onnx_provider_compatibility
FAILED tests/integration/services/test_service_integration.py::test_embedding_database_consistency
FAILED tests/integration/strategies/test_contextual_integration.py::test_error_recovery
FAILED tests/integration/strategies/test_knowledge_graph_integration.py::test_hybrid_retrieval
FAILED tests/integration/strategies/test_knowledge_graph_integration.py::test_relationship_queries
FAILED tests/integration/strategies/test_late_chunking_integration.py::test_late_chunking_workflow
FAILED tests/integration/strategies/test_late_chunking_integration.py::test_fixed_size_chunking_integration
FAILED tests/integration/strategies/test_late_chunking_integration.py::test_adaptive_chunking_integration
FAILED tests/integration/strategies/test_late_chunking_integration.py::test_multiple_documents
FAILED tests/integration/strategies/test_late_chunking_integration.py::test_coherence_scores_computed
FAILED tests/integration/strategies/test_late_chunking_integration.py::test_short_document
FAILED tests/integration/strategies/test_late_chunking_integration.py::test_chunk_embeddings_valid
FAILED tests/integration/strategies/test_late_chunking_integration.py::test_embedding_quality
FAILED tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_multi_query_complete_workflow
FAILED tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_multi_query_async_workflow
FAILED tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_variant_diversity
FAILED tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_performance_requirements
FAILED tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_fallback_on_failure
FAILED tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryIntegration::test_ranking_strategy_comparison
FAILED tests/integration/strategies/test_multi_query_integration.py::TestMultiQueryWithLMStudio::test_with_real_llm
FAILED tests/integration/test_factory_integration.py::test_register_create_use_strategy
FAILED tests/integration/test_factory_integration.py::test_create_multiple_instances_of_same_strategy
FAILED tests/integration/test_factory_integration.py::test_config_file_with_yaml
FAILED tests/integration/test_factory_integration.py::test_config_file_with_json
FAILED tests/integration/test_factory_integration.py::test_factory_error_recovery
FAILED tests/integration/test_factory_integration.py::test_factory_state_after_failed_creation
FAILED tests/integration/test_package_integration.py::TestPackageInstallation::test_package_installable
FAILED tests/integration/test_package_integration.py::TestFullWorkflow::test_full_workflow_with_installed_package
FAILED tests/integration/test_pipeline_integration.py::test_pipeline_continues_after_non_critical_failure
FAILED tests/integration/test_pipeline_integration.py::test_async_fallback_execution
FAILED tests/integration/test_pipeline_integration.py::test_parallel_execution_with_failures
ERROR tests/integration/services/test_service_integration.py::test_rag_workflow
== 34 failed, 41 passed, 2 skipped, 17 warnings, 1 error in 122.89s (0:02:02) ==

========================================
Test run completed at 2025-12-15 08:44:29
========================================
