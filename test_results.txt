========================================
Test Results - 2025-12-17 19:07:47
========================================
Environment Configuration:
  EMBEDDING_MODEL_NAME: Xenova/all-MiniLM-L6-v2
  EMBEDDING_MODEL_PATH: models/embeddings
  LM_STUDIO_BASE_URL: http://192.168.56.1:1234/v1
  LM_STUDIO_MODEL: phi-4-mini-instruct

Command: pytest tests/benchmarks/test_late_chunking_performance.py tests/integration_real/test_database_real.py tests/integration_real/test_end_to_end_real.py tests/integration_real/test_llm_real.py tests/integration/registry/test_registry_integration.py tests/integration/strategies/test_hierarchical_integration.py tests/integration/strategies/test_late_chunking_integration.py tests/integration/strategies/test_self_reflective_integration.py tests/integration/test_package_integration.py tests/test_mock_registry.py tests/unit/config/test_strategy_pair_manager.py tests/unit/database/test_migrations.py tests/unit/documentation/test_code_examples.py tests/unit/documentation/test_links.py tests/unit/registry/test_service_factory.py tests/unit/services/embeddings/test_onnx_local.py tests/unit/services/embedding/test_onnx_local_provider.py tests/unit/strategies/indexing/test_context_aware.py tests/unit/strategies/self_reflective/test_strategy.py tests/unit/test_package.py tests/integration/database/test_migration_integration.py tests/integration/database/test_migration_validator_integration.py
========================================

============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-9.0.1, pluggy-1.6.0 -- /mnt/MCPProyects/ragTools/venv/bin/python3
cachedir: .pytest_cache
rootdir: /mnt/MCPProyects/ragTools
configfile: pytest.ini (WARNING: ignoring pytest config in pyproject.toml!)
plugins: anyio-4.12.0, cov-7.0.0, asyncio-1.3.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 188 items

tests/benchmarks/test_late_chunking_performance.py::test_document_embedding_speed PASSED [  0%]
tests/benchmarks/test_late_chunking_performance.py::test_embedding_chunking_speed PASSED [  1%]
tests/benchmarks/test_late_chunking_performance.py::test_semantic_boundary_speed PASSED [  1%]
tests/benchmarks/test_late_chunking_performance.py::test_end_to_end_latency PASSED [  2%]
tests/benchmarks/test_late_chunking_performance.py::test_coherence_analysis_overhead PASSED [  2%]
tests/benchmarks/test_late_chunking_performance.py::test_batch_processing_speed PASSED [  3%]
tests/benchmarks/test_late_chunking_performance.py::test_adaptive_chunking_speed PASSED [  3%]
tests/benchmarks/test_late_chunking_performance.py::test_memory_efficiency PASSED [  4%]
tests/integration_real/test_database_real.py::test_postgres_connection PASSED [  4%]
tests/integration_real/test_database_real.py::test_table_creation PASSED [  5%]
tests/integration_real/test_database_real.py::test_store_and_retrieve_chunks FAILED [  5%]
tests/integration_real/test_database_real.py::test_vector_similarity_search FAILED [  6%]
tests/integration_real/test_database_real.py::test_batch_embedding_and_storage FAILED [  6%]
tests/integration_real/test_database_real.py::test_chunk_metadata_persistence FAILED [  7%]
tests/integration_real/test_database_real.py::test_database_context_table_mapping FAILED [  7%]
tests/integration_real/test_database_real.py::test_connection_pooling PASSED [  8%]
tests/integration_real/test_end_to_end_real.py::test_document_indexing_pipeline FAILED [  9%]
tests/integration_real/test_end_to_end_real.py::test_retrieval_pipeline FAILED [  9%]
tests/integration_real/test_end_to_end_real.py::test_full_rag_pipeline FAILED [ 10%]
tests/integration_real/test_end_to_end_real.py::test_multiple_document_batches FAILED [ 10%]
tests/integration_real/test_end_to_end_real.py::test_retrieval_with_metadata_filtering FAILED [ 11%]
tests/integration_real/test_end_to_end_real.py::test_large_document_indexing FAILED [ 11%]
tests/integration_real/test_end_to_end_real.py::test_retrieval_accuracy FAILED [ 12%]
tests/integration_real/test_llm_real.py::test_llm_basic_generation PASSED [ 12%]
tests/integration_real/test_llm_real.py::test_llm_conversation PASSED    [ 13%]
tests/integration_real/test_llm_real.py::test_llm_token_counting PASSED  [ 13%]
tests/integration_real/test_llm_real.py::test_llm_with_system_prompt PASSED [ 14%]
tests/integration_real/test_llm_real.py::test_llm_json_response PASSED   [ 14%]
tests/integration_real/test_llm_real.py::test_llm_max_tokens PASSED      [ 15%]
tests/integration_real/test_llm_real.py::test_llm_temperature PASSED     [ 15%]
tests/integration_real/test_llm_real.py::test_llm_streaming FAILED       [ 16%]
tests/integration_real/test_llm_real.py::test_llm_rag_context PASSED     [ 17%]
tests/integration_real/test_llm_real.py::test_llm_error_handling PASSED  [ 17%]
tests/integration/registry/test_registry_integration.py::TestRealServiceInstantiation::test_instantiate_onnx_embedding_service PASSED [ 18%]
tests/integration/registry/test_registry_integration.py::TestRealServiceInstantiation::test_embedding_service_functionality PASSED [ 18%]
tests/integration/registry/test_registry_integration.py::TestRealServiceInstantiation::test_multiple_service_instantiation FAILED [ 19%]
tests/integration/registry/test_registry_integration.py::TestEnvironmentVariableResolution::test_env_var_resolution PASSED [ 19%]
tests/integration/registry/test_registry_integration.py::TestEnvironmentVariableResolution::test_env_var_defaults PASSED [ 20%]
tests/integration/registry/test_registry_integration.py::TestServiceLifecycle::test_context_manager_cleanup PASSED [ 20%]
tests/integration/registry/test_registry_integration.py::TestServiceLifecycle::test_reload_service PASSED [ 21%]
tests/integration/registry/test_registry_integration.py::TestServiceLifecycle::test_shutdown_cleanup PASSED [ 21%]
tests/integration/registry/test_registry_integration.py::TestServiceSharing::test_service_instance_sharing PASSED [ 22%]
tests/integration/registry/test_registry_integration.py::TestServiceSharing::test_service_sharing_memory_efficiency PASSED [ 22%]
tests/integration/registry/test_registry_integration.py::TestErrorHandling::test_invalid_service_config FAILED [ 23%]
tests/integration/registry/test_registry_integration.py::TestErrorHandling::test_missing_service_file PASSED [ 23%]
tests/integration/registry/test_registry_integration.py::TestOpenAIServices::test_openai_llm_instantiation PASSED [ 24%]
tests/integration/registry/test_registry_integration.py::TestOpenAIServices::test_openai_embedding_instantiation PASSED [ 25%]
tests/integration/registry/test_registry_integration.py::TestConfigurationValidation::test_valid_configuration_loads PASSED [ 25%]
tests/integration/registry/test_registry_integration.py::TestConfigurationValidation::test_configuration_warnings FAILED [ 26%]
tests/integration/strategies/test_hierarchical_integration.py::TestHierarchicalIntegration::test_end_to_end_workflow PASSED [ 26%]
tests/integration/strategies/test_hierarchical_integration.py::TestHierarchicalIntegration::test_expansion_strategy_comparison PASSED [ 27%]
tests/integration/strategies/test_hierarchical_integration.py::TestHierarchicalIntegration::test_hierarchy_validation PASSED [ 27%]
tests/integration/strategies/test_hierarchical_integration.py::TestHierarchicalIntegration::test_multiple_documents PASSED [ 28%]
tests/integration/strategies/test_late_chunking_integration.py::test_late_chunking_workflow PASSED [ 28%]
tests/integration/strategies/test_late_chunking_integration.py::test_fixed_size_chunking_integration PASSED [ 29%]
tests/integration/strategies/test_late_chunking_integration.py::test_adaptive_chunking_integration PASSED [ 29%]
tests/integration/strategies/test_late_chunking_integration.py::test_multiple_documents PASSED [ 30%]
tests/integration/strategies/test_late_chunking_integration.py::test_strategy_properties PASSED [ 30%]
tests/integration/strategies/test_late_chunking_integration.py::test_coherence_scores_computed PASSED [ 31%]
tests/integration/strategies/test_late_chunking_integration.py::test_short_document PASSED [ 31%]
tests/integration/strategies/test_late_chunking_integration.py::test_chunk_embeddings_valid PASSED [ 32%]
tests/integration/strategies/test_late_chunking_integration.py::test_embedding_quality PASSED [ 32%]
tests/integration/strategies/test_self_reflective_integration.py::TestSelfReflectiveIntegration::test_end_to_end_workflow FAILED [ 33%]
tests/integration/strategies/test_self_reflective_integration.py::TestSelfReflectiveIntegration::test_retry_with_poor_results FAILED [ 34%]
tests/integration/strategies/test_self_reflective_integration.py::TestSelfReflectiveIntegration::test_performance_within_limits FAILED [ 34%]
tests/integration/strategies/test_self_reflective_integration.py::TestSelfReflectiveWithLMStudio::test_with_real_llm FAILED [ 35%]
tests/integration/test_package_integration.py::TestPackageInstallation::test_package_installable PASSED [ 35%]
tests/integration/test_package_integration.py::TestSmokeTest::test_basic_usage_smoke_test FAILED [ 36%]
tests/integration/test_package_integration.py::TestFullWorkflow::test_full_workflow_with_installed_package FAILED [ 36%]
tests/integration/test_package_integration.py::TestBuildAndDistribution::test_package_can_be_built SKIPPED [ 37%]
tests/unit/config/test_strategy_pair_manager.py::test_load_pair_success FAILED [ 37%]
tests/unit/config/test_strategy_pair_manager.py::test_load_pair_compatibility_error PASSED [ 38%]
tests/unit/config/test_strategy_pair_manager.py::test_migration_error PASSED [ 38%]
tests/unit/config/test_strategy_pair_manager.py::test_db_context_creation FAILED [ 39%]
tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_upgrade_to_head FAILED [ 39%]
tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_downgrade FAILED [ 40%]
tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_history PASSED [ 40%]
tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_idempotency FAILED [ 41%]
tests/unit/database/test_migrations.py::TestAlembicMigrations::test_get_current_version FAILED [ 42%]
tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_creates_tables FAILED [ 42%]
tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_creates_indexes FAILED [ 43%]
tests/unit/documentation/test_code_examples.py::TestCodeExamples::test_all_code_examples_have_valid_syntax FAILED [ 43%]
tests/unit/documentation/test_code_examples.py::TestCodeExamples::test_strategy_examples_have_imports PASSED [ 44%]
tests/unit/documentation/test_code_examples.py::TestCodeExamples::test_configuration_examples_valid SKIPPED [ 44%]
tests/unit/documentation/test_code_examples.py::TestCodeExamples::test_quick_start_example_complete PASSED [ 45%]
tests/unit/documentation/test_code_examples.py::TestCodeExamples::test_no_placeholder_code PASSED [ 45%]
tests/unit/documentation/test_links.py::TestDocumentationLinks::test_no_broken_internal_links FAILED [ 46%]
tests/unit/documentation/test_links.py::TestDocumentationLinks::test_all_diagrams_valid PASSED [ 46%]
tests/unit/documentation/test_links.py::TestDocumentationLinks::test_no_todo_links PASSED [ 47%]
tests/unit/documentation/test_links.py::TestDocumentationLinks::test_external_links_valid SKIPPED [ 47%]
tests/unit/registry/test_service_factory.py::TestServiceTypeDetection::test_is_llm_service PASSED [ 48%]
tests/unit/registry/test_service_factory.py::TestServiceTypeDetection::test_is_llm_service_missing_url PASSED [ 48%]
tests/unit/registry/test_service_factory.py::TestServiceTypeDetection::test_is_llm_service_missing_model PASSED [ 49%]
tests/unit/registry/test_service_factory.py::TestServiceTypeDetection::test_is_embedding_service PASSED [ 50%]
tests/unit/registry/test_service_factory.py::TestServiceTypeDetection::test_is_embedding_service_missing_provider PASSED [ 50%]
tests/unit/registry/test_service_factory.py::TestServiceTypeDetection::test_is_database_service_postgres PASSED [ 51%]
tests/unit/registry/test_service_factory.py::TestServiceTypeDetection::test_is_database_service_neo4j PASSED [ 51%]
tests/unit/registry/test_service_factory.py::TestServiceTypeDetection::test_is_database_service_unknown_type PASSED [ 52%]
tests/unit/registry/test_service_factory.py::TestLLMServiceCreation::test_create_llm_service_openai FAILED [ 52%]
tests/unit/registry/test_service_factory.py::TestLLMServiceCreation::test_create_llm_service_lm_studio PASSED [ 53%]
tests/unit/registry/test_service_factory.py::TestLLMServiceCreation::test_create_llm_service_with_defaults PASSED [ 53%]
tests/unit/registry/test_service_factory.py::TestEmbeddingServiceCreation::test_create_embedding_service_onnx FAILED [ 54%]
tests/unit/registry/test_service_factory.py::TestEmbeddingServiceCreation::test_create_embedding_service_onnx_with_defaults FAILED [ 54%]
tests/unit/registry/test_service_factory.py::TestEmbeddingServiceCreation::test_create_embedding_service_openai FAILED [ 55%]
tests/unit/registry/test_service_factory.py::TestEmbeddingServiceCreation::test_create_embedding_service_cohere_not_implemented PASSED [ 55%]
tests/unit/registry/test_service_factory.py::TestEmbeddingServiceCreation::test_create_embedding_service_unknown_provider PASSED [ 56%]
tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_postgres_with_connection_string FAILED [ 56%]
tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_postgres_with_components FAILED [ 57%]
tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_postgres_with_defaults FAILED [ 57%]
tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_neo4j FAILED [ 58%]
tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_neo4j_with_defaults FAILED [ 59%]
tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_unknown_type PASSED [ 59%]
tests/unit/registry/test_service_factory.py::TestServiceCreation::test_create_service_llm PASSED [ 60%]
tests/unit/registry/test_service_factory.py::TestServiceCreation::test_create_service_embedding PASSED [ 60%]
tests/unit/registry/test_service_factory.py::TestServiceCreation::test_create_service_database PASSED [ 61%]
tests/unit/registry/test_service_factory.py::TestServiceCreation::test_create_service_unknown_type PASSED [ 61%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_initialization PASSED [ 62%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_initialization_with_custom_config PASSED [ 62%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_get_embeddings_single_text PASSED [ 63%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_get_embeddings_multiple_texts PASSED [ 63%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_get_embeddings_empty_list PASSED [ 64%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_embedding_normalization PASSED [ 64%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_get_dimensions PASSED [ 65%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_get_max_batch_size PASSED [ 65%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_get_model_name PASSED [ 66%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_calculate_cost PASSED [ 67%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_known_models PASSED [ 67%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_missing_onnx_runtime PASSED [ 68%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_model_loading_failure PASSED [ 68%]
tests/unit/services/embeddings/test_onnx_local.py::TestONNXLocalProvider::test_session_creation_failure PASSED [ 69%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_provider_not_available_raises_error PASSED [ 69%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_provider_initialization PASSED [ 70%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_get_embeddings PASSED [ 70%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_calculate_cost_is_zero FAILED [ 71%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_known_model_dimensions PASSED [ 71%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_unknown_model_uses_output_shape PASSED [ 72%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_custom_batch_size PASSED [ 72%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_model_loading_failure PASSED [ 73%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_get_model_name PASSED [ 73%]
tests/unit/services/embedding/test_onnx_local_provider.py::test_mean_pooling PASSED [ 74%]
tests/unit/strategies/indexing/test_context_aware.py::test_capabilities FAILED [ 75%]
tests/unit/strategies/indexing/test_context_aware.py::test_requirements PASSED [ 75%]
tests/unit/strategies/indexing/test_context_aware.py::test_split_into_sentences PASSED [ 76%]
tests/unit/strategies/indexing/test_context_aware.py::test_create_windows PASSED [ 76%]
tests/unit/strategies/indexing/test_context_aware.py::test_find_boundaries PASSED [ 77%]
tests/unit/strategies/indexing/test_context_aware.py::test_create_chunks_respects_boundaries PASSED [ 77%]
tests/unit/strategies/indexing/test_context_aware.py::test_create_chunks_respects_min_size PASSED [ 78%]
tests/unit/strategies/indexing/test_context_aware.py::test_create_chunks_respects_max_size PASSED [ 78%]
tests/unit/strategies/indexing/test_context_aware.py::test_process_flow PASSED [ 79%]
tests/unit/strategies/self_reflective/test_strategy.py::test_retrieve_good_results_no_retry PASSED [ 79%]
tests/unit/strategies/self_reflective/test_strategy.py::test_retrieve_poor_results_triggers_retry PASSED [ 80%]
tests/unit/strategies/self_reflective/test_strategy.py::test_max_retries_enforced PASSED [ 80%]
tests/unit/strategies/self_reflective/test_strategy.py::test_result_aggregation PASSED [ 81%]
tests/unit/strategies/self_reflective/test_strategy.py::test_timeout_protection PASSED [ 81%]
tests/unit/strategies/self_reflective/test_strategy.py::test_same_query_prevention PASSED [ 82%]
tests/unit/strategies/self_reflective/test_strategy.py::test_normalize_results PASSED [ 82%]
tests/unit/strategies/self_reflective/test_strategy.py::test_strategy_properties PASSED [ 83%]
tests/unit/test_package.py::TestImports::test_import_main_package PASSED [ 84%]
tests/unit/test_package.py::TestImports::test_import_factory FAILED      [ 84%]
tests/unit/test_package.py::TestImports::test_import_pipeline FAILED     [ 85%]
tests/unit/test_package.py::TestImports::test_import_config FAILED       [ 85%]
tests/unit/test_package.py::TestImports::test_import_base_strategy PASSED [ 86%]
tests/unit/test_package.py::TestImports::test_import_all_exports PASSED  [ 86%]
tests/unit/test_package.py::TestVersion::test_version_format PASSED      [ 87%]
tests/unit/test_package.py::TestVersion::test_version_accessible PASSED  [ 87%]
tests/unit/test_package.py::TestPackageStructure::test_strategies_subpackage_exists PASSED [ 88%]
tests/unit/test_package.py::TestPackageStructure::test_no_circular_imports FAILED [ 88%]
tests/unit/test_package.py::TestDependencies::test_required_dependencies_installed PASSED [ 89%]
tests/unit/test_package.py::TestDependencies::test_optional_dependencies_handled FAILED [ 89%]
tests/integration/database/test_migration_integration.py::TestMigrationIntegration::test_real_migration_execution FAILED [ 90%]
tests/integration/database/test_migration_integration.py::TestMigrationIntegration::test_migration_with_existing_data FAILED [ 90%]
tests/integration/database/test_migration_integration.py::TestMigrationIntegration::test_rollback_functionality FAILED [ 91%]
tests/integration/database/test_migration_integration.py::TestMigrationIntegration::test_pgvector_extension_installed FAILED [ 92%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_with_no_migrations FAILED [ 92%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_with_partial_migrations FAILED [ 93%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_with_all_migrations FAILED [ 93%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_or_raise_success FAILED [ 94%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_or_raise_failure FAILED [ 94%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_get_current_revision FAILED [ 95%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_get_all_revisions PASSED [ 95%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_is_at_head FAILED [ 96%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_error_message_details FAILED [ 96%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_single_migration FAILED [ 97%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_nonexistent_migration FAILED [ 97%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_after_downgrade FAILED [ 98%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_multiple_validators_same_database FAILED [ 98%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorEdgeCases::test_validator_with_auto_discovered_config PASSED [ 99%]
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorEdgeCases::test_validate_empty_requirements PASSED [100%]

=================================== FAILURES ===================================
________________________ test_store_and_retrieve_chunks ________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7405fe269970>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x7405fe269850>
sample_texts = ['The quick brown fox jumps over the lazy dog', 'A fast auburn canine leaps above a sleepy hound', 'Python is a high-l... is a subset of artificial intelligence', 'Natural language processing enables computers to understand human language']

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.asyncio
    async def test_store_and_retrieve_chunks(real_db_service, real_embedding_service, sample_texts):
        """Test storing and retrieving chunks with real embeddings."""
        # Generate real embeddings
        chunks = []
        for i, text in enumerate(sample_texts[:3]):  # Use first 3 texts
            embedding = await real_embedding_service.embed(text)
            chunk = Chunk(
                chunk_id=f"test_chunk_{i}",
                text=text,
                embedding=embedding,
                metadata={"source": f"test_{i}.txt", "index": i}
            )
            chunks.append(chunk)
    
        # Store chunks
>       await real_db_service.store_chunks(chunks)

tests/integration_real/test_database_real.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7405fe269970>
chunks = [<Chunk(id=test_chunk_0, doc_id=None, index=None, embedding=yes, text='The quick brown fox jumps over the lazy dog')>,..., <Chunk(id=test_chunk_2, doc_id=None, index=None, embedding=yes, text='Python is a high-level programming language')>]

    async def store_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """Store document chunks.
    
        Args:
            chunks: List of chunk dictionaries. Each chunk should contain
                   at minimum: text content and embedding vector. Additional
                   fields like metadata, chunk_id, etc. are implementation-specific.
    
        Raises:
            Exception: If storage fails
        """
        if not chunks:
            return
    
        pool = await self._get_pool()
    
        async with pool.acquire() as conn:
            # Prepare data for insertion
            for chunk in chunks:
                # Handle both Chunk objects and dictionaries
                if hasattr(chunk, '__dataclass_fields__'):
                    # It's a Chunk dataclass object
                    chunk_id = getattr(chunk, 'chunk_id', getattr(chunk, 'id', None))
                    text = getattr(chunk, 'text', '')
                    embedding = getattr(chunk, 'embedding', [])
                    metadata = getattr(chunk, 'metadata', {})
                else:
                    # It's a dictionary
>                   chunk_id = chunk.get("chunk_id", chunk.get("id"))
                               ^^^^^^^^^
E                   AttributeError: 'Chunk' object has no attribute 'get'

rag_factory/services/database/postgres.py:203: AttributeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.services.database.postgres:Ensured table test_chunks_real exists
------------------------------ Captured log call -------------------------------
INFO     rag_factory.services.database.postgres:postgres.py:173 Ensured table test_chunks_real exists
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Closed PostgreSQL async connection pool
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:514 Closed PostgreSQL async connection pool
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
________________________ test_vector_similarity_search _________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7406011f1220>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x7406011f3680>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.asyncio
    async def test_vector_similarity_search(real_db_service, real_embedding_service):
        """Test actual vector similarity search with real embeddings."""
        # Prepare test data with semantically related texts
        texts = [
            "The quick brown fox jumps over the lazy dog",
            "A fast auburn canine leaps above a sleepy hound",  # Similar to first
            "Python is a programming language",  # Different topic
        ]
    
        # Generate and store chunks
        chunks = []
        for i, text in enumerate(texts):
            embedding = await real_embedding_service.embed(text)
            chunk = Chunk(
                chunk_id=f"similarity_test_{i}",
                text=text,
                embedding=embedding,
                metadata={"source": "similarity_test.txt"}
            )
            chunks.append(chunk)
    
>       await real_db_service.store_chunks(chunks)

tests/integration_real/test_database_real.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7406011f1220>
chunks = [<Chunk(id=similarity_test_0, doc_id=None, index=None, embedding=yes, text='The quick brown fox jumps over the lazy do...und')>, <Chunk(id=similarity_test_2, doc_id=None, index=None, embedding=yes, text='Python is a programming language')>]

    async def store_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """Store document chunks.
    
        Args:
            chunks: List of chunk dictionaries. Each chunk should contain
                   at minimum: text content and embedding vector. Additional
                   fields like metadata, chunk_id, etc. are implementation-specific.
    
        Raises:
            Exception: If storage fails
        """
        if not chunks:
            return
    
        pool = await self._get_pool()
    
        async with pool.acquire() as conn:
            # Prepare data for insertion
            for chunk in chunks:
                # Handle both Chunk objects and dictionaries
                if hasattr(chunk, '__dataclass_fields__'):
                    # It's a Chunk dataclass object
                    chunk_id = getattr(chunk, 'chunk_id', getattr(chunk, 'id', None))
                    text = getattr(chunk, 'text', '')
                    embedding = getattr(chunk, 'embedding', [])
                    metadata = getattr(chunk, 'metadata', {})
                else:
                    # It's a dictionary
>                   chunk_id = chunk.get("chunk_id", chunk.get("id"))
                               ^^^^^^^^^
E                   AttributeError: 'Chunk' object has no attribute 'get'

rag_factory/services/database/postgres.py:203: AttributeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.services.database.postgres:Ensured table test_chunks_real exists
------------------------------ Captured log call -------------------------------
INFO     rag_factory.services.database.postgres:postgres.py:173 Ensured table test_chunks_real exists
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Closed PostgreSQL async connection pool
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:514 Closed PostgreSQL async connection pool
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
_______________________ test_batch_embedding_and_storage _______________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7406011f2690>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x7406011f2300>
sample_texts = ['The quick brown fox jumps over the lazy dog', 'A fast auburn canine leaps above a sleepy hound', 'Python is a high-l... is a subset of artificial intelligence', 'Natural language processing enables computers to understand human language']

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.asyncio
    async def test_batch_embedding_and_storage(real_db_service, real_embedding_service, sample_texts):
        """Test batch embedding and storage operations."""
        # Generate embeddings in batch
        embeddings = await real_embedding_service.embed_batch(sample_texts)
    
        assert len(embeddings) == len(sample_texts)
    
        # Create chunks
        chunks = [
            Chunk(
                chunk_id=f"batch_test_{i}",
                text=text,
                embedding=embedding,
                metadata={"batch": True, "index": i}
            )
            for i, (text, embedding) in enumerate(zip(sample_texts, embeddings))
        ]
    
        # Store all chunks
>       await real_db_service.store_chunks(chunks)

tests/integration_real/test_database_real.py:149: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7406011f2690>
chunks = [<Chunk(id=batch_test_0, doc_id=None, index=None, embedding=yes, text='The quick brown fox jumps over the lazy dog')>,...d=batch_test_4, doc_id=None, index=None, embedding=yes, text='Natural language processing enables computers to u...')>]

    async def store_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """Store document chunks.
    
        Args:
            chunks: List of chunk dictionaries. Each chunk should contain
                   at minimum: text content and embedding vector. Additional
                   fields like metadata, chunk_id, etc. are implementation-specific.
    
        Raises:
            Exception: If storage fails
        """
        if not chunks:
            return
    
        pool = await self._get_pool()
    
        async with pool.acquire() as conn:
            # Prepare data for insertion
            for chunk in chunks:
                # Handle both Chunk objects and dictionaries
                if hasattr(chunk, '__dataclass_fields__'):
                    # It's a Chunk dataclass object
                    chunk_id = getattr(chunk, 'chunk_id', getattr(chunk, 'id', None))
                    text = getattr(chunk, 'text', '')
                    embedding = getattr(chunk, 'embedding', [])
                    metadata = getattr(chunk, 'metadata', {})
                else:
                    # It's a dictionary
>                   chunk_id = chunk.get("chunk_id", chunk.get("id"))
                               ^^^^^^^^^
E                   AttributeError: 'Chunk' object has no attribute 'get'

rag_factory/services/database/postgres.py:203: AttributeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.services.database.postgres:Ensured table test_chunks_real exists
------------------------------ Captured log call -------------------------------
INFO     rag_factory.services.database.postgres:postgres.py:173 Ensured table test_chunks_real exists
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Closed PostgreSQL async connection pool
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:514 Closed PostgreSQL async connection pool
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
_______________________ test_chunk_metadata_persistence ________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7406011f38c0>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x740600e74bc0>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.asyncio
    async def test_chunk_metadata_persistence(real_db_service, real_embedding_service):
        """Test that chunk metadata is correctly stored and retrieved."""
        # Create chunk with rich metadata
        text = "Test document with metadata"
        embedding = await real_embedding_service.embed(text)
    
        metadata = {
            "source": "test.pdf",
            "page": 42,
            "author": "Test Author",
            "tags": ["test", "metadata"],
            "score": 0.95
        }
    
        chunk = Chunk(
            chunk_id="metadata_test",
            text=text,
            embedding=embedding,
            metadata=metadata
        )
    
>       await real_db_service.store_chunks([chunk])

tests/integration_real/test_database_real.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7406011f38c0>
chunks = [<Chunk(id=metadata_test, doc_id=None, index=None, embedding=yes, text='Test document with metadata')>]

    async def store_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """Store document chunks.
    
        Args:
            chunks: List of chunk dictionaries. Each chunk should contain
                   at minimum: text content and embedding vector. Additional
                   fields like metadata, chunk_id, etc. are implementation-specific.
    
        Raises:
            Exception: If storage fails
        """
        if not chunks:
            return
    
        pool = await self._get_pool()
    
        async with pool.acquire() as conn:
            # Prepare data for insertion
            for chunk in chunks:
                # Handle both Chunk objects and dictionaries
                if hasattr(chunk, '__dataclass_fields__'):
                    # It's a Chunk dataclass object
                    chunk_id = getattr(chunk, 'chunk_id', getattr(chunk, 'id', None))
                    text = getattr(chunk, 'text', '')
                    embedding = getattr(chunk, 'embedding', [])
                    metadata = getattr(chunk, 'metadata', {})
                else:
                    # It's a dictionary
>                   chunk_id = chunk.get("chunk_id", chunk.get("id"))
                               ^^^^^^^^^
E                   AttributeError: 'Chunk' object has no attribute 'get'

rag_factory/services/database/postgres.py:203: AttributeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.services.database.postgres:Ensured table test_chunks_real exists
------------------------------ Captured log call -------------------------------
INFO     rag_factory.services.database.postgres:postgres.py:173 Ensured table test_chunks_real exists
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Closed PostgreSQL async connection pool
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:514 Closed PostgreSQL async connection pool
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
_____________________ test_database_context_table_mapping ______________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740600e75370>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.asyncio
    async def test_database_context_table_mapping(real_db_service):
        """Test DatabaseContext with custom table/field mappings."""
        # Get a database context with custom table name
>       context = real_db_service.get_context(table_name="custom_chunks_test")
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: PostgresqlDatabaseService.get_context() got an unexpected keyword argument 'table_name'

tests/integration_real/test_database_real.py:200: TypeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
_______________________ test_document_indexing_pipeline ________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740600e77980>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x740600f91d60>
sample_documents = [{'content': 'The quick brown fox jumps over the lazy dog', 'metadata': {'category': 'animals', 'source': 'test1.txt'}...data and make predictions without being explicitly programmed', 'metadata': {'category': 'ai', 'source': 'test3.txt'}}]

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.asyncio
    async def test_document_indexing_pipeline(real_db_service, real_embedding_service, sample_documents):
        """Test complete document indexing pipeline."""
        from rag_factory.strategies.indexing.vector_embedding import VectorEmbeddingIndexing
        from rag_factory.strategies.base import StrategyConfig
        from rag_factory.services.dependencies import StrategyDependencies
    
        # Create indexing strategy
>       config = StrategyConfig(
            name="test_indexing",
            chunk_size=200,
            chunk_overlap=50
        )
E       TypeError: StrategyConfig.__init__() got an unexpected keyword argument 'name'

tests/integration_real/test_end_to_end_real.py:23: TypeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
___________________________ test_retrieval_pipeline ____________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740600f90b30>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x740600f909b0>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.asyncio
    async def test_retrieval_pipeline(real_db_service, real_embedding_service):
        """Test complete retrieval pipeline."""
        from rag_factory.strategies.retrieval.semantic_retriever import SemanticRetriever
        from rag_factory.strategies.base import StrategyConfig
        from rag_factory.services.dependencies import StrategyDependencies
    
        # First, index some documents
        texts = [
            "Python is a high-level programming language.",
            "Machine learning is a subset of artificial intelligence.",
            "The Eiffel Tower is located in Paris, France."
        ]
    
        chunks = []
        for i, text in enumerate(texts):
            embedding = await real_embedding_service.embed(text)
            chunk = Chunk(
                chunk_id=f"retrieval_test_{i}",
                text=text,
                embedding=embedding,
                metadata={"source": f"doc{i}.txt"}
            )
            chunks.append(chunk)
    
>       await real_db_service.store_chunks(chunks)

tests/integration_real/test_end_to_end_real.py:77: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740600f90b30>
chunks = [<Chunk(id=retrieval_test_0, doc_id=None, index=None, embedding=yes, text='Python is a high-level programming language...nk(id=retrieval_test_2, doc_id=None, index=None, embedding=yes, text='The Eiffel Tower is located in Paris, France.')>]

    async def store_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """Store document chunks.
    
        Args:
            chunks: List of chunk dictionaries. Each chunk should contain
                   at minimum: text content and embedding vector. Additional
                   fields like metadata, chunk_id, etc. are implementation-specific.
    
        Raises:
            Exception: If storage fails
        """
        if not chunks:
            return
    
        pool = await self._get_pool()
    
        async with pool.acquire() as conn:
            # Prepare data for insertion
            for chunk in chunks:
                # Handle both Chunk objects and dictionaries
                if hasattr(chunk, '__dataclass_fields__'):
                    # It's a Chunk dataclass object
                    chunk_id = getattr(chunk, 'chunk_id', getattr(chunk, 'id', None))
                    text = getattr(chunk, 'text', '')
                    embedding = getattr(chunk, 'embedding', [])
                    metadata = getattr(chunk, 'metadata', {})
                else:
                    # It's a dictionary
>                   chunk_id = chunk.get("chunk_id", chunk.get("id"))
                               ^^^^^^^^^
E                   AttributeError: 'Chunk' object has no attribute 'get'

rag_factory/services/database/postgres.py:203: AttributeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.services.database.postgres:Ensured table test_chunks_real exists
------------------------------ Captured log call -------------------------------
INFO     rag_factory.services.database.postgres:postgres.py:173 Ensured table test_chunks_real exists
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Closed PostgreSQL async connection pool
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:514 Closed PostgreSQL async connection pool
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
____________________________ test_full_rag_pipeline ____________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740600e75c70>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x740600e74ce0>
real_llm_service = <rag_factory.services.llm.service.LLMService object at 0x740603a97ef0>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.requires_llm
    @pytest.mark.asyncio
    async def test_full_rag_pipeline(real_db_service, real_embedding_service, real_llm_service):
        """Test complete RAG pipeline: indexing -> retrieval -> generation."""
        from rag_factory.strategies.indexing.vector_embedding import VectorEmbeddingIndexing
        from rag_factory.strategies.retrieval.semantic_retriever import SemanticRetriever
        from rag_factory.strategies.base import StrategyConfig
        from rag_factory.services.dependencies import StrategyDependencies
        from rag_factory.services.llm.base import Message, MessageRole
    
        # Step 1: Index documents
        documents = [
>           Document(
                content="The Eiffel Tower was built in 1889 by Gustave Eiffel. It is located in Paris, France and stands 330 meters tall.",
                metadata={"source": "eiffel.txt"}
            ),
            Document(
                content="Python is a high-level programming language created by Guido van Rossum in 1991. It is known for its simplicity and readability.",
                metadata={"source": "python.txt"}
            )
        ]

tests/integration_real/test_end_to_end_real.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:4: in __init__
    ???
venv/lib/python3.12/site-packages/sqlalchemy/orm/state.py:566: in _initialize_instance
    with util.safe_reraise():
venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:146: in __exit__
    raise exc_value.with_traceback(exc_tb)
venv/lib/python3.12/site-packages/sqlalchemy/orm/state.py:564: in _initialize_instance
    manager.original_init(*mixed[1:], **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Document(id=None, filename='None', status='None', chunks=None)>
kwargs = {'content': 'The Eiffel Tower was built in 1889 by Gustave Eiffel. It is located in Paris, France and stands 330 meters tall.', 'metadata': {'source': 'eiffel.txt'}}
cls_ = <class 'rag_factory.database.models.Document'>, k = 'content'

    def _declarative_constructor(self: Any, **kwargs: Any) -> None:
        """A simple constructor that allows initialization from kwargs.
    
        Sets attributes on the constructed instance using the names and
        values in ``kwargs``.
    
        Only keys that are present as
        attributes of the instance's class are allowed. These could be,
        for example, any mapped columns or relationships.
        """
        cls_ = type(self)
        for k in kwargs:
            if not hasattr(cls_, k):
>               raise TypeError(
                    "%r is an invalid keyword argument for %s" % (k, cls_.__name__)
                )
E               TypeError: 'content' is an invalid keyword argument for Document

venv/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py:2142: TypeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 192.168.56.1:1234
DEBUG:urllib3.connectionpool:http://192.168.56.1:1234 "GET /v1/models HTTP/1.1" 200 1082
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
DEBUG    urllib3.connectionpool:connectionpool.py:241 Starting new HTTP connection (1): 192.168.56.1:1234
DEBUG    urllib3.connectionpool:connectionpool.py:544 http://192.168.56.1:1234 "GET /v1/models HTTP/1.1" 200 1082
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
________________________ test_multiple_document_batches ________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740603303e00>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x7406033001a0>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.asyncio
    async def test_multiple_document_batches(real_db_service, real_embedding_service):
        """Test indexing multiple batches of documents."""
        from rag_factory.strategies.indexing.vector_embedding import VectorEmbeddingIndexing
        from rag_factory.strategies.base import StrategyConfig
        from rag_factory.services.dependencies import StrategyDependencies
    
>       config = StrategyConfig(name="batch_test", chunk_size=100, chunk_overlap=20)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: StrategyConfig.__init__() got an unexpected keyword argument 'name'

tests/integration_real/test_end_to_end_real.py:180: TypeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
____________________ test_retrieval_with_metadata_filtering ____________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740603302690>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x740603300e90>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.asyncio
    async def test_retrieval_with_metadata_filtering(real_db_service, real_embedding_service):
        """Test retrieval with metadata filtering."""
        # Store chunks with different categories
        categories = ["science", "history", "technology"]
    
        for category in categories:
            for i in range(3):
                text = f"This is a {category} document number {i}"
                embedding = await real_embedding_service.embed(text)
                chunk = Chunk(
                    chunk_id=f"{category}_{i}",
                    text=text,
                    embedding=embedding,
                    metadata={"category": category, "index": i}
                )
>               await real_db_service.store_chunks([chunk])

tests/integration_real/test_end_to_end_real.py:229: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740603302690>
chunks = [<Chunk(id=science_0, doc_id=None, index=None, embedding=yes, text='This is a science document number 0')>]

    async def store_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """Store document chunks.
    
        Args:
            chunks: List of chunk dictionaries. Each chunk should contain
                   at minimum: text content and embedding vector. Additional
                   fields like metadata, chunk_id, etc. are implementation-specific.
    
        Raises:
            Exception: If storage fails
        """
        if not chunks:
            return
    
        pool = await self._get_pool()
    
        async with pool.acquire() as conn:
            # Prepare data for insertion
            for chunk in chunks:
                # Handle both Chunk objects and dictionaries
                if hasattr(chunk, '__dataclass_fields__'):
                    # It's a Chunk dataclass object
                    chunk_id = getattr(chunk, 'chunk_id', getattr(chunk, 'id', None))
                    text = getattr(chunk, 'text', '')
                    embedding = getattr(chunk, 'embedding', [])
                    metadata = getattr(chunk, 'metadata', {})
                else:
                    # It's a dictionary
>                   chunk_id = chunk.get("chunk_id", chunk.get("id"))
                               ^^^^^^^^^
E                   AttributeError: 'Chunk' object has no attribute 'get'

rag_factory/services/database/postgres.py:203: AttributeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.services.database.postgres:Ensured table test_chunks_real exists
------------------------------ Captured log call -------------------------------
INFO     rag_factory.services.database.postgres:postgres.py:173 Ensured table test_chunks_real exists
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Closed PostgreSQL async connection pool
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:514 Closed PostgreSQL async connection pool
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
_________________________ test_large_document_indexing _________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740603302930>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x740603302210>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.slow
    @pytest.mark.asyncio
    async def test_large_document_indexing(real_db_service, real_embedding_service):
        """Test indexing a large document."""
        from rag_factory.strategies.indexing.vector_embedding import VectorEmbeddingIndexing
        from rag_factory.strategies.base import StrategyConfig
        from rag_factory.services.dependencies import StrategyDependencies
    
        # Create a large document
        large_text = " ".join([f"This is sentence number {i}." for i in range(1000)])
    
>       document = Document(
            content=large_text,
            metadata={"source": "large_doc.txt", "size": "large"}
        )

tests/integration_real/test_end_to_end_real.py:253: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:4: in __init__
    ???
venv/lib/python3.12/site-packages/sqlalchemy/orm/state.py:566: in _initialize_instance
    with util.safe_reraise():
venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:146: in __exit__
    raise exc_value.with_traceback(exc_tb)
venv/lib/python3.12/site-packages/sqlalchemy/orm/state.py:564: in _initialize_instance
    manager.original_init(*mixed[1:], **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Document(id=None, filename='None', status='None', chunks=None)>
kwargs = {'content': 'This is sentence number 0. This is sentence number 1. This is sentence number 2. This is sentence number .... This is sentence number 998. This is sentence number 999.', 'metadata': {'size': 'large', 'source': 'large_doc.txt'}}
cls_ = <class 'rag_factory.database.models.Document'>, k = 'content'

    def _declarative_constructor(self: Any, **kwargs: Any) -> None:
        """A simple constructor that allows initialization from kwargs.
    
        Sets attributes on the constructed instance using the names and
        values in ``kwargs``.
    
        Only keys that are present as
        attributes of the instance's class are allowed. These could be,
        for example, any mapped columns or relationships.
        """
        cls_ = type(self)
        for k in kwargs:
            if not hasattr(cls_, k):
>               raise TypeError(
                    "%r is an invalid keyword argument for %s" % (k, cls_.__name__)
                )
E               TypeError: 'content' is an invalid keyword argument for Document

venv/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py:2142: TypeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
___________________________ test_retrieval_accuracy ____________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740603303710>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x740603303800>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.asyncio
    async def test_retrieval_accuracy(real_db_service, real_embedding_service):
        """Test retrieval accuracy with known relevant documents."""
        # Create documents with clear semantic relationships
        documents_data = [
            ("Dogs are domesticated animals that are often kept as pets.", "animals"),
            ("Cats are independent animals that enjoy hunting.", "animals"),
            ("Python is a popular programming language.", "programming"),
            ("JavaScript is used for web development.", "programming"),
            ("The sun is a star at the center of our solar system.", "astronomy"),
        ]
    
        chunks = []
        for i, (text, category) in enumerate(documents_data):
            embedding = await real_embedding_service.embed(text)
            chunk = Chunk(
                chunk_id=f"accuracy_test_{i}",
                text=text,
                embedding=embedding,
                metadata={"category": category}
            )
            chunks.append(chunk)
    
>       await real_db_service.store_chunks(chunks)

tests/integration_real/test_end_to_end_real.py:302: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x740603303710>
chunks = [<Chunk(id=accuracy_test_0, doc_id=None, index=None, embedding=yes, text='Dogs are domesticated animals that are often...ccuracy_test_4, doc_id=None, index=None, embedding=yes, text='The sun is a star at the center of our solar syste...')>]

    async def store_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """Store document chunks.
    
        Args:
            chunks: List of chunk dictionaries. Each chunk should contain
                   at minimum: text content and embedding vector. Additional
                   fields like metadata, chunk_id, etc. are implementation-specific.
    
        Raises:
            Exception: If storage fails
        """
        if not chunks:
            return
    
        pool = await self._get_pool()
    
        async with pool.acquire() as conn:
            # Prepare data for insertion
            for chunk in chunks:
                # Handle both Chunk objects and dictionaries
                if hasattr(chunk, '__dataclass_fields__'):
                    # It's a Chunk dataclass object
                    chunk_id = getattr(chunk, 'chunk_id', getattr(chunk, 'id', None))
                    text = getattr(chunk, 'text', '')
                    embedding = getattr(chunk, 'embedding', [])
                    metadata = getattr(chunk, 'metadata', {})
                else:
                    # It's a dictionary
>                   chunk_id = chunk.get("chunk_id", chunk.get("id"))
                               ^^^^^^^^^
E                   AttributeError: 'Chunk' object has no attribute 'get'

rag_factory/services/database/postgres.py:203: AttributeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.services.database.postgres:Ensured table test_chunks_real exists
------------------------------ Captured log call -------------------------------
INFO     rag_factory.services.database.postgres:postgres.py:173 Ensured table test_chunks_real exists
--------------------------- Captured stderr teardown ---------------------------
INFO:rag_factory.services.database.postgres:Created synchronous SQLAlchemy engine for contexts
INFO:rag_factory.services.database.postgres:Closed PostgreSQL async connection pool
INFO:rag_factory.services.database.postgres:Disposed synchronous SQLAlchemy engine
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
---------------------------- Captured log teardown -----------------------------
INFO     rag_factory.services.database.postgres:postgres.py:429 Created synchronous SQLAlchemy engine for contexts
INFO     rag_factory.services.database.postgres:postgres.py:514 Closed PostgreSQL async connection pool
INFO     rag_factory.services.database.postgres:postgres.py:520 Disposed synchronous SQLAlchemy engine
DEBUG    rag_factory.services.database.postgres:postgres.py:523 Cleared DatabaseContext cache
______________________________ test_llm_streaming ______________________________

real_llm_service = <rag_factory.services.llm.service.LLMService object at 0x740603302300>

    @pytest.mark.real_integration
    @pytest.mark.requires_llm
    @pytest.mark.asyncio
    async def test_llm_streaming(real_llm_service):
        """Test LLM streaming responses."""
        from rag_factory.services.llm.base import Message, MessageRole
    
        messages = [
            Message(
                role=MessageRole.USER,
                content="Count from 1 to 5."
            )
        ]
    
        # Check if service supports streaming
        if hasattr(real_llm_service, 'stream'):
            chunks = []
>           async for chunk in real_llm_service.stream(messages):
E           TypeError: 'async for' requires an object with __aiter__ method, got generator

tests/integration_real/test_llm_real.py:197: TypeError
---------------------------- Captured stderr setup -----------------------------
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 192.168.56.1:1234
DEBUG:urllib3.connectionpool:http://192.168.56.1:1234 "GET /v1/models HTTP/1.1" 200 1082
DEBUG:asyncio:Using selector: EpollSelector
------------------------------ Captured log setup ------------------------------
DEBUG    urllib3.connectionpool:connectionpool.py:241 Starting new HTTP connection (1): 192.168.56.1:1234
DEBUG    urllib3.connectionpool:connectionpool.py:544 http://192.168.56.1:1234 "GET /v1/models HTTP/1.1" 200 1082
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
--------------------------- Captured stderr teardown ---------------------------
DEBUG:httpcore.connection:close.started
DEBUG:httpcore.connection:close.complete
_______ TestRealServiceInstantiation.test_multiple_service_instantiation _______

self = <rag_factory.registry.service_registry.ServiceRegistry object at 0x7406027a4e90>
service_ref = 'embedding_local'

    def get(self, service_ref: str) -> Any:
        """Get or create a service instance.
    
        Args:
            service_ref: Service reference like "$llm1" or "llm1"
    
        Returns:
            Service instance implementing appropriate interface
    
        Raises:
            ServiceNotFoundError: If service not found in registry
            ServiceInstantiationError: If service creation fails
        """
        # Strip $ prefix if present
        service_name = service_ref.lstrip('$')
    
        # Return cached instance if exists
        if service_name in self._instances:
            logger.debug(f"Service '{service_name}' returned from cache")
            return self._instances[service_name]
    
        # Thread-safe instantiation
        with self._locks[service_name]:
            # Double-check after acquiring lock
            if service_name in self._instances:
                return self._instances[service_name]
    
            # Validate service exists
            if 'services' not in self.config:
                raise ServiceNotFoundError(
                    f"No services defined in registry configuration"
                )
    
            if service_name not in self.config['services']:
                available = list(self.config['services'].keys())
                raise ServiceNotFoundError(
                    f"Service '{service_name}' not found in registry. "
                    f"Available services: {available}"
                )
    
            # Get service configuration
            service_config = self.config['services'][service_name]
    
            # Create service instance
            logger.info(f"Instantiating service: {service_name}")
            start_time = time.time()
    
            try:
>               service_instance = self._factory.create_service(
                    service_name,
                    service_config
                )

rag_factory/registry/service_registry.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/registry/service_factory.py:48: in create_service
    return self._create_embedding_service(service_name, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/registry/service_factory.py:127: in _create_embedding_service
    return ONNXEmbeddingService(
rag_factory/services/onnx/embedding.py:68: in __init__
    self._provider = ONNXLocalProvider(config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^
rag_factory/services/embedding/providers/onnx_local.py:116: in __init__
    self.model_path = get_onnx_model_path(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model_name = 'Xenova/all-MiniLM-L6-v2', cache_dir = PosixPath('models')
filename = 'model.onnx'

    def get_onnx_model_path(
        model_name: str,
        cache_dir: Optional[Path] = None,
        filename: str = "model.onnx",
    ) -> Path:
        """Get path to local ONNX model.
    
        This function checks:
        1. Local model paths (if model exists locally)
        2. Local cache directory
    
        Args:
            model_name: Model identifier or local path
            cache_dir: Directory for model caching
            filename: Name of the ONNX model file
    
        Returns:
            Path to ONNX model file
    
        Raises:
            FileNotFoundError: If model is not found locally
        """
        import os
    
        check_dependencies()
    
        # Check for environment variable configuration
        env_model_path = os.getenv("EMBEDDING_MODEL_PATH")
        env_model_name = os.getenv("EMBEDDING_MODEL_NAME")
    
        # Use environment variable for cache_dir if not specified
        if cache_dir is None:
            if env_model_path:
                cache_dir = Path(env_model_path)
            elif Path("models/embeddings").exists():
                cache_dir = Path("models/embeddings").resolve()
                logger.info(f"Using local project cache: {cache_dir}")
            else:
                # Default to standard location, but don't create it if we're not downloading
                cache_dir = Path.home() / ".cache" / "rag_factory" / "onnx_models"
    
        cache_dir = Path(cache_dir)
    
        # Use environment variable for model_name if it matches default
        if env_model_name and model_name == "Xenova/all-MiniLM-L6-v2":
            logger.info(f"Using model from EMBEDDING_MODEL_NAME env: {env_model_name}")
            model_name = env_model_name
    
        # Check if model_name is a local path
        potential_local_path = Path(model_name)
        if potential_local_path.exists() and potential_local_path.is_file():
            logger.info(f"Using local ONNX model: {potential_local_path}")
            return potential_local_path
    
        # Check if model exists in cache directory
        # Try both naming conventions: underscore (old) and double-dash (HF standard)
        model_dir_name_underscore = model_name.replace("/", "_")
        model_dir_name_dash = model_name.replace("/", "--")
    
        for model_dir_name in [model_dir_name_dash, model_dir_name_underscore]:
            local_model_dir = cache_dir / model_dir_name
    
            if local_model_dir.exists():
                # Look for ONNX files in the directory (recursively, Xenova models have onnx/ subdirectory)
                onnx_files = list(local_model_dir.rglob("*.onnx"))
                if onnx_files:
                    # Prefer the main model.onnx if available, otherwise use first one
                    main_model = next((f for f in onnx_files if f.name == "model.onnx"), onnx_files[0])
                    logger.info(f"Using cached ONNX model: {main_model}")
                    return main_model
    
        # Model not found locally
        error_msg = (
            f"ONNX model '{model_name}' not found locally.\n"
            f"Checked path: {cache_dir}\n"
            f"Automatic downloading is disabled. Please ensure the model is available directly in the infrastructure.\n"
            f"Set EMBEDDING_MODEL_PATH environment variable to the directory containing the model."
        )
>       raise FileNotFoundError(error_msg)
E       FileNotFoundError: ONNX model 'Xenova/all-MiniLM-L6-v2' not found locally.
E       Checked path: models
E       Automatic downloading is disabled. Please ensure the model is available directly in the infrastructure.
E       Set EMBEDDING_MODEL_PATH environment variable to the directory containing the model.

rag_factory/services/utils/onnx_utils.py:123: FileNotFoundError

During handling of the above exception, another exception occurred:

self = <tests.integration.registry.test_registry_integration.TestRealServiceInstantiation object at 0x740603b5e150>
test_services_yaml_with_env = '/tmp/pytest-of-admindevmac/pytest-48/test_multiple_service_instanti0/services.yaml'

    def test_multiple_service_instantiation(self, test_services_yaml_with_env):
        """Test instantiating multiple services."""
        registry = ServiceRegistry(test_services_yaml_with_env)
    
        # Get both services
>       embedding = registry.get("embedding_local")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/registry/test_registry_integration.py:95: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.registry.service_registry.ServiceRegistry object at 0x7406027a4e90>
service_ref = 'embedding_local'

    def get(self, service_ref: str) -> Any:
        """Get or create a service instance.
    
        Args:
            service_ref: Service reference like "$llm1" or "llm1"
    
        Returns:
            Service instance implementing appropriate interface
    
        Raises:
            ServiceNotFoundError: If service not found in registry
            ServiceInstantiationError: If service creation fails
        """
        # Strip $ prefix if present
        service_name = service_ref.lstrip('$')
    
        # Return cached instance if exists
        if service_name in self._instances:
            logger.debug(f"Service '{service_name}' returned from cache")
            return self._instances[service_name]
    
        # Thread-safe instantiation
        with self._locks[service_name]:
            # Double-check after acquiring lock
            if service_name in self._instances:
                return self._instances[service_name]
    
            # Validate service exists
            if 'services' not in self.config:
                raise ServiceNotFoundError(
                    f"No services defined in registry configuration"
                )
    
            if service_name not in self.config['services']:
                available = list(self.config['services'].keys())
                raise ServiceNotFoundError(
                    f"Service '{service_name}' not found in registry. "
                    f"Available services: {available}"
                )
    
            # Get service configuration
            service_config = self.config['services'][service_name]
    
            # Create service instance
            logger.info(f"Instantiating service: {service_name}")
            start_time = time.time()
    
            try:
                service_instance = self._factory.create_service(
                    service_name,
                    service_config
                )
                instantiation_time = time.time() - start_time
    
                logger.info(
                    f"Service '{service_name}' instantiated successfully "
                    f"in {instantiation_time:.2f}s"
                )
    
            except Exception as e:
                logger.error(f"Failed to instantiate service '{service_name}': {e}")
>               raise ServiceInstantiationError(
                    f"Service instantiation failed for '{service_name}': {e}"
                )
E               rag_factory.registry.exceptions.ServiceInstantiationError: Service instantiation failed for 'embedding_local': ONNX model 'Xenova/all-MiniLM-L6-v2' not found locally.
E               Checked path: models
E               Automatic downloading is disabled. Please ensure the model is available directly in the infrastructure.
E               Set EMBEDDING_MODEL_PATH environment variable to the directory containing the model.

rag_factory/registry/service_registry.py:150: ServiceInstantiationError
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.registry.service_registry:Loading service registry from: /tmp/pytest-of-admindevmac/pytest-48/test_multiple_service_instanti0/services.yaml
DEBUG:httpcore.connection:close.started
DEBUG:httpcore.connection:close.complete
WARNING:rag_factory.registry.service_registry:WARNING: Potential plaintext secret in services.llm_local.api_key. Consider using environment variable: ${ENV_VAR}
INFO:rag_factory.registry.service_registry:Service registry loaded: 2 services available
INFO:rag_factory.registry.service_registry:Instantiating service: embedding_local
DEBUG:rag_factory.registry.service_factory:Creating embedding service: embedding_local
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
ERROR:rag_factory.registry.service_registry:Failed to instantiate service 'embedding_local': ONNX model 'Xenova/all-MiniLM-L6-v2' not found locally.
Checked path: models
Automatic downloading is disabled. Please ensure the model is available directly in the infrastructure.
Set EMBEDDING_MODEL_PATH environment variable to the directory containing the model.
------------------------------ Captured log call -------------------------------
INFO     rag_factory.registry.service_registry:service_registry.py:55 Loading service registry from: /tmp/pytest-of-admindevmac/pytest-48/test_multiple_service_instanti0/services.yaml
DEBUG    httpcore.connection:_trace.py:47 close.started
DEBUG    httpcore.connection:_trace.py:47 close.complete
WARNING  rag_factory.registry.service_registry:service_registry.py:70 WARNING: Potential plaintext secret in services.llm_local.api_key. Consider using environment variable: ${ENV_VAR}
INFO     rag_factory.registry.service_registry:service_registry.py:75 Service registry loaded: 2 services available
INFO     rag_factory.registry.service_registry:service_registry.py:133 Instantiating service: embedding_local
DEBUG    rag_factory.registry.service_factory:service_factory.py:121 Creating embedding service: embedding_local
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
ERROR    rag_factory.registry.service_registry:service_registry.py:149 Failed to instantiate service 'embedding_local': ONNX model 'Xenova/all-MiniLM-L6-v2' not found locally.
Checked path: models
Automatic downloading is disabled. Please ensure the model is available directly in the infrastructure.
Set EMBEDDING_MODEL_PATH environment variable to the directory containing the model.
________________ TestErrorHandling.test_invalid_service_config _________________

self = <rag_factory.config.validator.ConfigValidator object at 0x7406027a7bc0>
config = {'services': {'invalid_service': {'unknown_field': 'value'}}}
file_path = '/tmp/pytest-of-admindevmac/pytest-48/test_invalid_service_config0/services.yaml'

    def validate_services_yaml(
        self,
        config: Dict[str, Any],
        file_path: Optional[str] = None
    ) -> List[str]:
        """
        Validate services.yaml configuration.
    
        Args:
            config: Parsed YAML configuration
            file_path: Path to YAML file (for error messages)
    
        Returns:
            List of warning messages (empty if no warnings)
    
        Raises:
            ConfigValidationError: If validation fails
    
        Examples:
            >>> validator = ConfigValidator()
            >>> config = {
            ...     "services": {
            ...         "llm1": {
            ...             "name": "test-llm",
            ...             "type": "llm",
            ...             "url": "http://localhost:1234/v1",
            ...             "api_key": "${API_KEY}",
            ...             "model": "test-model"
            ...         }
            ...     }
            ... }
            >>> warnings = validator.validate_services_yaml(config)
            >>> len(warnings)
            0
        """
        try:
            # JSON Schema validation
>           jsonschema.validate(
                instance=config,
                schema=self._schemas["service_registry"]
            )

rag_factory/config/validator.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

instance = {'services': {'invalid_service': {'unknown_field': 'value'}}}
schema = {'$schema': 'http://json-schema.org/draft-07/schema#', 'definitions': {'database_service': {'properties': {'connection...tion': 'Schema version (semver)', 'pattern': '^\\d+\\.\\d+\\.\\d+$', 'type': 'string'}}, 'required': ['services'], ...}
cls = <class 'jsonschema.validators.Draft7Validator'>, args = (), kwargs = {}
validator = Draft7Validator(schema={'$schema': 'http://json-...ft-07/schema#', 'definitions': {'database_service': {'properties': ... (semver)', 'pattern': '^\\d+\\.\\d+\\.\\d+$', 'type': 'string'}}, 'required': ['services'], ...}, format_checker=None)
error = <ValidationError: "{'unknown_field': 'value'} is not valid under any of the given schemas">

    def validate(instance, schema, cls=None, *args, **kwargs):  # noqa: D417
        """
        Validate an instance under the given schema.
    
            >>> validate([2, 3, 4], {"maxItems": 2})
            Traceback (most recent call last):
                ...
            ValidationError: [2, 3, 4] is too long
    
        :func:`~jsonschema.validators.validate` will first verify that the
        provided schema is itself valid, since not doing so can lead to less
        obvious error messages and fail in less obvious or consistent ways.
    
        If you know you have a valid schema already, especially
        if you intend to validate multiple instances with
        the same schema, you likely would prefer using the
        `jsonschema.protocols.Validator.validate` method directly on a
        specific validator (e.g. ``Draft202012Validator.validate``).
    
    
        Arguments:
    
            instance:
    
                The instance to validate
    
            schema:
    
                The schema to validate with
    
            cls (jsonschema.protocols.Validator):
    
                The class that will be used to validate the instance.
    
        If the ``cls`` argument is not provided, two things will happen
        in accordance with the specification. First, if the schema has a
        :kw:`$schema` keyword containing a known meta-schema [#]_ then the
        proper validator will be used. The specification recommends that
        all schemas contain :kw:`$schema` properties for this reason. If no
        :kw:`$schema` property is found, the default validator class is the
        latest released draft.
    
        Any other provided positional and keyword arguments will be passed
        on when instantiating the ``cls``.
    
        Raises:
    
            `jsonschema.exceptions.ValidationError`:
    
                if the instance is invalid
    
            `jsonschema.exceptions.SchemaError`:
    
                if the schema itself is invalid
    
        .. rubric:: Footnotes
        .. [#] known by a validator registered with
            `jsonschema.validators.validates`
    
        """
        if cls is None:
            cls = validator_for(schema)
    
        cls.check_schema(schema)
        validator = cls(schema, *args, **kwargs)
        error = exceptions.best_match(validator.iter_errors(instance))
        if error is not None:
>           raise error
E           jsonschema.exceptions.ValidationError: {'unknown_field': 'value'} is not valid under any of the given schemas
E           
E           Failed validating 'oneOf' in schema['properties']['services']['patternProperties']['^[a-zA-Z0-9_]+$']:
E               {'oneOf': [{'$ref': '#/definitions/llm_service'},
E                          {'$ref': '#/definitions/embedding_service'},
E                          {'$ref': '#/definitions/database_service'}]}
E           
E           On instance['services']['invalid_service']:
E               {'unknown_field': 'value'}

venv/lib/python3.12/site-packages/jsonschema/validators.py:1332: ValidationError

During handling of the above exception, another exception occurred:

self = <rag_factory.registry.service_registry.ServiceRegistry object at 0x7406027a6ed0>

    def _load_config(self) -> None:
        """Load and validate services.yaml configuration."""
        logger.info(f"Loading service registry from: {self.config_path}")
    
        try:
            # Load YAML
            with open(self.config_path, 'r') as f:
                raw_config = yaml.safe_load(f)
    
            # Validate schema
>           warnings = self._validator.validate_services_yaml(
                raw_config,
                file_path=self.config_path
            )

rag_factory/registry/service_registry.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.config.validator.ConfigValidator object at 0x7406027a7bc0>
config = {'services': {'invalid_service': {'unknown_field': 'value'}}}
file_path = '/tmp/pytest-of-admindevmac/pytest-48/test_invalid_service_config0/services.yaml'

    def validate_services_yaml(
        self,
        config: Dict[str, Any],
        file_path: Optional[str] = None
    ) -> List[str]:
        """
        Validate services.yaml configuration.
    
        Args:
            config: Parsed YAML configuration
            file_path: Path to YAML file (for error messages)
    
        Returns:
            List of warning messages (empty if no warnings)
    
        Raises:
            ConfigValidationError: If validation fails
    
        Examples:
            >>> validator = ConfigValidator()
            >>> config = {
            ...     "services": {
            ...         "llm1": {
            ...             "name": "test-llm",
            ...             "type": "llm",
            ...             "url": "http://localhost:1234/v1",
            ...             "api_key": "${API_KEY}",
            ...             "model": "test-model"
            ...         }
            ...     }
            ... }
            >>> warnings = validator.validate_services_yaml(config)
            >>> len(warnings)
            0
        """
        try:
            # JSON Schema validation
            jsonschema.validate(
                instance=config,
                schema=self._schemas["service_registry"]
            )
        except jsonschema.ValidationError as e:
>           raise ConfigValidationError(
                message=f"Schema validation failed: {e.message}",
                file_path=file_path,
                field=".".join(str(p) for p in e.path)
            )
E           rag_factory.config.validator.ConfigValidationError: Schema validation failed: {'unknown_field': 'value'} is not valid under any of the given schemas
E           File: /tmp/pytest-of-admindevmac/pytest-48/test_invalid_service_config0/services.yaml
E           Field: services.invalid_service

rag_factory/config/validator.py:118: ConfigValidationError

During handling of the above exception, another exception occurred:

self = <tests.integration.registry.test_registry_integration.TestErrorHandling object at 0x740603b79e50>
tmp_path = PosixPath('/tmp/pytest-of-admindevmac/pytest-48/test_invalid_service_config0')

        def test_invalid_service_config(self, tmp_path):
            """Test handling of invalid service configuration."""
            content = """
    services:
      invalid_service:
        unknown_field: "value"
    """
            services_file = tmp_path / "services.yaml"
            services_file.write_text(content)
    
>           registry = ServiceRegistry(str(services_file))
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/registry/test_registry_integration.py:230: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/registry/service_registry.py:51: in __init__
    self._load_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.registry.service_registry.ServiceRegistry object at 0x7406027a6ed0>

    def _load_config(self) -> None:
        """Load and validate services.yaml configuration."""
        logger.info(f"Loading service registry from: {self.config_path}")
    
        try:
            # Load YAML
            with open(self.config_path, 'r') as f:
                raw_config = yaml.safe_load(f)
    
            # Validate schema
            warnings = self._validator.validate_services_yaml(
                raw_config,
                file_path=self.config_path
            )
    
            # Print warnings
            for warning in warnings:
                logger.warning(warning)
    
            # Resolve environment variables
            self.config = EnvResolver.resolve(raw_config)
    
            logger.info(
                f"Service registry loaded: "
                f"{len(self.config.get('services', {}))} services available"
            )
    
        except FileNotFoundError:
            raise ServiceInstantiationError(
                f"Service registry configuration not found: {self.config_path}"
            )
        except Exception as e:
>           raise ServiceInstantiationError(
                f"Failed to load service registry: {e}"
            )
E           rag_factory.registry.exceptions.ServiceInstantiationError: Failed to load service registry: Schema validation failed: {'unknown_field': 'value'} is not valid under any of the given schemas
E           File: /tmp/pytest-of-admindevmac/pytest-48/test_invalid_service_config0/services.yaml
E           Field: services.invalid_service

rag_factory/registry/service_registry.py:85: ServiceInstantiationError
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.registry.service_registry:Loading service registry from: /tmp/pytest-of-admindevmac/pytest-48/test_invalid_service_config0/services.yaml
------------------------------ Captured log call -------------------------------
INFO     rag_factory.registry.service_registry:service_registry.py:55 Loading service registry from: /tmp/pytest-of-admindevmac/pytest-48/test_invalid_service_config0/services.yaml
___________ TestConfigurationValidation.test_configuration_warnings ____________

self = <rag_factory.config.validator.ConfigValidator object at 0x740602306c30>
config = {'services': {'test_service': {'api_key': 'plaintext-secret', 'model': 'test-model', 'provider': 'onnx'}}}
file_path = '/tmp/pytest-of-admindevmac/pytest-48/test_configuration_warnings0/services.yaml'

    def validate_services_yaml(
        self,
        config: Dict[str, Any],
        file_path: Optional[str] = None
    ) -> List[str]:
        """
        Validate services.yaml configuration.
    
        Args:
            config: Parsed YAML configuration
            file_path: Path to YAML file (for error messages)
    
        Returns:
            List of warning messages (empty if no warnings)
    
        Raises:
            ConfigValidationError: If validation fails
    
        Examples:
            >>> validator = ConfigValidator()
            >>> config = {
            ...     "services": {
            ...         "llm1": {
            ...             "name": "test-llm",
            ...             "type": "llm",
            ...             "url": "http://localhost:1234/v1",
            ...             "api_key": "${API_KEY}",
            ...             "model": "test-model"
            ...         }
            ...     }
            ... }
            >>> warnings = validator.validate_services_yaml(config)
            >>> len(warnings)
            0
        """
        try:
            # JSON Schema validation
>           jsonschema.validate(
                instance=config,
                schema=self._schemas["service_registry"]
            )

rag_factory/config/validator.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

instance = {'services': {'test_service': {'api_key': 'plaintext-secret', 'model': 'test-model', 'provider': 'onnx'}}}
schema = {'$schema': 'http://json-schema.org/draft-07/schema#', 'definitions': {'database_service': {'properties': {'connection...tion': 'Schema version (semver)', 'pattern': '^\\d+\\.\\d+\\.\\d+$', 'type': 'string'}}, 'required': ['services'], ...}
cls = <class 'jsonschema.validators.Draft7Validator'>, args = (), kwargs = {}
validator = Draft7Validator(schema={'$schema': 'http://json-...ft-07/schema#', 'definitions': {'database_service': {'properties': ... (semver)', 'pattern': '^\\d+\\.\\d+\\.\\d+$', 'type': 'string'}}, 'required': ['services'], ...}, format_checker=None)
error = <ValidationError: "{'provider': 'onnx', 'model': 'test-model', 'api_key': 'plaintext-secret'} is not valid under any of the given schemas">

    def validate(instance, schema, cls=None, *args, **kwargs):  # noqa: D417
        """
        Validate an instance under the given schema.
    
            >>> validate([2, 3, 4], {"maxItems": 2})
            Traceback (most recent call last):
                ...
            ValidationError: [2, 3, 4] is too long
    
        :func:`~jsonschema.validators.validate` will first verify that the
        provided schema is itself valid, since not doing so can lead to less
        obvious error messages and fail in less obvious or consistent ways.
    
        If you know you have a valid schema already, especially
        if you intend to validate multiple instances with
        the same schema, you likely would prefer using the
        `jsonschema.protocols.Validator.validate` method directly on a
        specific validator (e.g. ``Draft202012Validator.validate``).
    
    
        Arguments:
    
            instance:
    
                The instance to validate
    
            schema:
    
                The schema to validate with
    
            cls (jsonschema.protocols.Validator):
    
                The class that will be used to validate the instance.
    
        If the ``cls`` argument is not provided, two things will happen
        in accordance with the specification. First, if the schema has a
        :kw:`$schema` keyword containing a known meta-schema [#]_ then the
        proper validator will be used. The specification recommends that
        all schemas contain :kw:`$schema` properties for this reason. If no
        :kw:`$schema` property is found, the default validator class is the
        latest released draft.
    
        Any other provided positional and keyword arguments will be passed
        on when instantiating the ``cls``.
    
        Raises:
    
            `jsonschema.exceptions.ValidationError`:
    
                if the instance is invalid
    
            `jsonschema.exceptions.SchemaError`:
    
                if the schema itself is invalid
    
        .. rubric:: Footnotes
        .. [#] known by a validator registered with
            `jsonschema.validators.validates`
    
        """
        if cls is None:
            cls = validator_for(schema)
    
        cls.check_schema(schema)
        validator = cls(schema, *args, **kwargs)
        error = exceptions.best_match(validator.iter_errors(instance))
        if error is not None:
>           raise error
E           jsonschema.exceptions.ValidationError: {'provider': 'onnx', 'model': 'test-model', 'api_key': 'plaintext-secret'} is not valid under any of the given schemas
E           
E           Failed validating 'oneOf' in schema['properties']['services']['patternProperties']['^[a-zA-Z0-9_]+$']:
E               {'oneOf': [{'$ref': '#/definitions/llm_service'},
E                          {'$ref': '#/definitions/embedding_service'},
E                          {'$ref': '#/definitions/database_service'}]}
E           
E           On instance['services']['test_service']:
E               {'provider': 'onnx',
E                'model': 'test-model',
E                'api_key': 'plaintext-secret'}

venv/lib/python3.12/site-packages/jsonschema/validators.py:1332: ValidationError

During handling of the above exception, another exception occurred:

self = <rag_factory.registry.service_registry.ServiceRegistry object at 0x740602307f20>

    def _load_config(self) -> None:
        """Load and validate services.yaml configuration."""
        logger.info(f"Loading service registry from: {self.config_path}")
    
        try:
            # Load YAML
            with open(self.config_path, 'r') as f:
                raw_config = yaml.safe_load(f)
    
            # Validate schema
>           warnings = self._validator.validate_services_yaml(
                raw_config,
                file_path=self.config_path
            )

rag_factory/registry/service_registry.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.config.validator.ConfigValidator object at 0x740602306c30>
config = {'services': {'test_service': {'api_key': 'plaintext-secret', 'model': 'test-model', 'provider': 'onnx'}}}
file_path = '/tmp/pytest-of-admindevmac/pytest-48/test_configuration_warnings0/services.yaml'

    def validate_services_yaml(
        self,
        config: Dict[str, Any],
        file_path: Optional[str] = None
    ) -> List[str]:
        """
        Validate services.yaml configuration.
    
        Args:
            config: Parsed YAML configuration
            file_path: Path to YAML file (for error messages)
    
        Returns:
            List of warning messages (empty if no warnings)
    
        Raises:
            ConfigValidationError: If validation fails
    
        Examples:
            >>> validator = ConfigValidator()
            >>> config = {
            ...     "services": {
            ...         "llm1": {
            ...             "name": "test-llm",
            ...             "type": "llm",
            ...             "url": "http://localhost:1234/v1",
            ...             "api_key": "${API_KEY}",
            ...             "model": "test-model"
            ...         }
            ...     }
            ... }
            >>> warnings = validator.validate_services_yaml(config)
            >>> len(warnings)
            0
        """
        try:
            # JSON Schema validation
            jsonschema.validate(
                instance=config,
                schema=self._schemas["service_registry"]
            )
        except jsonschema.ValidationError as e:
>           raise ConfigValidationError(
                message=f"Schema validation failed: {e.message}",
                file_path=file_path,
                field=".".join(str(p) for p in e.path)
            )
E           rag_factory.config.validator.ConfigValidationError: Schema validation failed: {'provider': 'onnx', 'model': 'test-model', 'api_key': 'plaintext-secret'} is not valid under any of the given schemas
E           File: /tmp/pytest-of-admindevmac/pytest-48/test_configuration_warnings0/services.yaml
E           Field: services.test_service

rag_factory/config/validator.py:118: ConfigValidationError

During handling of the above exception, another exception occurred:

self = <tests.integration.registry.test_registry_integration.TestConfigurationValidation object at 0x740603b7a660>
tmp_path = PosixPath('/tmp/pytest-of-admindevmac/pytest-48/test_configuration_warnings0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x740602306de0>

        def test_configuration_warnings(self, tmp_path, caplog):
            """Test that configuration warnings are logged."""
            import logging
            caplog.set_level(logging.WARNING)
    
            content = """
    services:
      test_service:
        provider: "onnx"
        model: "test-model"
        api_key: "plaintext-secret"  # Should trigger warning
    """
            services_file = tmp_path / "services.yaml"
            services_file.write_text(content)
    
>           registry = ServiceRegistry(str(services_file))
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/registry/test_registry_integration.py:332: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
rag_factory/registry/service_registry.py:51: in __init__
    self._load_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <rag_factory.registry.service_registry.ServiceRegistry object at 0x740602307f20>

    def _load_config(self) -> None:
        """Load and validate services.yaml configuration."""
        logger.info(f"Loading service registry from: {self.config_path}")
    
        try:
            # Load YAML
            with open(self.config_path, 'r') as f:
                raw_config = yaml.safe_load(f)
    
            # Validate schema
            warnings = self._validator.validate_services_yaml(
                raw_config,
                file_path=self.config_path
            )
    
            # Print warnings
            for warning in warnings:
                logger.warning(warning)
    
            # Resolve environment variables
            self.config = EnvResolver.resolve(raw_config)
    
            logger.info(
                f"Service registry loaded: "
                f"{len(self.config.get('services', {}))} services available"
            )
    
        except FileNotFoundError:
            raise ServiceInstantiationError(
                f"Service registry configuration not found: {self.config_path}"
            )
        except Exception as e:
>           raise ServiceInstantiationError(
                f"Failed to load service registry: {e}"
            )
E           rag_factory.registry.exceptions.ServiceInstantiationError: Failed to load service registry: Schema validation failed: {'provider': 'onnx', 'model': 'test-model', 'api_key': 'plaintext-secret'} is not valid under any of the given schemas
E           File: /tmp/pytest-of-admindevmac/pytest-48/test_configuration_warnings0/services.yaml
E           Field: services.test_service

rag_factory/registry/service_registry.py:85: ServiceInstantiationError
____________ TestSelfReflectiveIntegration.test_end_to_end_workflow ____________

self = <tests.integration.strategies.test_self_reflective_integration.TestSelfReflectiveIntegration object at 0x740603bd4080>
mock_base_strategy = <Mock id='127569105257776'>
mock_llm_service = <Mock id='127569105257488'>

    def test_end_to_end_workflow(self, mock_base_strategy, mock_llm_service):
        """Test complete self-reflective retrieval workflow."""
        # Create strategy
>       strategy = SelfReflectiveRAGStrategy(
            base_retrieval_strategy=mock_base_strategy,
            llm_service=mock_llm_service,
            config={"grade_threshold": 4.0, "max_retries": 2}
        )
E       TypeError: SelfReflectiveRAGStrategy.__init__() got an unexpected keyword argument 'base_retrieval_strategy'

tests/integration/strategies/test_self_reflective_integration.py:80: TypeError
__________ TestSelfReflectiveIntegration.test_retry_with_poor_results __________

self = <tests.integration.strategies.test_self_reflective_integration.TestSelfReflectiveIntegration object at 0x740603bd4440>
mock_base_strategy = <Mock id='127569105259552'>
mock_llm_service = <Mock id='127569105259744'>

    def test_retry_with_poor_results(self, mock_base_strategy, mock_llm_service):
        """Test that retry is triggered for poor results."""
>       strategy = SelfReflectiveRAGStrategy(
            base_retrieval_strategy=mock_base_strategy,
            llm_service=mock_llm_service,
            config={"grade_threshold": 4.0, "max_retries": 2}
        )
E       TypeError: SelfReflectiveRAGStrategy.__init__() got an unexpected keyword argument 'base_retrieval_strategy'

tests/integration/strategies/test_self_reflective_integration.py:100: TypeError
_________ TestSelfReflectiveIntegration.test_performance_within_limits _________

self = <tests.integration.strategies.test_self_reflective_integration.TestSelfReflectiveIntegration object at 0x740603bd47d0>
mock_base_strategy = <Mock id='127569105261904'>
mock_llm_service = <Mock id='127569105262144'>

    def test_performance_within_limits(self, mock_base_strategy, mock_llm_service):
        """Test that self-reflective retrieval completes within timeout."""
        import time
    
>       strategy = SelfReflectiveRAGStrategy(
            base_retrieval_strategy=mock_base_strategy,
            llm_service=mock_llm_service,
            config={"grade_threshold": 4.0, "max_retries": 2, "timeout_seconds": 10}
        )
E       TypeError: SelfReflectiveRAGStrategy.__init__() got an unexpected keyword argument 'base_retrieval_strategy'

tests/integration/strategies/test_self_reflective_integration.py:119: TypeError
______________ TestSelfReflectiveWithLMStudio.test_with_real_llm _______________

self = <tests.integration.strategies.test_self_reflective_integration.TestSelfReflectiveWithLMStudio object at 0x740603bd48c0>
llm_service_from_env = <rag_factory.services.llm.service.LLMService object at 0x7405ff342060>

    def test_with_real_llm(self, llm_service_from_env):
        """Test with real LLM service from environment (LM Studio)."""
        from unittest.mock import Mock
    
        # Mock base strategy
        base_strategy = Mock()
        base_strategy.retrieve.return_value = [
            {"chunk_id": "c1", "text": "Sample result", "score": 0.9}
        ]
    
        # Create self-reflective strategy
>       strategy = SelfReflectiveRAGStrategy(
            base_retrieval_strategy=base_strategy,
            llm_service=llm_service_from_env,
            config={"grade_threshold": 4.0, "max_retries": 1}
        )
E       TypeError: SelfReflectiveRAGStrategy.__init__() got an unexpected keyword argument 'base_retrieval_strategy'

tests/integration/strategies/test_self_reflective_integration.py:149: TypeError
__________________ TestSmokeTest.test_basic_usage_smoke_test ___________________

self = <tests.integration.test_package_integration.TestSmokeTest object at 0x740603bd5a90>

    def test_basic_usage_smoke_test(self) -> None:
        """Test basic usage works after import."""
>       from rag_factory import RAGFactory, StrategyPipeline, ConfigManager
E       ImportError: cannot import name 'RAGFactory' from 'rag_factory' (/mnt/MCPProyects/ragTools/rag_factory/__init__.py)

tests/integration/test_package_integration.py:60: ImportError
__________ TestFullWorkflow.test_full_workflow_with_installed_package __________

self = <tests.integration.test_package_integration.TestFullWorkflow object at 0x740603bd5df0>

    def test_full_workflow_with_installed_package(self) -> None:
        """Test complete workflow using installed package."""
>       from rag_factory import RAGFactory, StrategyPipeline
E       ImportError: cannot import name 'RAGFactory' from 'rag_factory' (/mnt/MCPProyects/ragTools/rag_factory/__init__.py)

tests/integration/test_package_integration.py:90: ImportError
____________________________ test_load_pair_success ____________________________

manager = <rag_factory.config.strategy_pair_manager.StrategyPairManager object at 0x7405ff2d6270>
mock_loader = <MagicMock name='StrategyPairLoader()' id='127569159330496'>
mock_registry = <MagicMock spec='ServiceRegistry' id='127569105102960'>

    def test_load_pair_success(manager, mock_loader, mock_registry):
        # Setup Loader return
        mock_config = {
            "strategy_name": "test-pair",
            "indexer": {
                "strategy": "indexer.module.Class",
                "services": {"llm": "$gpt4", "db": "$db1"},
                "config": {"some": "param"}
            },
            "retriever": {
                "strategy": "retriever.module.Class",
                "services": {"embedding": "$embed1"},
                "config": {"top_k": 5}
            },
            "migrations": {
                "required_revisions": ["1234"]
            }
        }
        mock_loader.load_config.return_value = mock_config
    
        # Mock _import_strategy_class
        with patch.object(manager, "_import_strategy_class") as mock_import:
            mock_import.side_effect = lambda x: MockIndexingStrategy if "indexer" in x else MockRetrievalStrategy
    
            # Run
            idx, ret = manager.load_pair("test-pair")
    
            # Assertions
            assert isinstance(idx, MockIndexingStrategy)
            assert isinstance(ret, MockRetrievalStrategy)
    
            # Verify loader usage
            mock_loader.load_config.assert_called_with(Path("/tmp/strategies/test-pair.yaml"))
    
            # Verify Migration Validation
            manager.migration_validator.validate.assert_called_with(["1234"])
    
            # Verify Service Resolution
>           mock_registry.get.assert_any_call("$gpt4")

tests/unit/config/test_strategy_pair_manager.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='mock.get' id='127569159634624'>, args = ('$gpt4',)
kwargs = {}, expected = call('$gpt4'), cause = None
actual = [call('db_main'), call('gpt4'), call('db1'), call('embed1')]
expected_string = "get('$gpt4')"

    def assert_any_call(self, /, *args, **kwargs):
        """assert the mock has been called with the specified arguments.
    
        The assert passes if the mock has *ever* been called, unlike
        `assert_called_with` and `assert_called_once_with` that only pass if
        the call is the most recent one."""
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        cause = expected if isinstance(expected, Exception) else None
        actual = [self._call_matcher(c) for c in self.call_args_list]
        if cause or expected not in _AnyComparer(actual):
            expected_string = self._format_mock_call_signature(args, kwargs)
>           raise AssertionError(
                '%s call not found' % expected_string
            ) from cause
E           AssertionError: get('$gpt4') call not found

/usr/lib/python3.12/unittest/mock.py:1015: AssertionError
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.config.strategy_pair_manager:Loading strategy pair from /tmp/strategies/test-pair.yaml
------------------------------ Captured log call -------------------------------
INFO     rag_factory.config.strategy_pair_manager:strategy_pair_manager.py:96 Loading strategy pair from /tmp/strategies/test-pair.yaml
___________________________ test_db_context_creation ___________________________

manager = <rag_factory.config.strategy_pair_manager.StrategyPairManager object at 0x7405ff225f40>
mock_loader = <MagicMock name='StrategyPairLoader()' id='127569104104736'>
mock_registry = <MagicMock spec='ServiceRegistry' id='127569104802192'>

    def test_db_context_creation(manager, mock_loader, mock_registry):
        mock_config = {
            "strategy_name": "db-context-pair",
            "indexer": {
                "strategy": "indexer.module.Class",
                "services": {"db": "$db1"},
                "db_config": {
                    "tables": {"logical": "physical"},
                    "fields": {"f1": "col1"}
                }
            },
            "retriever": { "strategy": "retriever.module.Class" }
        }
        mock_loader.load_config.return_value = mock_config
    
        # Mock DB service with get_context
        mock_db = MagicMock()
        mock_context = MagicMock()
        mock_db.get_context.return_value = mock_context
    
        # Configure registry to return mock_db
        def get_service(ref):
            if ref == "$db1": return mock_db
            return MagicMock()
        mock_registry.get.side_effect = get_service
    
        with patch.object(manager, "_import_strategy_class") as mock_import:
            mock_import.return_value = MagicMock() # Generic mock strategy
    
            manager.load_pair("db-context-pair")
    
            # Verify get_context called
>           mock_db.get_context.assert_called_with(
                table_mapping={"logical": "physical"},
                field_mapping={"f1": "col1"}
            )

tests/unit/config/test_strategy_pair_manager.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='mock.get_context' id='127569160203024'>, args = ()
kwargs = {'field_mapping': {'f1': 'col1'}, 'table_mapping': {'logical': 'physical'}}
expected = "get_context(table_mapping={'logical': 'physical'}, field_mapping={'f1': 'col1'})"
actual = 'not called.'
error_message = "expected call not found.\nExpected: get_context(table_mapping={'logical': 'physical'}, field_mapping={'f1': 'col1'})\n  Actual: not called."

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\n  Actual: %s'
                    % (expected, actual))
>           raise AssertionError(error_message)
E           AssertionError: expected call not found.
E           Expected: get_context(table_mapping={'logical': 'physical'}, field_mapping={'f1': 'col1'})
E             Actual: not called.

/usr/lib/python3.12/unittest/mock.py:935: AssertionError
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.config.strategy_pair_manager:Loading strategy pair from /tmp/strategies/db-context-pair.yaml
------------------------------ Captured log call -------------------------------
INFO     rag_factory.config.strategy_pair_manager:strategy_pair_manager.py:96 Loading strategy pair from /tmp/strategies/db-context-pair.yaml
_____________ TestAlembicMigrations.test_migration_upgrade_to_head _____________

self = <sqlalchemy.engine.base.Connection object at 0x7406027f6e70>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740603c26150>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043d700>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x74060043d6a0>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740603c26150>
cursor = <cursor object at 0x7406023df790; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043d700>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.unit.database.test_migrations.TestAlembicMigrations object at 0x740603beec30>
alembic_config = <alembic.config.Config object at 0x7406004a6a50>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    def test_migration_upgrade_to_head(self, alembic_config: Config, test_db_url: str) -> None:
        """Test upgrading migrations to head."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/unit/database/test_migrations.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740603c26150>
cursor = <cursor object at 0x7406023df790; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043d700>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
________________ TestAlembicMigrations.test_migration_downgrade ________________

self = <sqlalchemy.engine.base.Connection object at 0x74060045a9c0>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602393830>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x740602393bf0>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x740602392c90>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602393830>
cursor = <cursor object at 0x7406023dfc40; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x740602393bf0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.unit.database.test_migrations.TestAlembicMigrations object at 0x740603beeed0>
alembic_config = <alembic.config.Config object at 0x7405ff2e8e90>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    def test_migration_downgrade(self, alembic_config: Config, test_db_url: str) -> None:
        """Test downgrading migrations."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/unit/database/test_migrations.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602393830>
cursor = <cursor object at 0x7406023dfc40; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x740602393bf0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
_______________ TestAlembicMigrations.test_migration_idempotency _______________

self = <sqlalchemy.engine.base.Connection object at 0x740602392840>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2d0230>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff225c40>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405ff225be0>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2d0230>
cursor = <cursor object at 0x7405fef54d60; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff225c40>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.unit.database.test_migrations.TestAlembicMigrations object at 0x740603bef530>
alembic_config = <alembic.config.Config object at 0x7405ff2d35f0>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    def test_migration_idempotency(self, alembic_config: Config, test_db_url: str) -> None:
        """Test that running migrations twice doesn't cause errors."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/unit/database/test_migrations.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2d0230>
cursor = <cursor object at 0x7405fef54d60; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff225c40>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
________________ TestAlembicMigrations.test_get_current_version ________________

self = <sqlalchemy.engine.base.Connection object at 0x740602393350>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff340c20>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff224050>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405ff224b90>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff340c20>
cursor = <cursor object at 0x7405fef563e0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff224050>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.unit.database.test_migrations.TestAlembicMigrations object at 0x740603bef860>
alembic_config = <alembic.config.Config object at 0x740603303740>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    def test_get_current_version(self, alembic_config: Config, test_db_url: str) -> None:
        """Test retrieving current schema version."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/unit/database/test_migrations.py:129: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff340c20>
cursor = <cursor object at 0x7405fef563e0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff224050>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
_____________ TestAlembicMigrations.test_migration_creates_tables ______________

self = <sqlalchemy.engine.base.Connection object at 0x7405ff341460>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2c4740>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043f8f0>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x74060043f7a0>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2c4740>
cursor = <cursor object at 0x7405fef576a0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043f8f0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.unit.database.test_migrations.TestAlembicMigrations object at 0x740603bef410>
alembic_config = <alembic.config.Config object at 0x7405ff3401d0>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    def test_migration_creates_tables(self, alembic_config: Config, test_db_url: str) -> None:
        """Test that migrations create expected tables."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/unit/database/test_migrations.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2c4740>
cursor = <cursor object at 0x7405fef576a0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043f8f0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
_____________ TestAlembicMigrations.test_migration_creates_indexes _____________

self = <sqlalchemy.engine.base.Connection object at 0x7405fef4e7b0>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef4e4e0>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fef20ce0>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405fef20c20>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef4e4e0>
cursor = <cursor object at 0x7405fef56f20; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fef20ce0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.unit.database.test_migrations.TestAlembicMigrations object at 0x740603bef110>
alembic_config = <alembic.config.Config object at 0x7405fef4d970>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    def test_migration_creates_indexes(self, alembic_config: Config, test_db_url: str) -> None:
        """Test that migrations create expected indexes."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/unit/database/test_migrations.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef4e4e0>
cursor = <cursor object at 0x7405fef56f20; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fef20ce0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
__________ TestCodeExamples.test_all_code_examples_have_valid_syntax ___________

self = <test_code_examples.TestCodeExamples object at 0x740603befa40>
docs_root = PosixPath('/mnt/MCPProyects/ragTools/docs')

    def test_all_code_examples_have_valid_syntax(self, docs_root):
        """Test that all Python code examples have valid syntax."""
        errors = []
    
        for md_file in docs_root.rglob("*.md"):
            # Skip project planning documents (epics, stories, verification docs)
            # and internal/migration documentation
            file_str = str(md_file).lower()
            skip_patterns = ['epic', 'stor', 'verification', 'completion-summary',
                           'onnx', 'migration', 'project-plan', 'readme']
            if any(skip in file_str for skip in skip_patterns):
                continue
    
            examples = self.extract_python_examples(md_file)
    
            for i, code in enumerate(examples):
                try:
                    # Try to compile the code
                    compile(code, f"{md_file.name}:example_{i}", "exec")
                except SyntaxError as e:
                    errors.append(f"{md_file.name}:example_{i}: {e}")
    
>       assert len(errors) == 0, \
            f"Syntax errors in code examples:\n" + "\n".join(errors)
E       AssertionError: Syntax errors in code examples:
E         semantic-local-pair-guide.md:example_0: 'await' outside function (semantic-local-pair-guide.md:example_0, line 21)
E         environment-variables.md:example_3: invalid syntax (environment-variables.md:example_3, line 13)
E         QUICK-REFERENCE.md:example_2: unexpected indent (QUICK-REFERENCE.md:example_2, line 1)
E       assert 3 == 0
E        +  where 3 = len(["semantic-local-pair-guide.md:example_0: 'await' outside function (semantic-local-pair-guide.md:example_0, line 21)", 'environment-variables.md:example_3: invalid syntax (environment-variables.md:example_3, line 13)', 'QUICK-REFERENCE.md:example_2: unexpected indent (QUICK-REFERENCE.md:example_2, line 1)'])

tests/unit/documentation/test_code_examples.py:57: AssertionError
_____________ TestDocumentationLinks.test_no_broken_internal_links _____________

self = <test_links.TestDocumentationLinks object at 0x740603a250d0>
docs_root = PosixPath('/mnt/MCPProyects/ragTools/docs')

    def test_no_broken_internal_links(self, docs_root):
        """Test that all internal links are valid."""
        broken_links = []
    
        for md_file in docs_root.rglob("*.md"):
            links = self.extract_links(md_file)
    
            for text, link in links:
                # Skip external links
                if link.startswith("http"):
                    continue
    
                # Skip anchors
                if link.startswith("#"):
                    continue
    
                # Skip file:// links (used in implementation plan)
                if link.startswith("file://"):
                    continue
    
                # Resolve relative path
                target = (md_file.parent / link).resolve()
    
                if not target.exists():
                    broken_links.append(f"{md_file.name} -> {link}")
    
>       assert len(broken_links) == 0, \
            f"Broken internal links:\n" + "\n".join(broken_links)
E       AssertionError: Broken internal links:
E         MIGRATION_MANAGER_REMOVAL.md -> ../stories/epic-16/story-16.5-remove-migration-manager.md
E         README.md -> ./story-17.1-service-registry-configuration-schema.md
E         README.md -> ./story-17.2-service-registry-implementation.md
E         README.md -> ./story-17.4-migration-validator-alembic.md
E         README.md -> ./story-17.6-first-strategy-pair-testing.md
E         README.md -> ./story-17.7-remaining-strategy-pairs.md
E         README.md -> ./story-17.8-cli-validation-sample-docs.md
E         README.md -> ../../epics/epic-16-database-consolidation.md
E         README.md -> ../../epics/epic-16-database-consolidation.md
E         README.md -> ../../database/README.md
E         README.md -> ../../getting-started/installation.md
E       assert 11 == 0
E        +  where 11 = len(['MIGRATION_MANAGER_REMOVAL.md -> ../stories/epic-16/story-16.5-remove-migration-manager.md', 'README.md -> ./story-17.1-service-registry-configuration-schema.md', 'README.md -> ./story-17.2-service-registry-implementation.md', 'README.md -> ./story-17.4-migration-validator-alembic.md', 'README.md -> ./story-17.6-first-strategy-pair-testing.md', 'README.md -> ./story-17.7-remaining-strategy-pairs.md', ...])

tests/unit/documentation/test_links.py:60: AssertionError
____________ TestLLMServiceCreation.test_create_llm_service_openai _____________

self = <tests.unit.registry.test_service_factory.TestLLMServiceCreation object at 0x740603a270e0>
factory = <rag_factory.registry.service_factory.ServiceFactory object at 0x7405ff2c6840>

    def test_create_llm_service_openai(self, factory):
        """Test creating OpenAI LLM service."""
        config = {
            "name": "openai-llm",
            "url": "https://api.openai.com/v1",
            "api_key": "sk-test",
            "model": "gpt-4",
            "temperature": 0.8,
            "max_tokens": 2000
        }
    
>       with patch('rag_factory.registry.service_factory.OpenAILLMService') as mock_class:

tests/unit/registry/test_service_factory.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7405ff2c5280>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.registry.service_factory' from '/mnt/MCPProyects/ragTools/rag_factory/registry/service_factory.py'> does not have the attribute 'OpenAILLMService'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
_______ TestEmbeddingServiceCreation.test_create_embedding_service_onnx ________

self = <tests.unit.registry.test_service_factory.TestEmbeddingServiceCreation object at 0x740603a27ad0>
factory = <rag_factory.registry.service_factory.ServiceFactory object at 0x7405ff2c70b0>

    def test_create_embedding_service_onnx(self, factory):
        """Test creating ONNX embedding service."""
        config = {
            "name": "onnx-embed",
            "provider": "onnx",
            "model": "Xenova/all-MiniLM-L6-v2",
            "cache_dir": "./models",
            "batch_size": 32
        }
    
>       with patch('rag_factory.registry.service_factory.ONNXEmbeddingService') as mock_class:

tests/unit/registry/test_service_factory.py:128: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7405ff2c42c0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.registry.service_factory' from '/mnt/MCPProyects/ragTools/rag_factory/registry/service_factory.py'> does not have the attribute 'ONNXEmbeddingService'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
_ TestEmbeddingServiceCreation.test_create_embedding_service_onnx_with_defaults _

self = <tests.unit.registry.test_service_factory.TestEmbeddingServiceCreation object at 0x740603a540e0>
factory = <rag_factory.registry.service_factory.ServiceFactory object at 0x7406004a4680>

    def test_create_embedding_service_onnx_with_defaults(self, factory):
        """Test creating ONNX embedding service with defaults."""
        config = {
            "provider": "onnx",
            "model": "Xenova/all-MiniLM-L6-v2"
        }
    
>       with patch('rag_factory.registry.service_factory.ONNXEmbeddingService') as mock_class:

tests/unit/registry/test_service_factory.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7405ff2e87d0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.registry.service_factory' from '/mnt/MCPProyects/ragTools/rag_factory/registry/service_factory.py'> does not have the attribute 'ONNXEmbeddingService'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
______ TestEmbeddingServiceCreation.test_create_embedding_service_openai _______

self = <tests.unit.registry.test_service_factory.TestEmbeddingServiceCreation object at 0x740603a27d70>
factory = <rag_factory.registry.service_factory.ServiceFactory object at 0x7406004a4ec0>

    def test_create_embedding_service_openai(self, factory):
        """Test creating OpenAI embedding service."""
        config = {
            "provider": "openai",
            "api_key": "sk-test",
            "model": "text-embedding-ada-002"
        }
    
>       with patch('rag_factory.registry.service_factory.OpenAIEmbeddingService') as mock_class:

tests/unit/registry/test_service_factory.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7406004a76b0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.registry.service_factory' from '/mnt/MCPProyects/ragTools/rag_factory/registry/service_factory.py'> does not have the attribute 'OpenAIEmbeddingService'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
_ TestDatabaseServiceCreation.test_create_database_service_postgres_with_connection_string _

self = <tests.unit.registry.test_service_factory.TestDatabaseServiceCreation object at 0x740603a26e10>
factory = <rag_factory.registry.service_factory.ServiceFactory object at 0x7405ff2c7650>

    def test_create_database_service_postgres_with_connection_string(self, factory):
        """Test creating PostgreSQL database service with connection string."""
        config = {
            "name": "postgres-db",
            "type": "postgres",
            "connection_string": "postgresql://user:pass@localhost:5432/db",
            "pool_size": 10,
            "max_overflow": 20
        }
    
>       with patch('rag_factory.registry.service_factory.PostgresqlDatabaseService') as mock_class:

tests/unit/registry/test_service_factory.py:218: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7405ff2c6540>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.registry.service_factory' from '/mnt/MCPProyects/ragTools/rag_factory/registry/service_factory.py'> does not have the attribute 'PostgresqlDatabaseService'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
_ TestDatabaseServiceCreation.test_create_database_service_postgres_with_components _

self = <tests.unit.registry.test_service_factory.TestDatabaseServiceCreation object at 0x740603a26a80>
factory = <rag_factory.registry.service_factory.ServiceFactory object at 0x7405ff2c4260>

    def test_create_database_service_postgres_with_components(self, factory):
        """Test creating PostgreSQL database service with connection components."""
        config = {
            "type": "postgres",
            "user": "testuser",
            "password": "testpass",
            "host": "localhost",
            "port": 5432,
            "database": "testdb",
            "pool_size": 5
        }
    
>       with patch('rag_factory.registry.service_factory.PostgresqlDatabaseService') as mock_class:

tests/unit/registry/test_service_factory.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7405ff2c7a70>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.registry.service_factory' from '/mnt/MCPProyects/ragTools/rag_factory/registry/service_factory.py'> does not have the attribute 'PostgresqlDatabaseService'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
_ TestDatabaseServiceCreation.test_create_database_service_postgres_with_defaults _

self = <tests.unit.registry.test_service_factory.TestDatabaseServiceCreation object at 0x740603a24890>
factory = <rag_factory.registry.service_factory.ServiceFactory object at 0x740603a25be0>

    def test_create_database_service_postgres_with_defaults(self, factory):
        """Test creating PostgreSQL database service with default values."""
        config = {
            "type": "postgres",
            "user": "testuser",
            "password": "testpass",
            "host": "localhost",
            "database": "testdb"
        }
    
>       with patch('rag_factory.registry.service_factory.PostgresqlDatabaseService') as mock_class:

tests/unit/registry/test_service_factory.py:264: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x740603a256a0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.registry.service_factory' from '/mnt/MCPProyects/ragTools/rag_factory/registry/service_factory.py'> does not have the attribute 'PostgresqlDatabaseService'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
________ TestDatabaseServiceCreation.test_create_database_service_neo4j ________

self = <tests.unit.registry.test_service_factory.TestDatabaseServiceCreation object at 0x740603beedb0>
factory = <rag_factory.registry.service_factory.ServiceFactory object at 0x7405ff2d1a00>

    def test_create_database_service_neo4j(self, factory):
        """Test creating Neo4j database service."""
        config = {
            "type": "neo4j",
            "uri": "bolt://localhost:7687",
            "user": "neo4j",
            "password": "testpass"
        }
    
>       with patch('rag_factory.registry.service_factory.Neo4jGraphService') as mock_class:

tests/unit/registry/test_service_factory.py:287: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7405ff2d1730>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.registry.service_factory' from '/mnt/MCPProyects/ragTools/rag_factory/registry/service_factory.py'> does not have the attribute 'Neo4jGraphService'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
_ TestDatabaseServiceCreation.test_create_database_service_neo4j_with_defaults _

self = <tests.unit.registry.test_service_factory.TestDatabaseServiceCreation object at 0x740603bef350>
factory = <rag_factory.registry.service_factory.ServiceFactory object at 0x7405ff343da0>

    def test_create_database_service_neo4j_with_defaults(self, factory):
        """Test creating Neo4j database service with default URI."""
        config = {
            "type": "neo4j",
            "host": "localhost",
            "port": 7687,
            "user": "neo4j",
            "password": "testpass"
        }
    
>       with patch('rag_factory.registry.service_factory.Neo4jGraphService') as mock_class:

tests/unit/registry/test_service_factory.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7405ff341880>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'rag_factory.registry.service_factory' from '/mnt/MCPProyects/ragTools/rag_factory/registry/service_factory.py'> does not have the attribute 'Neo4jGraphService'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
_________________________ test_calculate_cost_is_zero __________________________

onnx_config = {'max_batch_size': 32, 'model': 'Xenova/all-MiniLM-L6-v2'}
mock_onnx_env = <function create_mock_onnx_environment at 0x740603d8d580>

    def test_calculate_cost_is_zero(onnx_config, mock_onnx_env):
        """Test that local ONNX provider has zero cost.
    
        Uses centralized mock_onnx_env to handle all ONNX mocking.
        """
        with mock_onnx_env(dimension=384):
            from rag_factory.services.embedding.providers.onnx_local import ONNXLocalProvider
    
            provider = ONNXLocalProvider(onnx_config)
>           cost = provider.calculate_cost(num_texts=100)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: ONNXLocalProvider.calculate_cost() got an unexpected keyword argument 'num_texts'

tests/unit/services/embedding/test_onnx_local_provider.py:78: TypeError
______________________________ test_capabilities _______________________________

strategy = <rag_factory.strategies.indexing.context_aware.ContextAwareChunkingIndexing object at 0x7405fef2a720>

    def test_capabilities(strategy):
        """Test produces capabilities."""
>       assert strategy.produces() == {
            IndexCapability.CHUNKS,
            IndexCapability.DATABASE
        }
E       AssertionError: assert {<IndexCapabi...y.VECTORS: 1>} == {<IndexCapabi...ty.CHUNKS: 5>}
E         
E         Extra items in the left set:
E         <IndexCapability.VECTORS: 1>
E         
E         Full diff:
E           {
E         +     <IndexCapability.VECTORS: 1>,...
E         
E         ...Full output truncated (3 lines hidden), use '-vv' to show

tests/unit/strategies/indexing/test_context_aware.py:33: AssertionError
_______________________ TestImports.test_import_factory ________________________

self = <tests.unit.test_package.TestImports object at 0x740603a7b080>

    def test_import_factory(self) -> None:
        """Test RAGFactory can be imported."""
>       from rag_factory import RAGFactory
E       ImportError: cannot import name 'RAGFactory' from 'rag_factory' (/mnt/MCPProyects/ragTools/rag_factory/__init__.py)

tests/unit/test_package.py:18: ImportError
_______________________ TestImports.test_import_pipeline _______________________

self = <tests.unit.test_package.TestImports object at 0x740603a7b0b0>

    def test_import_pipeline(self) -> None:
        """Test StrategyPipeline can be imported."""
>       from rag_factory import StrategyPipeline
E       ImportError: cannot import name 'StrategyPipeline' from 'rag_factory' (/mnt/MCPProyects/ragTools/rag_factory/__init__.py)

tests/unit/test_package.py:24: ImportError
________________________ TestImports.test_import_config ________________________

self = <tests.unit.test_package.TestImports object at 0x740603a7a780>

    def test_import_config(self) -> None:
        """Test ConfigManager can be imported."""
>       from rag_factory import ConfigManager
E       ImportError: cannot import name 'ConfigManager' from 'rag_factory' (/mnt/MCPProyects/ragTools/rag_factory/__init__.py)

tests/unit/test_package.py:30: ImportError
________________ TestPackageStructure.test_no_circular_imports _________________

self = <tests.unit.test_package.TestPackageStructure object at 0x740603a7b980>

    def test_no_circular_imports(self) -> None:
        """Test importing doesn't cause circular import errors."""
        try:
>           from rag_factory import RAGFactory
E           ImportError: cannot import name 'RAGFactory' from 'rag_factory' (/mnt/MCPProyects/ragTools/rag_factory/__init__.py)

tests/unit/test_package.py:81: ImportError

During handling of the above exception, another exception occurred:

self = <tests.unit.test_package.TestPackageStructure object at 0x740603a7b980>

    def test_no_circular_imports(self) -> None:
        """Test importing doesn't cause circular import errors."""
        try:
            from rag_factory import RAGFactory
            from rag_factory import StrategyPipeline
            from rag_factory import ConfigManager
            from rag_factory.strategies import IRAGStrategy
    
            # If we get here, no circular imports
            assert RAGFactory is not None
            assert StrategyPipeline is not None
            assert ConfigManager is not None
            assert IRAGStrategy is not None
        except ImportError as e:
>           pytest.fail(f"Circular import detected: {e}")
E           Failed: Circular import detected: cannot import name 'RAGFactory' from 'rag_factory' (/mnt/MCPProyects/ragTools/rag_factory/__init__.py)

tests/unit/test_package.py:92: Failed
_____________ TestDependencies.test_optional_dependencies_handled ______________

self = <tests.unit.test_package.TestDependencies object at 0x740603a94320>

    def test_optional_dependencies_handled(self) -> None:
        """Test package works without optional dependencies."""
        # Should not fail if optional dependencies missing
>       from rag_factory import RAGFactory
E       ImportError: cannot import name 'RAGFactory' from 'rag_factory' (/mnt/MCPProyects/ragTools/rag_factory/__init__.py)

tests/unit/test_package.py:111: ImportError
____________ TestMigrationIntegration.test_real_migration_execution ____________

self = <sqlalchemy.engine.base.Connection object at 0x7405fef4e720>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2d5550>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2d4e90>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405ff2d4e00>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2d5550>
cursor = <cursor object at 0x740603332200; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2d4e90>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_integration.TestMigrationIntegration object at 0x740603a94ad0>
alembic_config = <alembic.config.Config object at 0x7406004a52e0>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    @pytest.mark.asyncio
    async def test_real_migration_execution(self, alembic_config: Config, test_db_url: str) -> None:
        """Test running migrations against real database."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/integration/database/test_migration_integration.py:29: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2d5550>
cursor = <cursor object at 0x740603332200; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2d4e90>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
__________ TestMigrationIntegration.test_migration_with_existing_data __________

self = <sqlalchemy.engine.base.Connection object at 0x7405ff2d54c0>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef2a4e0>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff3cb530>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405ff3c97c0>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef2a4e0>
cursor = <cursor object at 0x7406023dc5e0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff3cb530>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_integration.TestMigrationIntegration object at 0x740603a94d70>
alembic_config = <alembic.config.Config object at 0x7405ff2320c0>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    @pytest.mark.asyncio
    async def test_migration_with_existing_data(self, alembic_config: Config, test_db_url: str) -> None:
        """Test migration with existing data in database."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/integration/database/test_migration_integration.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef2a4e0>
cursor = <cursor object at 0x7406023dc5e0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff3cb530>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
_____________ TestMigrationIntegration.test_rollback_functionality _____________

self = <sqlalchemy.engine.base.Connection object at 0x740603a55310>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602305490>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2c5280>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405ff2c47a0>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602305490>
cursor = <cursor object at 0x7406033163e0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2c5280>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_integration.TestMigrationIntegration object at 0x740603a95190>
alembic_config = <alembic.config.Config object at 0x7405ff3cbb90>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    @pytest.mark.asyncio
    async def test_rollback_functionality(self, alembic_config: Config, test_db_url: str) -> None:
        """Test rolling back migrations."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/integration/database/test_migration_integration.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602305490>
cursor = <cursor object at 0x7406033163e0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2c5280>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
__________ TestMigrationIntegration.test_pgvector_extension_installed __________

self = <sqlalchemy.engine.base.Connection object at 0x7405ff318530>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7406023045f0>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7406027f6ea0>
statement = <sqlalchemy.dialects.postgresql.base.PGDDLCompiler object at 0x7406027f5cd0>
parameters = [immutabledict({})]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7406023045f0>
cursor = <cursor object at 0x7405ff3cc8b0; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7406027f6ea0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.UndefinedTable: relation "chunks" does not exist

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: UndefinedTable

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_integration.TestMigrationIntegration object at 0x740603a7be00>
alembic_config = <alembic.config.Config object at 0x7405ff31b230>
test_db_url = 'postgresql://rag_user:rag_password@192.168.56.1:5432/rag_test'

    @pytest.mark.asyncio
    async def test_pgvector_extension_installed(self, alembic_config: Config, test_db_url: str) -> None:
        """Test that pgvector extension is installed by migrations."""
        # Upgrade to head
>       command.upgrade(alembic_config, "head")

tests/integration/database/test_migration_integration.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:403: in upgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/002_add_hierarchy_support.py:26: in upgrade
    op.add_column(
<string>:8: in add_column
    ???
<string>:3: in add_column
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2142: in add_column
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:171: in add_column
    operations.impl.add_column(table_name, column, schema=schema, **kw)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:334: in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/ddl.py:181: in _execute_on_connection
    return connection._execute_ddl(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1528: in _execute_ddl
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7406023045f0>
cursor = <cursor object at 0x7405ff3cc8b0; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7406027f6ea0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "chunks" does not exist
E       
E       [SQL: ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID]
E       (Background on this error at: https://sqlalche.me/e/20/f405)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: ProgrammingError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 001 -> 002, Add hierarchy support to chunks table
______ TestMigrationValidatorIntegration.test_validate_with_no_migrations ______

self = <sqlalchemy.engine.base.Connection object at 0x7405ff319b20>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7406023e85f0>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043ff50>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x74060043fbc0>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7406023e85f0>
cursor = <cursor object at 0x7405ff3cd990; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043ff50>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a94c20>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7406023e97c0>
alembic_config = <alembic.config.Config object at 0x7406023e8c80>

    @pytest.mark.asyncio
    async def test_validate_with_no_migrations(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test validation when no migrations are applied."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/integration/database/test_migration_validator_integration.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7406023e85f0>
cursor = <cursor object at 0x7405ff3cd990; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043ff50>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
___ TestMigrationValidatorIntegration.test_validate_with_partial_migrations ____

self = <sqlalchemy.engine.base.Connection object at 0x7406023e8950>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602390c20>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fef4b890>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405fef48d40>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602390c20>
cursor = <cursor object at 0x7405ff3cee30; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fef4b890>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a94110>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7405ff224c80>
alembic_config = <alembic.config.Config object at 0x7405fef23350>

    @pytest.mark.asyncio
    async def test_validate_with_partial_migrations(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test validation when only some migrations are applied."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/integration/database/test_migration_validator_integration.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602390c20>
cursor = <cursor object at 0x7405ff3cee30; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fef4b890>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
_____ TestMigrationValidatorIntegration.test_validate_with_all_migrations ______

self = <sqlalchemy.engine.base.Connection object at 0x740602392f90>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff3cac60>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff230fe0>
statement = <sqlalchemy.dialects.postgresql.base.PGDDLCompiler object at 0x7405ff2315e0>
parameters = [immutabledict({})]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff3cac60>
cursor = <cursor object at 0x740603332200; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff230fe0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.UndefinedTable: relation "chunks" does not exist

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: UndefinedTable

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a95a30>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7405ff319610>
alembic_config = <alembic.config.Config object at 0x740603bd5f70>

    @pytest.mark.asyncio
    async def test_validate_with_all_migrations(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test validation when all required migrations are applied."""
        # Apply all migrations
>       command.upgrade(alembic_config, "head")

tests/integration/database/test_migration_validator_integration.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:403: in upgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/002_add_hierarchy_support.py:26: in upgrade
    op.add_column(
<string>:8: in add_column
    ???
<string>:3: in add_column
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2142: in add_column
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:171: in add_column
    operations.impl.add_column(table_name, column, schema=schema, **kw)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:334: in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/ddl.py:181: in _execute_on_connection
    return connection._execute_ddl(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1528: in _execute_ddl
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff3cac60>
cursor = <cursor object at 0x740603332200; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff230fe0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "chunks" does not exist
E       
E       [SQL: ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID]
E       (Background on this error at: https://sqlalche.me/e/20/f405)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: ProgrammingError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 001 -> 002, Add hierarchy support to chunks table
_______ TestMigrationValidatorIntegration.test_validate_or_raise_success _______

self = <sqlalchemy.engine.base.Connection object at 0x7405ff3cb890>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef20c20>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043d070>
statement = <sqlalchemy.dialects.postgresql.base.PGDDLCompiler object at 0x74060043f4d0>
parameters = [immutabledict({})]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef20c20>
cursor = <cursor object at 0x7405fef54400; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043d070>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.UndefinedTable: relation "chunks" does not exist

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: UndefinedTable

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a95e20>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7405ff3cb950>
alembic_config = <alembic.config.Config object at 0x7405fef20080>

    @pytest.mark.asyncio
    async def test_validate_or_raise_success(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test validate_or_raise when all migrations are applied."""
        # Apply all migrations
>       command.upgrade(alembic_config, "head")

tests/integration/database/test_migration_validator_integration.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:403: in upgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/002_add_hierarchy_support.py:26: in upgrade
    op.add_column(
<string>:8: in add_column
    ???
<string>:3: in add_column
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2142: in add_column
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:171: in add_column
    operations.impl.add_column(table_name, column, schema=schema, **kw)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:334: in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/ddl.py:181: in _execute_on_connection
    return connection._execute_ddl(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1528: in _execute_ddl
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef20c20>
cursor = <cursor object at 0x7405fef54400; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x74060043d070>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "chunks" does not exist
E       
E       [SQL: ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID]
E       (Background on this error at: https://sqlalche.me/e/20/f405)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: ProgrammingError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 001 -> 002, Add hierarchy support to chunks table
_______ TestMigrationValidatorIntegration.test_validate_or_raise_failure _______

self = <sqlalchemy.engine.base.Connection object at 0x7405fef21430>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602304a70>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2d7f80>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405ff2d79b0>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602304a70>
cursor = <cursor object at 0x7405fef56980; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2d7f80>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a96210>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x74060043c3e0>
alembic_config = <alembic.config.Config object at 0x7406027f4a40>

    @pytest.mark.asyncio
    async def test_validate_or_raise_failure(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test validate_or_raise when migrations are missing."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/integration/database/test_migration_validator_integration.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x740602304a70>
cursor = <cursor object at 0x7405fef56980; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2d7f80>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
_________ TestMigrationValidatorIntegration.test_get_current_revision __________

self = <sqlalchemy.engine.base.Connection object at 0x740602307ec0>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef4ecf0>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fef4daf0>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405fef4f9b0>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef4ecf0>
cursor = <cursor object at 0x7405ff3cc040; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fef4daf0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a96600>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7405ff2d4740>
alembic_config = <alembic.config.Config object at 0x7405ff2d4650>

    @pytest.mark.asyncio
    async def test_get_current_revision(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test getting current revision from database."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/integration/database/test_migration_validator_integration.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fef4ecf0>
cursor = <cursor object at 0x7405ff3cc040; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fef4daf0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
______________ TestMigrationValidatorIntegration.test_is_at_head _______________

self = <sqlalchemy.engine.base.Connection object at 0x7405fef4d9a0>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd5934a0>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd591e50>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405fd591d00>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd5934a0>
cursor = <cursor object at 0x7405ff3cfb50; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd591e50>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a96d80>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7405ff356270>
alembic_config = <alembic.config.Config object at 0x7405ff356f30>

    @pytest.mark.asyncio
    async def test_is_at_head(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test checking if database is at head revision."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/integration/database/test_migration_validator_integration.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd5934a0>
cursor = <cursor object at 0x7405ff3cfb50; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd591e50>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
_________ TestMigrationValidatorIntegration.test_error_message_details _________

self = <sqlalchemy.engine.base.Connection object at 0x7405fd5900b0>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd5ea0f0>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd521940>
statement = <sqlalchemy.dialects.postgresql.base.PGCompiler object at 0x7405fd521880>
parameters = [{}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd5ea0f0>
cursor = <cursor object at 0x7405fd5549a0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd521940>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.DependentObjectsStillExist: cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: DependentObjectsStillExist

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a96f30>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7405fd5e8170>
alembic_config = <alembic.config.Config object at 0x7405fd5e8ec0>

    @pytest.mark.asyncio
    async def test_error_message_details(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test that error messages include migration details."""
        # Start from clean state
>       command.downgrade(alembic_config, "base")

tests/integration/database/test_migration_validator_integration.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:449: in downgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/001_initial_schema.py:129: in downgrade
    op.execute("DROP EXTENSION IF EXISTS vector")
<string>:8: in execute
    ???
<string>:3: in execute
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2537: in execute
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:224: in execute_sql
    operations.migration_context.impl.execute(
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:214: in execute
    self._exec(sql, execution_options)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:516: in _execute_on_connection
    return connection._execute_clauseelement(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1639: in _execute_clauseelement
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd5ea0f0>
cursor = <cursor object at 0x7405fd5549a0; closed: -1>
statement = 'DROP EXTENSION IF EXISTS vector', parameters = {}
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd521940>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.InternalError: (psycopg2.errors.DependentObjectsStillExist) cannot drop extension vector because other objects depend on it
E       DETAIL:  column embedding of table test_chunks_real depends on type vector
E       HINT:  Use DROP ... CASCADE to drop the dependent objects too.
E       
E       [SQL: DROP EXTENSION IF EXISTS vector]
E       (Background on this error at: https://sqlalche.me/e/20/2j85)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: InternalError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running downgrade 001 -> , Initial schema with documents and chunks tables
_______ TestMigrationValidatorIntegration.test_validate_single_migration _______

self = <sqlalchemy.engine.base.Connection object at 0x7405fd5eab10>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd0ddb50>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd0dfcb0>
statement = <sqlalchemy.dialects.postgresql.base.PGDDLCompiler object at 0x7405fd0dfbf0>
parameters = [immutabledict({})]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd0ddb50>
cursor = <cursor object at 0x7405fd555b70; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd0dfcb0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.UndefinedTable: relation "chunks" does not exist

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: UndefinedTable

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a96870>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7405fd522480>
alembic_config = <alembic.config.Config object at 0x7405fd523bf0>

    @pytest.mark.asyncio
    async def test_validate_single_migration(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test validating a single migration requirement."""
        # Apply all migrations
>       command.upgrade(alembic_config, "head")

tests/integration/database/test_migration_validator_integration.py:240: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:403: in upgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/002_add_hierarchy_support.py:26: in upgrade
    op.add_column(
<string>:8: in add_column
    ???
<string>:3: in add_column
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2142: in add_column
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:171: in add_column
    operations.impl.add_column(table_name, column, schema=schema, **kw)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:334: in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/ddl.py:181: in _execute_on_connection
    return connection._execute_ddl(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1528: in _execute_ddl
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd0ddb50>
cursor = <cursor object at 0x7405fd555b70; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd0dfcb0>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "chunks" does not exist
E       
E       [SQL: ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID]
E       (Background on this error at: https://sqlalche.me/e/20/f405)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: ProgrammingError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 001 -> 002, Add hierarchy support to chunks table
____ TestMigrationValidatorIntegration.test_validate_nonexistent_migration _____

self = <sqlalchemy.engine.base.Connection object at 0x7405fd0ddc40>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd0bbe60>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd0f9c40>
statement = <sqlalchemy.dialects.postgresql.base.PGDDLCompiler object at 0x7405fd0f9be0>
parameters = [immutabledict({})]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd0bbe60>
cursor = <cursor object at 0x7405fd556e30; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd0f9c40>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.UndefinedTable: relation "chunks" does not exist

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: UndefinedTable

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a962d0>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7405fd0b8f80>
alembic_config = <alembic.config.Config object at 0x7405fd0baf00>

    @pytest.mark.asyncio
    async def test_validate_nonexistent_migration(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test validating a migration that doesn't exist."""
        # Apply all migrations
>       command.upgrade(alembic_config, "head")

tests/integration/database/test_migration_validator_integration.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:403: in upgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/002_add_hierarchy_support.py:26: in upgrade
    op.add_column(
<string>:8: in add_column
    ???
<string>:3: in add_column
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2142: in add_column
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:171: in add_column
    operations.impl.add_column(table_name, column, schema=schema, **kw)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:334: in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/ddl.py:181: in _execute_on_connection
    return connection._execute_ddl(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1528: in _execute_ddl
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405fd0bbe60>
cursor = <cursor object at 0x7405fd556e30; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405fd0f9c40>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "chunks" does not exist
E       
E       [SQL: ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID]
E       (Background on this error at: https://sqlalche.me/e/20/f405)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: ProgrammingError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 001 -> 002, Add hierarchy support to chunks table
_______ TestMigrationValidatorIntegration.test_validate_after_downgrade ________

self = <sqlalchemy.engine.base.Connection object at 0x7405ff354e60>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff354a70>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2d5130>
statement = <sqlalchemy.dialects.postgresql.base.PGDDLCompiler object at 0x7405ff2d56a0>
parameters = [immutabledict({})]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff354a70>
cursor = <cursor object at 0x7406023dc4f0; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2d5130>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.UndefinedTable: relation "chunks" does not exist

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: UndefinedTable

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a95b80>
validator = <rag_factory.services.database.migration_validator.MigrationValidator object at 0x7405fd5eaba0>
alembic_config = <alembic.config.Config object at 0x7405fd5eb800>

    @pytest.mark.asyncio
    async def test_validate_after_downgrade(
        self,
        validator: MigrationValidator,
        alembic_config: Config
    ) -> None:
        """Test validation after downgrading migrations."""
        # Apply all migrations
>       command.upgrade(alembic_config, "head")

tests/integration/database/test_migration_validator_integration.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:403: in upgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/002_add_hierarchy_support.py:26: in upgrade
    op.add_column(
<string>:8: in add_column
    ???
<string>:3: in add_column
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2142: in add_column
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:171: in add_column
    operations.impl.add_column(table_name, column, schema=schema, **kw)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:334: in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/ddl.py:181: in _execute_on_connection
    return connection._execute_ddl(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1528: in _execute_ddl
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff354a70>
cursor = <cursor object at 0x7406023dc4f0; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff2d5130>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "chunks" does not exist
E       
E       [SQL: ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID]
E       (Background on this error at: https://sqlalche.me/e/20/f405)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: ProgrammingError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 001 -> 002, Add hierarchy support to chunks table
___ TestMigrationValidatorIntegration.test_multiple_validators_same_database ___

self = <sqlalchemy.engine.base.Connection object at 0x7405ff3552e0>
dialect = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2c53a0>
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff39fe00>
statement = <sqlalchemy.dialects.postgresql.base.PGDDLCompiler object at 0x7405ff39f1a0>
parameters = [immutabledict({})]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2c53a0>
cursor = <cursor object at 0x740603332d40; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff39fe00>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       psycopg2.errors.UndefinedTable: relation "chunks" does not exist

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: UndefinedTable

The above exception was the direct cause of the following exception:

self = <tests.integration.database.test_migration_validator_integration.TestMigrationValidatorIntegration object at 0x740603a94d10>
db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7405ff2d5bb0>
alembic_config = <alembic.config.Config object at 0x740602304290>

    @pytest.mark.asyncio
    async def test_multiple_validators_same_database(
        self,
        db_service: PostgresqlDatabaseService,
        alembic_config: Config
    ) -> None:
        """Test multiple validators on the same database."""
        # Apply migrations
>       command.upgrade(alembic_config, "head")

tests/integration/database/test_migration_validator_integration.py:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/alembic/command.py:403: in upgrade
    script.run_env()
venv/lib/python3.12/site-packages/alembic/script/base.py:583: in run_env
    util.load_python_file(self.dir, "env.py")
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:95: in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/util/pyfiles.py:113: in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
migrations/env.py:93: in <module>
    run_migrations_online()
migrations/env.py:87: in run_migrations_online
    context.run_migrations()
<string>:8: in run_migrations
    ???
venv/lib/python3.12/site-packages/alembic/runtime/environment.py:948: in run_migrations
    self.get_context().run_migrations(**kw)
venv/lib/python3.12/site-packages/alembic/runtime/migration.py:627: in run_migrations
    step.migration_fn(**kw)
migrations/versions/002_add_hierarchy_support.py:26: in upgrade
    op.add_column(
<string>:8: in add_column
    ???
<string>:3: in add_column
    ???
venv/lib/python3.12/site-packages/alembic/operations/ops.py:2142: in add_column
    return operations.invoke(op)
           ^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/base.py:445: in invoke
    return fn(self, operation)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/alembic/operations/toimpl.py:171: in add_column
    operations.impl.add_column(table_name, column, schema=schema, **kw)
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:334: in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
venv/lib/python3.12/site-packages/alembic/ddl/impl.py:207: in _exec
    return conn.execute(construct, multiparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1416: in execute
    return meth(
venv/lib/python3.12/site-packages/sqlalchemy/sql/ddl.py:181: in _execute_on_connection
    return connection._execute_ddl(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1528: in _execute_ddl
    ret = self._execute_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1848: in _execute_context
    return self._exec_single_context(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1988: in _exec_single_context
    self._handle_dbapi_exception(
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2343: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1969: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg2.PGDialect_psycopg2 object at 0x7405ff2c53a0>
cursor = <cursor object at 0x740603332d40; closed: -1>
statement = 'ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID'
parameters = immutabledict({})
context = <sqlalchemy.dialects.postgresql.psycopg2.PGExecutionContext_psycopg2 object at 0x7405ff39fe00>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)
E       sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "chunks" does not exist
E       
E       [SQL: ALTER TABLE chunks ADD COLUMN parent_chunk_id UUID]
E       (Background on this error at: https://sqlalche.me/e/20/f405)

venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:922: ProgrammingError
----------------------------- Captured stderr call -----------------------------
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 001 -> 002, Add hierarchy support to chunks table
=============================== warnings summary ===============================
rag_factory/services/llm/config.py:8
  /mnt/MCPProyects/ragTools/rag_factory/services/llm/config.py:8: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class LLMServiceConfig(BaseModel):

rag_factory/database/config.py:12
  /mnt/MCPProyects/ragTools/rag_factory/database/config.py:12: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class DatabaseConfig(BaseSettings):

rag_factory/strategies/late_chunking/models.py:21
  /mnt/MCPProyects/ragTools/rag_factory/strategies/late_chunking/models.py:21: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class TokenEmbedding(BaseModel):

rag_factory/strategies/late_chunking/models.py:34
  /mnt/MCPProyects/ragTools/rag_factory/strategies/late_chunking/models.py:34: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class DocumentEmbedding(BaseModel):

rag_factory/strategies/late_chunking/models.py:49
  /mnt/MCPProyects/ragTools/rag_factory/strategies/late_chunking/models.py:49: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class EmbeddingChunk(BaseModel):

tests/unit/strategies/self_reflective/test_strategy.py: 12 warnings
  /mnt/MCPProyects/ragTools/rag_factory/strategies/self_reflective/strategy.py:179: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    result["refinements"] = [r.dict() for r in refinements]

tests/integration/database/test_migration_validator_integration.py: 13 warnings
  /mnt/MCPProyects/ragTools/tests/integration/database/test_migration_validator_integration.py:50: RuntimeWarning: coroutine 'PostgresqlDatabaseService.close' was never awaited
    service.close()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorEdgeCases::test_validator_with_auto_discovered_config
tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorEdgeCases::test_validate_empty_requirements
  /mnt/MCPProyects/ragTools/tests/integration/database/test_migration_validator_integration.py:336: RuntimeWarning: coroutine 'PostgresqlDatabaseService.close' was never awaited
    service.close()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_______________ coverage: platform linux, python 3.12.3-final-0 ________________

Name                                                               Stmts   Miss  Cover   Missing
------------------------------------------------------------------------------------------------
rag_factory/__init__.py                                                3      0   100%
rag_factory/__version__.py                                             1      0   100%
rag_factory/cli/__init__.py                                            2      0   100%
rag_factory/cli/commands/__init__.py                                   1      0   100%
rag_factory/cli/commands/benchmark.py                                 96     81    16%   40-53, 64-84, 148-263
rag_factory/cli/commands/check_consistency.py                         43     33    23%   64-120
rag_factory/cli/commands/config.py                                    78     68    13%   26-64, 102-188
rag_factory/cli/commands/index.py                                     65     52    20%   29-39, 93-185
rag_factory/cli/commands/query.py                                     47     36    23%   73-149
rag_factory/cli/commands/repl.py                                     152    130    14%   29-56, 63-108, 112-120, 124-133, 137-155, 159-172, 176-186, 190-207, 211-220, 253-260
rag_factory/cli/commands/strategies.py                                45     36    20%   48-120
rag_factory/cli/commands/validate_e2e.py                              91     69    24%   37-38, 42-142, 145-156
rag_factory/cli/commands/validate_pipeline.py                         70     57    19%   57-156
rag_factory/cli/formatters/__init__.py                                 4      0   100%
rag_factory/cli/formatters/consistency.py                             61     55    10%   21-69, 79-103, 108-138
rag_factory/cli/formatters/output.py                                  20      9    55%   23-24, 43-44, 63-64, 74, 79, 84
rag_factory/cli/formatters/results.py                                 71     62    13%   28-55, 72-99, 116-144, 157-181
rag_factory/cli/formatters/validation.py                              63     53    16%   25-43, 48-56, 65-77, 86-98, 103-129, 134-137
rag_factory/cli/main.py                                               31     10    68%   25-28, 48-49, 76-78, 82
rag_factory/cli/utils/__init__.py                                      3      0   100%
rag_factory/cli/utils/progress.py                                     14      8    43%   34-47, 73-76
rag_factory/cli/utils/validation.py                                   53     42    21%   30-41, 57-77, 93-118, 135
rag_factory/config/__init__.py                                         4      0   100%
rag_factory/config/env_resolver.py                                    52     21    60%   52, 81-86, 110, 129-147
rag_factory/config/schemas/__init__.py                                 2      0   100%
rag_factory/config/schemas/version.py                                 10      6    40%   29-38
rag_factory/config/strategy_loader.py                                 54     36    33%   32-41, 45-60, 75-89, 101-104
rag_factory/config/strategy_pair_manager.py                          105     23    78%   57-58, 66-70, 93, 114, 156-157, 191-196, 210-215, 274-275
rag_factory/config/validator.py                                       93     32    66%   70, 177-198, 271-295, 333-354
rag_factory/core/__init__.py                                           4      4     0%   7-23
rag_factory/core/capabilities.py                                      61     16    74%   129, 155, 173-174, 239-253
rag_factory/core/indexing_interface.py                                54     31    43%   55-57, 144, 164, 208, 260, 272, 292-313, 341-364
rag_factory/core/pipeline.py                                          49     35    29%   22-24, 33-40, 55-77, 95-96, 105-108, 117-120, 137-146
rag_factory/core/retrieval_interface.py                               38     20    47%   56-58, 112-117, 138, 158, 196, 247, 259, 285-309
rag_factory/database/__init__.py                                       4      0   100%
rag_factory/database/config.py                                        17      0   100%
rag_factory/database/connection.py                                    80     54    32%   40-48, 61-86, 96, 107, 126-137, 150-157, 165-170, 178-183, 196-197, 209-214, 218, 222
rag_factory/database/env_validator.py                                 38     18    53%   46-54, 66-77, 86-88, 118, 126, 139
rag_factory/database/models.py                                        85     28    67%   31-34, 37-45, 48-54, 66-69, 72-77, 80-85
rag_factory/database/vector_indexing.py                               38     38     0%   7-114
rag_factory/evaluation/__init__.py                                     4      4     0%   29-41
rag_factory/evaluation/analysis/__init__.py                            3      3     0%   8-11
rag_factory/evaluation/analysis/comparison.py                        135    135     0%   8-323
rag_factory/evaluation/analysis/statistics.py                         81     81     0%   8-320
rag_factory/evaluation/benchmarks/__init__.py                          3      3     0%   8-14
rag_factory/evaluation/benchmarks/config.py                           25     25     0%   7-64
rag_factory/evaluation/benchmarks/runner.py                          155    155     0%   8-423
rag_factory/evaluation/datasets/__init__.py                            3      3     0%   8-14
rag_factory/evaluation/datasets/loader.py                             88     88     0%   8-281
rag_factory/evaluation/datasets/schema.py                             52     52     0%   8-160
rag_factory/evaluation/datasets/statistics.py                         65     65     0%   8-226
rag_factory/evaluation/exporters/__init__.py                           4      4     0%   8-12
rag_factory/evaluation/exporters/csv_exporter.py                      52     52     0%   3-124
rag_factory/evaluation/exporters/html_exporter.py                     47     47     0%   3-293
rag_factory/evaluation/exporters/json_exporter.py                     19     19     0%   3-68
rag_factory/evaluation/metrics/__init__.py                             3      3     0%   12-25
rag_factory/evaluation/metrics/base.py                                31     31     0%   8-115
rag_factory/evaluation/metrics/cost.py                                65     65     0%   8-313
rag_factory/evaluation/metrics/performance.py                         42     42     0%   8-202
rag_factory/evaluation/metrics/quality.py                             84     84     0%   8-377
rag_factory/evaluation/metrics/retrieval.py                           77     77     0%   9-401
rag_factory/exceptions.py                                             13      0   100%
rag_factory/factory.py                                               187    132    29%   144, 165-170, 209-230, 256-270, 300-324, 347-348, 365, 384, 401, 418, 435, 451-460, 480-502, 528-545, 568-611, 644-699, 703-715
rag_factory/legacy_config.py                                         155    155     0%   22-542
rag_factory/models/__init__.py                                         6      6     0%   3-14
rag_factory/models/embedding/__init__.py                               4      4     0%   3-12
rag_factory/models/embedding/loader.py                               179    179     0%   3-367
rag_factory/models/embedding/models.py                                47     47     0%   3-106
rag_factory/models/embedding/registry.py                             128    128     0%   3-282
rag_factory/models/evaluation/__init__.py                              3      3     0%   3-6
rag_factory/models/evaluation/ab_testing.py                           98     98     0%   3-281
rag_factory/models/evaluation/models.py                               29     29     0%   3-74
rag_factory/observability/__init__.py                                  3      3     0%   3-10
rag_factory/observability/integrations/__init__.py                     1      1     0%   3
rag_factory/observability/integrations/prometheus.py                  47     47     0%   3-229
rag_factory/observability/logging/__init__.py                          2      2     0%   3-5
rag_factory/observability/logging/config.py                           16     16     0%   3-37
rag_factory/observability/logging/filters.py                          41     41     0%   3-103
rag_factory/observability/logging/logger.py                           86     86     0%   3-396
rag_factory/observability/metrics/__init__.py                          2      2     0%   3-9
rag_factory/observability/metrics/collector.py                       143    143     0%   3-459
rag_factory/observability/metrics/cost.py                             39     39     0%   3-221
rag_factory/observability/metrics/performance.py                      64     64     0%   3-196
rag_factory/observability/monitoring/__init__.py                       1      1     0%   3
rag_factory/observability/monitoring/api.py                          101    101     0%   3-328
rag_factory/pipeline.py                                              156    156     0%   34-507
rag_factory/registry/__init__.py                                       4      0   100%
rag_factory/registry/exceptions.py                                     6      0   100%
rag_factory/registry/service_factory.py                               62     10    84%   166-176, 184-191
rag_factory/registry/service_registry.py                              91     14    85%   114, 118, 123-124, 188-192, 214-218
rag_factory/repositories/__init__.py                                   5      0   100%
rag_factory/repositories/base.py                                      42     21    50%   38, 53, 70, 87, 103, 114-118, 125, 136-139, 159-164
rag_factory/repositories/chunk.py                                    206    180    13%   40-45, 68-76, 90-95, 120-133, 155-171, 187-202, 221-231, 250-261, 287-328, 355-396, 422-474, 489-499, 513-521, 537-543, 557-563, 577-587, 604-626, 639-670
rag_factory/repositories/document.py                                  93     74    20%   41-46, 63-68, 98-118, 140-156, 172-187, 206, 223-233, 247-255, 272-279, 290-293, 307-312
rag_factory/repositories/exceptions.py                                18      7    61%   34-36, 56-59
rag_factory/services/__init__.py                                       4      0   100%
rag_factory/services/api/__init__.py                                   4      0   100%
rag_factory/services/api/anthropic.py                                 27     18    33%   46-52, 76-94, 118-141
rag_factory/services/api/cohere.py                                    30     19    37%   14-15, 49-59, 82-113
rag_factory/services/api/openai.py                                    46     27    41%   75-93, 117-140, 189-199, 216-227, 235
rag_factory/services/consistency.py                                   32     24    25%   80-113, 144-170
rag_factory/services/database/__init__.py                              5      0   100%
rag_factory/services/database/database_context.py                    181    159    12%   58-64, 88-106, 123, 140-150, 179-200, 223-240, 256-266, 305-330, 338-339, 343-406, 414-415, 430-510, 516-517, 521-546
rag_factory/services/database/migration_validator.py                  85     28    67%   29-30, 88, 112, 137-141, 152-181, 197, 202-204, 234, 268-271
rag_factory/services/database/neo4j.py                                54     36    33%   14, 60-71, 79-84, 103-114, 134-150, 172-181, 188-191, 195, 199
rag_factory/services/database/postgres.py                            149     69    54%   17-18, 26-27, 85, 91-103, 187, 197-200, 204-228, 252-285, 299-314, 329-344, 376-394, 409, 484-502, 527, 531
rag_factory/services/dependencies.py                                  41     19    54%   96-109, 134-140, 179-180
rag_factory/services/embedding/__init__.py                             4      0   100%
rag_factory/services/embedding/base.py                                31      6    81%   44, 59, 68, 77, 86, 98
rag_factory/services/embedding/cache.py                               43     33    23%   26-32, 43-58, 67-76, 80-82, 90-94
rag_factory/services/embedding/config.py                              29     13    55%   37-43, 50-68, 80
rag_factory/services/embedding/providers/__init__.py                   5      0   100%
rag_factory/services/embedding/providers/cohere.py                    53     36    32%   10-20, 55-75, 90-117, 125, 133, 141, 152-153
rag_factory/services/embedding/providers/local.py                     43     27    37%   8-9, 46-67, 83-109, 117, 125, 133, 146
rag_factory/services/embedding/providers/onnx_local.py               114     30    74%   18-19, 114, 143-147, 158, 170-175, 248-250, 273, 289-308
rag_factory/services/embedding/providers/openai.py                    51     27    47%   10-20, 54, 64, 70, 87-108, 116, 124, 132, 143-144
rag_factory/services/embedding/rate_limiter.py                        20      1    95%   32
rag_factory/services/embedding/service.py                            100     83    17%   50-62, 79-93, 117-198, 220-222, 233-239, 248-266, 273-275
rag_factory/services/interfaces.py                                    35      0   100%
rag_factory/services/llm/__init__.py                                   5      0   100%
rag_factory/services/llm/base.py                                      43      0   100%
rag_factory/services/llm/config.py                                    23      6    74%   46-48, 51-53
rag_factory/services/llm/prompt_template.py                           43     24    44%   47-82, 97-100, 111-115
rag_factory/services/llm/providers/__init__.py                        12     10    17%   10-22
rag_factory/services/llm/providers/anthropic.py                       57     42    26%   44-50, 64-97, 120-147, 171-172, 184-189, 197, 205
rag_factory/services/llm/providers/ollama.py                          55     42    24%   22-24, 37-67, 93-122, 142-143, 157, 165, 174, 185-196
rag_factory/services/llm/providers/openai.py                          57     19    67%   56, 101-105, 133-158, 189-194, 202, 210
rag_factory/services/llm/service.py                                   66     24    64%   68, 127-129, 154-175, 186, 194-198, 215-216
rag_factory/services/llm/token_counter.py                             27      4    85%   56-57, 71-72
rag_factory/services/local/__init__.py                                 2      2     0%   7-9
rag_factory/services/local/reranker.py                                32     32     0%   7-116
rag_factory/services/onnx/__init__.py                                  2      0   100%
rag_factory/services/onnx/embedding.py                                29     12    59%   61, 65, 83-93, 110-121
rag_factory/services/utils/__init__.py                                 2      0   100%
rag_factory/services/utils/model_converter.py                        104    104     0%   8-291
rag_factory/services/utils/onnx_utils.py                             133     38    71%   22-23, 37, 79-84, 96-97, 142-146, 175, 189-190, 194, 198-201, 216-218, 258, 265-268, 361-362, 376, 392-404
rag_factory/services/utils/reranker_selector.py                       68     68     0%   8-249
rag_factory/strategies/__init__.py                                     2      0   100%
rag_factory/strategies/agentic/__init__.py                             7      7     0%   8-20
rag_factory/strategies/agentic/agent.py                              151    151     0%   8-420
rag_factory/strategies/agentic/config.py                              12     12     0%   5-31
rag_factory/strategies/agentic/frameworks/__init__.py                  1      1     0%   9
rag_factory/strategies/agentic/query_analyzer.py                      89     89     0%   8-278
rag_factory/strategies/agentic/strategy.py                            87     87     0%   8-284
rag_factory/strategies/agentic/tool_implementations.py               135    135     0%   11-511
rag_factory/strategies/agentic/tools.py                               40     40     0%   8-142
rag_factory/strategies/base.py                                        55     10    82%   81, 83, 85, 169-178
rag_factory/strategies/chunking/__init__.py                           15     15     0%   28-67
rag_factory/strategies/chunking/base.py                               91     91     0%   3-256
rag_factory/strategies/chunking/docling_chunker.py                    45     45     0%   13-188
rag_factory/strategies/chunking/fixed_size_chunker.py                 74     74     0%   3-204
rag_factory/strategies/chunking/hybrid_chunker.py                     70     70     0%   3-192
rag_factory/strategies/chunking/semantic_chunker.py                  195    195     0%   3-534
rag_factory/strategies/chunking/structural_chunker.py                147    147     0%   3-425
rag_factory/strategies/chunking/utils.py                              89     89     0%   3-264
rag_factory/strategies/contextual/__init__.py                          7      7     0%   8-19
rag_factory/strategies/contextual/batch_processor.py                  91     91     0%   8-266
rag_factory/strategies/contextual/config.py                           46     46     0%   8-195
rag_factory/strategies/contextual/context_generator.py                84     84     0%   8-234
rag_factory/strategies/contextual/cost_tracker.py                     32     32     0%   8-126
rag_factory/strategies/contextual/prompts.py                          25     25     0%   8-119
rag_factory/strategies/contextual/storage.py                          44     44     0%   8-118
rag_factory/strategies/contextual/strategy.py                         78     78     0%   8-298
rag_factory/strategies/fine_tuned/__init__.py                          4      4     0%   1-5
rag_factory/strategies/fine_tuned/ab_testing.py                       78     78     0%   1-134
rag_factory/strategies/fine_tuned/config.py                           11     11     0%   1-30
rag_factory/strategies/fine_tuned/custom_loader.py                    40     40     0%   1-88
rag_factory/strategies/fine_tuned/model_registry.py                  126    126     0%   1-303
rag_factory/strategies/hierarchical/__init__.py                        5      0   100%
rag_factory/strategies/hierarchical/hierarchy_builder.py              79     25    68%   73, 117, 149, 220, 249-281, 310-335
rag_factory/strategies/hierarchical/models.py                         58      0   100%
rag_factory/strategies/hierarchical/parent_retriever.py               89     47    47%   66-74, 85, 88, 109-111, 141-142, 166-196, 213-227, 250-257, 268-278
rag_factory/strategies/hierarchical/strategy.py                       76     19    75%   77, 92-93, 101, 112-144, 254, 268-269
rag_factory/strategies/indexing/__init__.py                            6      0   100%
rag_factory/strategies/indexing/context_aware.py                     108     11    90%   74, 81, 89-91, 115, 126-128, 162, 197
rag_factory/strategies/indexing/hierarchical.py                       90     73    19%   50, 63, 88-134, 156-185, 201-236, 253-278, 292-293
rag_factory/strategies/indexing/in_memory.py                          41     25    39%   54, 65, 93-121, 143, 160, 173, 194-200
rag_factory/strategies/indexing/keyword_indexing.py                   42     42     0%   8-157
rag_factory/strategies/indexing/knowledge_graph_indexing.py           46     33    28%   18, 27, 48-89, 108-129
rag_factory/strategies/indexing/vector_embedding.py                   98     85    13%   25, 35, 55-116, 137-221
rag_factory/strategies/knowledge_graph/__init__.py                     4      4     0%   8-19
rag_factory/strategies/knowledge_graph/config.py                      21     21     0%   8-129
rag_factory/strategies/knowledge_graph/entity_extractor.py            70     70     0%   7-205
rag_factory/strategies/knowledge_graph/graph_store.py                 31     31     0%   8-124
rag_factory/strategies/knowledge_graph/hybrid_retriever.py            56     56     0%   8-164
rag_factory/strategies/knowledge_graph/memory_graph_store.py          97     97     0%   8-196
rag_factory/strategies/knowledge_graph/models.py                      53     53     0%   8-122
rag_factory/strategies/knowledge_graph/relationship_extractor.py      62     62     0%   8-189
rag_factory/strategies/knowledge_graph/strategy.py                    80     80     0%   8-256
rag_factory/strategies/late_chunking/__init__.py                       6      0   100%
rag_factory/strategies/late_chunking/coherence_analyzer.py            27      5    81%   59, 82-96
rag_factory/strategies/late_chunking/document_embedder.py            104     34    67%   35, 56, 62-63, 72, 227-235, 256-279, 296-303
rag_factory/strategies/late_chunking/embedding_chunker.py            117     11    91%   53, 57, 145, 191-192, 215-225, 298
rag_factory/strategies/late_chunking/models.py                        62      0   100%
rag_factory/strategies/late_chunking/strategy.py                      70     12    83%   58, 77, 88, 107-122
rag_factory/strategies/multi_query/__init__.py                         7      7     0%   8-19
rag_factory/strategies/multi_query/config.py                          36     36     0%   3-138
rag_factory/strategies/multi_query/deduplicator.py                    87     87     0%   3-197
rag_factory/strategies/multi_query/parallel_executor.py               52     52     0%   3-163
rag_factory/strategies/multi_query/prompts.py                         10     10     0%   3-84
rag_factory/strategies/multi_query/ranker.py                          80     80     0%   3-201
rag_factory/strategies/multi_query/strategy.py                        68     68     0%   3-184
rag_factory/strategies/multi_query/variant_generator.py               62     62     0%   3-187
rag_factory/strategies/query_expansion/__init__.py                     8      0   100%
rag_factory/strategies/query_expansion/base.py                        66     10    85%   90, 103, 114-118, 130-133
rag_factory/strategies/query_expansion/cache.py                       52     40    23%   20-22, 38-55, 64-66, 70-73, 81-97, 105-112
rag_factory/strategies/query_expansion/expander_service.py            92     71    23%   36-40, 55-58, 75-148, 159-180, 204-215, 233-234, 242-246, 254, 265-273, 283-284
rag_factory/strategies/query_expansion/hyde_expander.py               19     12    37%   25-27, 41-65
rag_factory/strategies/query_expansion/llm_expander.py                25     16    36%   23-25, 39-71, 99
rag_factory/strategies/query_expansion/metrics.py                     66     30    55%   44-45, 53-57, 76-109, 130, 134, 146-152, 160
rag_factory/strategies/query_expansion/prompts.py                     13      7    46%   16, 28-56, 68-107
rag_factory/strategies/reranking/__init__.py                           7      7     0%   8-21
rag_factory/strategies/reranking/base.py                              68     68     0%   8-130
rag_factory/strategies/reranking/bge_reranker.py                      51     51     0%   8-151
rag_factory/strategies/reranking/cache.py                             48     48     0%   8-138
rag_factory/strategies/reranking/cohere_reranker.py                   34     34     0%   8-119
rag_factory/strategies/reranking/cosine_reranker.py                   62     62     0%   8-249
rag_factory/strategies/reranking/cross_encoder_reranker.py            40     40     0%   8-124
rag_factory/strategies/reranking/metrics.py                           67     67     0%   8-264
rag_factory/strategies/reranking/reranker_service.py                 115    115     0%   8-310
rag_factory/strategies/retrieval/__init__.py                           6      0   100%
rag_factory/strategies/retrieval/keyword_retriever.py                 56     41    27%   41, 52, 71-113, 126-133, 153-172
rag_factory/strategies/retrieval/knowledge_graph_retriever.py         22      9    59%   19, 27, 48-70
rag_factory/strategies/retrieval/multi_query_retriever.py             22      9    59%   19, 26, 44-66
rag_factory/strategies/retrieval/query_expansion_retriever.py         24      9    62%   20, 27, 45-67
rag_factory/strategies/retrieval/semantic_retriever.py                20      9    55%   23, 32, 61-81
rag_factory/strategies/self_reflective/__init__.py                     6      0   100%
rag_factory/strategies/self_reflective/config.py                      20      8    60%   29-42
rag_factory/strategies/self_reflective/grader.py                      80     60    25%   51-67, 83-109, 135-171, 187-258
rag_factory/strategies/self_reflective/models.py                      48      0   100%
rag_factory/strategies/self_reflective/refiner.py                     69     18    74%   78-91, 114, 125, 127, 129, 131, 209-228
rag_factory/strategies/self_reflective/strategy.py                    92      2    98%   84-85
rag_factory/utils/__init__.py                                          3      0   100%
rag_factory/utils/token_counter.py                                    45     30    33%   36-40, 52-62, 74-82, 86, 90, 105-106, 124-135, 153-158
rag_factory/utils/tokenization.py                                     95     73    23%   36-40, 53-64, 87-114, 126-129, 141-144, 156, 175-190, 209-228, 241-243, 251-252, 270-271, 292-293, 314-315
------------------------------------------------------------------------------------------------
TOTAL                                                              12276   9240    25%
=========================== short test summary info ============================
FAILED tests/integration_real/test_database_real.py::test_store_and_retrieve_chunks
FAILED tests/integration_real/test_database_real.py::test_vector_similarity_search
FAILED tests/integration_real/test_database_real.py::test_batch_embedding_and_storage
FAILED tests/integration_real/test_database_real.py::test_chunk_metadata_persistence
FAILED tests/integration_real/test_database_real.py::test_database_context_table_mapping
FAILED tests/integration_real/test_end_to_end_real.py::test_document_indexing_pipeline
FAILED tests/integration_real/test_end_to_end_real.py::test_retrieval_pipeline
FAILED tests/integration_real/test_end_to_end_real.py::test_full_rag_pipeline
FAILED tests/integration_real/test_end_to_end_real.py::test_multiple_document_batches
FAILED tests/integration_real/test_end_to_end_real.py::test_retrieval_with_metadata_filtering
FAILED tests/integration_real/test_end_to_end_real.py::test_large_document_indexing
FAILED tests/integration_real/test_end_to_end_real.py::test_retrieval_accuracy
FAILED tests/integration_real/test_llm_real.py::test_llm_streaming - TypeErro...
FAILED tests/integration/registry/test_registry_integration.py::TestRealServiceInstantiation::test_multiple_service_instantiation
FAILED tests/integration/registry/test_registry_integration.py::TestErrorHandling::test_invalid_service_config
FAILED tests/integration/registry/test_registry_integration.py::TestConfigurationValidation::test_configuration_warnings
FAILED tests/integration/strategies/test_self_reflective_integration.py::TestSelfReflectiveIntegration::test_end_to_end_workflow
FAILED tests/integration/strategies/test_self_reflective_integration.py::TestSelfReflectiveIntegration::test_retry_with_poor_results
FAILED tests/integration/strategies/test_self_reflective_integration.py::TestSelfReflectiveIntegration::test_performance_within_limits
FAILED tests/integration/strategies/test_self_reflective_integration.py::TestSelfReflectiveWithLMStudio::test_with_real_llm
FAILED tests/integration/test_package_integration.py::TestSmokeTest::test_basic_usage_smoke_test
FAILED tests/integration/test_package_integration.py::TestFullWorkflow::test_full_workflow_with_installed_package
FAILED tests/unit/config/test_strategy_pair_manager.py::test_load_pair_success
FAILED tests/unit/config/test_strategy_pair_manager.py::test_db_context_creation
FAILED tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_upgrade_to_head
FAILED tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_downgrade
FAILED tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_idempotency
FAILED tests/unit/database/test_migrations.py::TestAlembicMigrations::test_get_current_version
FAILED tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_creates_tables
FAILED tests/unit/database/test_migrations.py::TestAlembicMigrations::test_migration_creates_indexes
FAILED tests/unit/documentation/test_code_examples.py::TestCodeExamples::test_all_code_examples_have_valid_syntax
FAILED tests/unit/documentation/test_links.py::TestDocumentationLinks::test_no_broken_internal_links
FAILED tests/unit/registry/test_service_factory.py::TestLLMServiceCreation::test_create_llm_service_openai
FAILED tests/unit/registry/test_service_factory.py::TestEmbeddingServiceCreation::test_create_embedding_service_onnx
FAILED tests/unit/registry/test_service_factory.py::TestEmbeddingServiceCreation::test_create_embedding_service_onnx_with_defaults
FAILED tests/unit/registry/test_service_factory.py::TestEmbeddingServiceCreation::test_create_embedding_service_openai
FAILED tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_postgres_with_connection_string
FAILED tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_postgres_with_components
FAILED tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_postgres_with_defaults
FAILED tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_neo4j
FAILED tests/unit/registry/test_service_factory.py::TestDatabaseServiceCreation::test_create_database_service_neo4j_with_defaults
FAILED tests/unit/services/embedding/test_onnx_local_provider.py::test_calculate_cost_is_zero
FAILED tests/unit/strategies/indexing/test_context_aware.py::test_capabilities
FAILED tests/unit/test_package.py::TestImports::test_import_factory - ImportE...
FAILED tests/unit/test_package.py::TestImports::test_import_pipeline - Import...
FAILED tests/unit/test_package.py::TestImports::test_import_config - ImportEr...
FAILED tests/unit/test_package.py::TestPackageStructure::test_no_circular_imports
FAILED tests/unit/test_package.py::TestDependencies::test_optional_dependencies_handled
FAILED tests/integration/database/test_migration_integration.py::TestMigrationIntegration::test_real_migration_execution
FAILED tests/integration/database/test_migration_integration.py::TestMigrationIntegration::test_migration_with_existing_data
FAILED tests/integration/database/test_migration_integration.py::TestMigrationIntegration::test_rollback_functionality
FAILED tests/integration/database/test_migration_integration.py::TestMigrationIntegration::test_pgvector_extension_installed
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_with_no_migrations
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_with_partial_migrations
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_with_all_migrations
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_or_raise_success
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_or_raise_failure
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_get_current_revision
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_is_at_head
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_error_message_details
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_single_migration
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_nonexistent_migration
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_validate_after_downgrade
FAILED tests/integration/database/test_migration_validator_integration.py::TestMigrationValidatorIntegration::test_multiple_validators_same_database
====== 64 failed, 121 passed, 3 skipped, 32 warnings in 323.94s (0:05:23) ======

========================================
Test run completed at 2025-12-17 19:13:50
========================================
