========================================
Test Results - 2025-12-18 23:22:20
========================================
Environment Configuration:
  EMBEDDING_MODEL_NAME: Xenova/all-MiniLM-L6-v2
  EMBEDDING_MODEL_PATH: models/embeddings
  LM_STUDIO_BASE_URL: http://192.168.56.1:1234/v1
  LM_STUDIO_MODEL: phi-4-mini-instruct

Command: pytest tests/integration_real/test_database_real.py::test_vector_similarity_search tests/integration_real/test_end_to_end_real.py::test_full_rag_pipeline -v
========================================

============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-9.0.1, pluggy-1.6.0 -- /mnt/MCPProyects/ragTools/venv/bin/python3
cachedir: .pytest_cache
rootdir: /mnt/MCPProyects/ragTools
configfile: pytest.ini (WARNING: ignoring pytest config in pyproject.toml!)
plugins: anyio-4.12.0, cov-7.0.0, asyncio-1.3.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 2 items

tests/integration_real/test_database_real.py::test_vector_similarity_search PASSED [ 50%]
tests/integration_real/test_end_to_end_real.py::test_full_rag_pipeline FAILED [100%]

=================================== FAILURES ===================================
____________________________ test_full_rag_pipeline ____________________________

real_db_service = <rag_factory.services.database.postgres.PostgresqlDatabaseService object at 0x7eff9db555b0>
real_embedding_service = <tests.integration_real.conftest.real_embedding_service.<locals>.AsyncEmbeddingWrapper object at 0x7eff9db55f10>
real_llm_service = <rag_factory.services.llm.service.LLMService object at 0x7eff9dac55e0>

    @pytest.mark.real_integration
    @pytest.mark.requires_postgres
    @pytest.mark.requires_embeddings
    @pytest.mark.requires_llm
    @pytest.mark.asyncio
    async def test_full_rag_pipeline(real_db_service, real_embedding_service, real_llm_service):
        """Test complete RAG pipeline: indexing -> retrieval -> generation."""
        from rag_factory.strategies.indexing.vector_embedding import VectorEmbeddingIndexing
        from rag_factory.strategies.retrieval.semantic_retriever import SemanticRetriever
        from rag_factory.strategies.base import StrategyConfig
        from rag_factory.services.dependencies import StrategyDependencies
        from rag_factory.services.llm.base import Message, MessageRole
        from rag_factory.core.indexing_interface import IndexingContext
        from dataclasses import asdict
    
        # Step 1: Index documents
        documents = [
            {
                "text": "The Eiffel Tower was built in 1889 by Gustave Eiffel. It is located in Paris, France and stands 330 meters tall.",
                "id": "eiffel_doc",
                "metadata": {"source": "eiffel.txt"}
            },
            {
                "text": "Python is a high-level programming language created by Guido van Rossum in 1991. It is known for its simplicity and readability.",
                "id": "python_doc",
                "metadata": {"source": "python.txt"}
            }
        ]
    
        indexing_config = StrategyConfig(strategy_name="indexing", chunk_size=200, chunk_overlap=50)
        indexing_deps = StrategyDependencies(
            embedding_service=real_embedding_service,
            database_service=real_db_service
        )
        indexing_strategy = VectorEmbeddingIndexing(config=asdict(indexing_config), dependencies=indexing_deps)
    
        # Create indexing context
        indexing_context = IndexingContext(database_service=real_db_service, config=asdict(indexing_config))
    
        await indexing_strategy.process(documents, indexing_context)
    
        # Step 2: Retrieve relevant chunks
        retrieval_config = StrategyConfig(strategy_name="retrieval", top_k=2)
        retrieval_deps = StrategyDependencies(
            embedding_service=real_embedding_service,
            database_service=real_db_service
        )
        retrieval_strategy = SemanticRetriever(config=retrieval_config, dependencies=retrieval_deps)
    
        # Create retrieval context
        from rag_factory.core.retrieval_interface import RetrievalContext
        retrieval_context = RetrievalContext(database_service=real_db_service, config={})
    
        query = "When was the Eiffel Tower built?"
        retrieved_chunks = await retrieval_strategy.retrieve(query, retrieval_context)
    
        assert len(retrieved_chunks) > 0
    
        # Step 3: Generate answer using LLM
        context = "\n\n".join([chunk.text for chunk in retrieved_chunks])
    
        messages = [
            Message(
                role=MessageRole.SYSTEM,
                content="Answer the question based only on the provided context. Be concise."
            ),
            Message(
                role=MessageRole.USER,
                content=f"Context:\n{context}\n\nQuestion: {query}"
            )
        ]
    
        response = real_llm_service.complete(messages)
    
        assert response is not None
>       assert "1889" in response.content
E       AssertionError: assert '1889' in 'The given context does not contain information about when the Eiffel Tower was constructed; it only provides details on Python, which has no relation to historical events or structures like the Eiffel Tower. The answer can be found independently of this text but is unrelated to its content regarding programming languages.'
E        +  where 'The given context does not contain information about when the Eiffel Tower was constructed; it only provides details on Python, which has no relation to historical events or structures like the Eiffel Tower. The answer can be found independently of this text but is unrelated to its content regarding programming languages.' = LLMResponse(content='The given context does not contain information about when the Eiffel Tower was constructed; it only provides details on Python, which has no relation to historical events or structures like the Eiffel Tower. The answer can be found independently of this text but is unrelated to its content regarding programming languages.', model='phi-4-mini-instruct', provider='openai', prompt_tokens=56, completion_tokens=56, total_tokens=112, cost=0.0, latency=3.963956117630005, metadata={'finish_reason': 'stop'}).content

tests/integration_real/test_end_to_end_real.py:186: AssertionError
---------------------------- Captured stderr setup -----------------------------
DEBUG:asyncio:Using selector: EpollSelector
INFO:tests.integration_real.conftest:[FIXTURE] Creating PostgresqlDatabaseService
INFO:rag_factory.services.database.postgres:Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO:tests.integration_real.conftest:[FIXTURE] Getting pool for cleanup
INFO:rag_factory.services.database.postgres:[_ensure_table] Starting table setup for test_chunks_real
INFO:rag_factory.services.database.postgres:[_ensure_table] Acquiring connection from pool
INFO:rag_factory.services.database.postgres:[_ensure_table] Connection acquired, enabling pgvector extension
INFO:rag_factory.services.database.postgres:[_ensure_table] pgvector extension enabled
INFO:rag_factory.services.database.postgres:[_ensure_table] Creating table test_chunks_real
INFO:rag_factory.services.database.postgres:[_ensure_table] Table created/verified
INFO:rag_factory.services.database.postgres:[_ensure_table] Checking if embedding column exists
INFO:rag_factory.services.database.postgres:[_ensure_table] Embedding column exists: True
INFO:rag_factory.services.database.postgres:[_ensure_table] Creating ivfflat index
INFO:rag_factory.services.database.postgres:[_ensure_table] Index created/verified
INFO:rag_factory.services.database.postgres:Ensured table test_chunks_real exists
INFO:tests.integration_real.conftest:[FIXTURE] Pool acquired, truncating table
INFO:tests.integration_real.conftest:[FIXTURE] Table truncated successfully
INFO:tests.integration_real.conftest:[FIXTURE] Yielding service to test
INFO:rag_factory.services.embedding.providers.onnx_local:Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO:rag_factory.services.utils.onnx_utils:Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO:rag_factory.services.utils.onnx_utils:Created ONNX session with providers: ['CPUExecutionProvider']
INFO:rag_factory.services.utils.onnx_utils:Model has 3 inputs and 1 outputs
DEBUG:rag_factory.services.utils.onnx_utils:  Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG:rag_factory.services.utils.onnx_utils:  Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO:rag_factory.services.utils.onnx_utils:Model validation passed
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO:rag_factory.services.embedding.providers.onnx_local:Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 192.168.56.1:1234
DEBUG:urllib3.connectionpool:http://192.168.56.1:1234 "GET /v1/models HTTP/1.1" 200 1082
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
INFO     tests.integration_real.conftest:conftest.py:166 [FIXTURE] Creating PostgresqlDatabaseService
INFO     rag_factory.services.database.postgres:postgres.py:121 Initialized PostgreSQL service for 192.168.56.1:5432/rag_test
INFO     tests.integration_real.conftest:conftest.py:179 [FIXTURE] Getting pool for cleanup
INFO     rag_factory.services.database.postgres:postgres.py:149 [_ensure_table] Starting table setup for test_chunks_real
INFO     rag_factory.services.database.postgres:postgres.py:150 [_ensure_table] Acquiring connection from pool
INFO     rag_factory.services.database.postgres:postgres.py:152 [_ensure_table] Connection acquired, enabling pgvector extension
INFO     rag_factory.services.database.postgres:postgres.py:155 [_ensure_table] pgvector extension enabled
INFO     rag_factory.services.database.postgres:postgres.py:158 [_ensure_table] Creating table test_chunks_real
INFO     rag_factory.services.database.postgres:postgres.py:169 [_ensure_table] Table created/verified
INFO     rag_factory.services.database.postgres:postgres.py:172 [_ensure_table] Checking if embedding column exists
INFO     rag_factory.services.database.postgres:postgres.py:184 [_ensure_table] Embedding column exists: True
INFO     rag_factory.services.database.postgres:postgres.py:192 [_ensure_table] Creating ivfflat index
INFO     rag_factory.services.database.postgres:postgres.py:199 [_ensure_table] Index created/verified
INFO     rag_factory.services.database.postgres:postgres.py:201 Ensured table test_chunks_real exists
INFO     tests.integration_real.conftest:conftest.py:181 [FIXTURE] Pool acquired, truncating table
INFO     tests.integration_real.conftest:conftest.py:185 [FIXTURE] Table truncated successfully
INFO     tests.integration_real.conftest:conftest.py:192 [FIXTURE] Yielding service to test
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:111 Loading ONNX model: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:90 Using model from EMBEDDING_MODEL_NAME env: Xenova/all-MiniLM-L6-v2
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:113 Using cached ONNX model: models/embeddings/Xenova_all-MiniLM-L6-v2/onnx/model.onnx
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:211 Created ONNX session with providers: ['CPUExecutionProvider']
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:243 Model has 3 inputs and 1 outputs
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: input_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: attention_mask, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:247   Input: token_type_ids, shape: ['batch_size', 'sequence_length'], type: tensor(int64)
DEBUG    rag_factory.services.utils.onnx_utils:onnx_utils.py:251   Output: last_hidden_state, shape: ['batch_size', 'sequence_length', 384], type: tensor(float)
INFO     rag_factory.services.utils.onnx_utils:onnx_utils.py:273 Model validation passed
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:168 Loaded tokenizer from models/embeddings/Xenova_all-MiniLM-L6-v2/tokenizer.json
INFO     rag_factory.services.embedding.providers.onnx_local:onnx_local.py:177 Loaded ONNX model Xenova/all-MiniLM-L6-v2 with 384 dimensions
DEBUG    urllib3.connectionpool:connectionpool.py:241 Starting new HTTP connection (1): 192.168.56.1:1234
DEBUG    urllib3.connectionpool:connectionpool.py:544 http://192.168.56.1:1234 "GET /v1/models HTTP/1.1" 200 1082
----------------------------- Captured stderr call -----------------------------
INFO:rag_factory.strategies.indexing.vector_embedding:Starting VectorEmbeddingIndexing.process with 2 documents
DEBUG:rag_factory.strategies.indexing.vector_embedding:Configuration: batch_size=32, chunk_size=200, overlap=50
INFO:rag_factory.strategies.indexing.vector_embedding:Using embedding service: AsyncEmbeddingWrapper
INFO:rag_factory.strategies.indexing.vector_embedding:Starting document chunking...
DEBUG:rag_factory.strategies.indexing.vector_embedding:_chunk_documents called with 2 documents, chunk_size=200, overlap=50
DEBUG:rag_factory.strategies.indexing.vector_embedding:Processing document 1/2: doc_id='eiffel_doc', text_length=112
DEBUG:rag_factory.strategies.indexing.vector_embedding:Document eiffel_doc fits in single chunk (length=112 <= 200)
DEBUG:rag_factory.strategies.indexing.vector_embedding:Processing document 2/2: doc_id='python_doc', text_length=128
DEBUG:rag_factory.strategies.indexing.vector_embedding:Document python_doc fits in single chunk (length=128 <= 200)
DEBUG:rag_factory.strategies.indexing.vector_embedding:_chunk_documents complete: total 2 chunks created
INFO:rag_factory.strategies.indexing.vector_embedding:Document chunking complete: created 2 chunks from 2 documents
INFO:rag_factory.strategies.indexing.vector_embedding:Starting embedding generation for 2 chunks in batches of 32...
DEBUG:rag_factory.strategies.indexing.vector_embedding:Processing batch 1/1 with 2 texts
DEBUG:rag_factory.strategies.indexing.vector_embedding:Batch 1/1 complete: generated 2 embeddings
INFO:rag_factory.strategies.indexing.vector_embedding:Embedding generation complete: created 2 embeddings
INFO:rag_factory.strategies.indexing.vector_embedding:Adding embeddings to chunks...
INFO:rag_factory.strategies.indexing.vector_embedding:Storing 2 chunks to database...
DEBUG:rag_factory.services.database.postgres:Storing chunk eiffel_doc_0: embedding dims=384, first 3 values=[0.054607685655355453, 0.10627976804971695, 0.011374696157872677]
DEBUG:rag_factory.services.database.postgres:Storing chunk python_doc_0: embedding dims=384, first 3 values=[0.038421668112277985, 0.10197843611240387, 0.02736414037644863]
DEBUG:rag_factory.services.database.postgres:Stored 2 chunks
INFO:rag_factory.strategies.indexing.vector_embedding:Database storage complete
INFO:rag_factory.strategies.indexing.vector_embedding:Indexing complete - Documents: 2, Chunks: 2, Embedding dimension: 384
DEBUG:rag_factory.services.database.postgres:Searching with embedding dims=384, first 3 values=[0.02967029996216297, 0.12840107083320618, 0.025954896584153175]
DEBUG:rag_factory.services.database.postgres:Found 2 chunks with embeddings in database
DEBUG:rag_factory.services.database.postgres:Found 1 similar chunks
INFO:rag_factory.services.llm.service:Sending request with 65 prompt tokens
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0a49e73e-003d-402c-b203-fa116d347f89', 'json_data': {'messages': [{'role': 'system', 'content': 'Answer the question based only on the provided context. Be concise.'}, {'role': 'user', 'content': 'Context:\nPython is a high-level programming language created by Guido van Rossum in 1991. It is known for its simplicity and readability.\n\nQuestion: When was the Eiffel Tower built?'}], 'model': 'phi-4-mini-instruct', 'max_tokens': 1000, 'temperature': 0.7}}
DEBUG:openai._base_client:Sending HTTP Request: POST http://192.168.56.1:1234/v1/chat/completions
DEBUG:httpcore.connection:connect_tcp.started host='192.168.56.1' port=1234 local_address=None timeout=5.0 socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7eff99bea6f0>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'857'), (b'ETag', b'W/"359-Y4UUgrTf0wJqkuhoG6lUb62xut4"'), (b'Date', b'Fri, 19 Dec 2025 02:23:07 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
INFO:httpx:HTTP Request: POST http://192.168.56.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST http://192.168.56.1:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'application/json; charset=utf-8', 'content-length': '857', 'etag': 'W/"359-Y4UUgrTf0wJqkuhoG6lUb62xut4"', 'date': 'Fri, 19 Dec 2025 02:23:07 GMT', 'connection': 'keep-alive', 'keep-alive': 'timeout=5'})
DEBUG:openai._base_client:request_id: None
INFO:rag_factory.services.llm.service:Completed in 3.96s, 112 tokens, $0.000000
------------------------------ Captured log call -------------------------------
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:55 Starting VectorEmbeddingIndexing.process with 2 documents
DEBUG    rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:61 Configuration: batch_size=32, chunk_size=200, overlap=50
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:67 Using embedding service: AsyncEmbeddingWrapper
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:70 Starting document chunking...
DEBUG    rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:137 _chunk_documents called with 2 documents, chunk_size=200, overlap=50
DEBUG    rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:146 Processing document 1/2: doc_id='eiffel_doc', text_length=112
DEBUG    rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:158 Document eiffel_doc fits in single chunk (length=112 <= 200)
DEBUG    rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:146 Processing document 2/2: doc_id='python_doc', text_length=128
DEBUG    rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:158 Document python_doc fits in single chunk (length=128 <= 200)
DEBUG    rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:220 _chunk_documents complete: total 2 chunks created
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:72 Document chunking complete: created 2 chunks from 2 documents
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:85 Starting embedding generation for 2 chunks in batches of 32...
DEBUG    rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:94 Processing batch 1/1 with 2 texts
DEBUG    rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:100 Batch 1/1 complete: generated 2 embeddings
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:102 Embedding generation complete: created 2 embeddings
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:105 Adding embeddings to chunks...
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:109 Storing 2 chunks to database...
DEBUG    rag_factory.services.database.postgres:postgres.py:251 Storing chunk eiffel_doc_0: embedding dims=384, first 3 values=[0.054607685655355453, 0.10627976804971695, 0.011374696157872677]
DEBUG    rag_factory.services.database.postgres:postgres.py:251 Storing chunk python_doc_0: embedding dims=384, first 3 values=[0.038421668112277985, 0.10197843611240387, 0.02736414037644863]
DEBUG    rag_factory.services.database.postgres:postgres.py:269 Stored 2 chunks
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:111 Database storage complete
INFO     rag_factory.strategies.indexing.vector_embedding:vector_embedding.py:114 Indexing complete - Documents: 2, Chunks: 2, Embedding dimension: 384
DEBUG    rag_factory.services.database.postgres:postgres.py:298 Searching with embedding dims=384, first 3 values=[0.02967029996216297, 0.12840107083320618, 0.025954896584153175]
DEBUG    rag_factory.services.database.postgres:postgres.py:308 Found 2 chunks with embeddings in database
DEBUG    rag_factory.services.database.postgres:postgres.py:345 Found 1 similar chunks
INFO     rag_factory.services.llm.service:service.py:104 Sending request with 65 prompt tokens
DEBUG    openai._base_client:_base_client.py:482 Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0a49e73e-003d-402c-b203-fa116d347f89', 'json_data': {'messages': [{'role': 'system', 'content': 'Answer the question based only on the provided context. Be concise.'}, {'role': 'user', 'content': 'Context:\nPython is a high-level programming language created by Guido van Rossum in 1991. It is known for its simplicity and readability.\n\nQuestion: When was the Eiffel Tower built?'}], 'model': 'phi-4-mini-instruct', 'max_tokens': 1000, 'temperature': 0.7}}
DEBUG    openai._base_client:_base_client.py:978 Sending HTTP Request: POST http://192.168.56.1:1234/v1/chat/completions
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.started host='192.168.56.1' port=1234 local_address=None timeout=5.0 socket_options=None
DEBUG    httpcore.connection:_trace.py:47 connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7eff99bea6f0>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_headers.complete
DEBUG    httpcore.http11:_trace.py:47 send_request_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 send_request_body.complete
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'857'), (b'ETag', b'W/"359-Y4UUgrTf0wJqkuhoG6lUb62xut4"'), (b'Date', b'Fri, 19 Dec 2025 02:23:07 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
INFO     httpx:_client.py:1025 HTTP Request: POST http://192.168.56.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.started request=<Request [b'POST']>
DEBUG    httpcore.http11:_trace.py:47 receive_response_body.complete
DEBUG    httpcore.http11:_trace.py:47 response_closed.started
DEBUG    httpcore.http11:_trace.py:47 response_closed.complete
DEBUG    openai._base_client:_base_client.py:1016 HTTP Response: POST http://192.168.56.1:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'application/json; charset=utf-8', 'content-length': '857', 'etag': 'W/"359-Y4UUgrTf0wJqkuhoG6lUb62xut4"', 'date': 'Fri, 19 Dec 2025 02:23:07 GMT', 'connection': 'keep-alive', 'keep-alive': 'timeout=5'})
DEBUG    openai._base_client:_base_client.py:1024 request_id: None
INFO     rag_factory.services.llm.service:service.py:119 Completed in 3.96s, 112 tokens, $0.000000
--------------------------- Captured stderr teardown ---------------------------
INFO:tests.integration_real.conftest:[FIXTURE] Test completed, starting cleanup
INFO:tests.integration_real.conftest:[FIXTURE] Pool acquired for cleanup
INFO:tests.integration_real.conftest:[FIXTURE] Table dropped successfully
INFO:tests.integration_real.conftest:[FIXTURE] Closing service
INFO:rag_factory.services.database.postgres:Closed PostgreSQL async connection pool
DEBUG:rag_factory.services.database.postgres:Cleared DatabaseContext cache
INFO:tests.integration_real.conftest:[FIXTURE] Service closed
---------------------------- Captured log teardown -----------------------------
INFO     tests.integration_real.conftest:conftest.py:196 [FIXTURE] Test completed, starting cleanup
INFO     tests.integration_real.conftest:conftest.py:199 [FIXTURE] Pool acquired for cleanup
INFO     tests.integration_real.conftest:conftest.py:202 [FIXTURE] Table dropped successfully
INFO     tests.integration_real.conftest:conftest.py:207 [FIXTURE] Closing service
INFO     rag_factory.services.database.postgres:postgres.py:610 Closed PostgreSQL async connection pool
DEBUG    rag_factory.services.database.postgres:postgres.py:619 Cleared DatabaseContext cache
INFO     tests.integration_real.conftest:conftest.py:209 [FIXTURE] Service closed
=============================== warnings summary ===============================
rag_factory/services/llm/config.py:8
  /mnt/MCPProyects/ragTools/rag_factory/services/llm/config.py:8: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class LLMServiceConfig(BaseModel):

rag_factory/database/config.py:12
  /mnt/MCPProyects/ragTools/rag_factory/database/config.py:12: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class DatabaseConfig(BaseSettings):

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_______________ coverage: platform linux, python 3.12.3-final-0 ________________

Name                                                               Stmts   Miss  Cover   Missing
------------------------------------------------------------------------------------------------
rag_factory/__init__.py                                                3      0   100%
rag_factory/__version__.py                                             1      0   100%
rag_factory/cli/__init__.py                                            2      0   100%
rag_factory/cli/commands/__init__.py                                   1      0   100%
rag_factory/cli/commands/benchmark.py                                 96     81    16%   40-53, 64-84, 148-263
rag_factory/cli/commands/check_consistency.py                         43     33    23%   64-120
rag_factory/cli/commands/config.py                                    78     68    13%   26-64, 102-188
rag_factory/cli/commands/index.py                                     65     52    20%   29-39, 93-185
rag_factory/cli/commands/query.py                                     47     36    23%   73-149
rag_factory/cli/commands/repl.py                                     152    130    14%   29-56, 63-108, 112-120, 124-133, 137-155, 159-172, 176-186, 190-207, 211-220, 253-260
rag_factory/cli/commands/strategies.py                                45     36    20%   48-120
rag_factory/cli/commands/validate_e2e.py                              91     69    24%   37-38, 42-142, 145-156
rag_factory/cli/commands/validate_pipeline.py                         70     57    19%   57-156
rag_factory/cli/formatters/__init__.py                                 4      0   100%
rag_factory/cli/formatters/consistency.py                             61     55    10%   21-69, 79-103, 108-138
rag_factory/cli/formatters/output.py                                  20      9    55%   23-24, 43-44, 63-64, 74, 79, 84
rag_factory/cli/formatters/results.py                                 71     62    13%   28-55, 72-99, 116-144, 157-181
rag_factory/cli/formatters/validation.py                              63     53    16%   25-43, 48-56, 65-77, 86-98, 103-129, 134-137
rag_factory/cli/main.py                                               31     10    68%   25-28, 48-49, 76-78, 82
rag_factory/cli/utils/__init__.py                                      3      0   100%
rag_factory/cli/utils/progress.py                                     14      8    43%   34-47, 73-76
rag_factory/cli/utils/validation.py                                   53     42    21%   30-41, 57-77, 93-118, 135
rag_factory/config/__init__.py                                         4      0   100%
rag_factory/config/env_resolver.py                                    52     37    29%   47-54, 70-90, 110, 129-147
rag_factory/config/schemas/__init__.py                                 2      0   100%
rag_factory/config/schemas/version.py                                 10      6    40%   29-38
rag_factory/config/strategy_loader.py                                 54     36    33%   32-41, 45-60, 75-89, 101-104
rag_factory/config/strategy_pair_manager.py                          105     83    21%   40-72, 92-132, 148-198, 210-215, 229, 259-282
rag_factory/config/validator.py                                       93     76    18%   27-30, 34-39, 53-58, 62-74, 111-132, 177-198, 215-235, 247-249, 271-295, 333-354
rag_factory/core/__init__.py                                           4      4     0%   7-23
rag_factory/core/capabilities.py                                      61     16    74%   129, 155, 173-174, 239-253
rag_factory/core/indexing_interface.py                                54     28    48%   144, 164, 208, 260, 272, 292-313, 341-364
rag_factory/core/pipeline.py                                          49     35    29%   22-24, 33-40, 55-77, 95-96, 105-108, 117-120, 137-146
rag_factory/core/retrieval_interface.py                               38     13    66%   138, 158, 196, 247, 259, 285-309
rag_factory/database/__init__.py                                       4      0   100%
rag_factory/database/config.py                                        17      0   100%
rag_factory/database/connection.py                                    80     54    32%   40-48, 61-86, 96, 107, 126-137, 150-157, 165-170, 178-183, 196-197, 209-214, 218, 222
rag_factory/database/env_validator.py                                 38     38     0%   7-139
rag_factory/database/models.py                                        85     32    62%   31-34, 37-45, 48-54, 66-69, 72-77, 80-85, 181, 323-325
rag_factory/database/vector_indexing.py                               38     38     0%   7-114
rag_factory/evaluation/__init__.py                                     4      4     0%   29-41
rag_factory/evaluation/analysis/__init__.py                            3      3     0%   8-11
rag_factory/evaluation/analysis/comparison.py                        135    135     0%   8-323
rag_factory/evaluation/analysis/statistics.py                         81     81     0%   8-320
rag_factory/evaluation/benchmarks/__init__.py                          3      3     0%   8-14
rag_factory/evaluation/benchmarks/config.py                           25     25     0%   7-64
rag_factory/evaluation/benchmarks/runner.py                          155    155     0%   8-423
rag_factory/evaluation/datasets/__init__.py                            3      3     0%   8-14
rag_factory/evaluation/datasets/loader.py                             88     88     0%   8-281
rag_factory/evaluation/datasets/schema.py                             52     52     0%   8-160
rag_factory/evaluation/datasets/statistics.py                         65     65     0%   8-226
rag_factory/evaluation/exporters/__init__.py                           4      4     0%   8-12
rag_factory/evaluation/exporters/csv_exporter.py                      52     52     0%   3-124
rag_factory/evaluation/exporters/html_exporter.py                     47     47     0%   3-293
rag_factory/evaluation/exporters/json_exporter.py                     19     19     0%   3-68
rag_factory/evaluation/metrics/__init__.py                             3      3     0%   12-25
rag_factory/evaluation/metrics/base.py                                31     31     0%   8-115
rag_factory/evaluation/metrics/cost.py                                65     65     0%   8-313
rag_factory/evaluation/metrics/performance.py                         42     42     0%   8-202
rag_factory/evaluation/metrics/quality.py                             84     84     0%   8-377
rag_factory/evaluation/metrics/retrieval.py                           77     77     0%   9-401
rag_factory/exceptions.py                                             13      0   100%
rag_factory/factory.py                                               187    137    27%   109-116, 144, 165-170, 202-230, 256-270, 300-324, 347-348, 365, 384, 401, 418, 435, 451-460, 480-502, 528-545, 568-611, 644-699, 703-715
rag_factory/legacy_config.py                                         155    155     0%   22-542
rag_factory/models/__init__.py                                         6      6     0%   3-14
rag_factory/models/embedding/__init__.py                               4      4     0%   3-12
rag_factory/models/embedding/loader.py                               179    179     0%   3-367
rag_factory/models/embedding/models.py                                47     47     0%   3-106
rag_factory/models/embedding/registry.py                             128    128     0%   3-282
rag_factory/models/evaluation/__init__.py                              3      3     0%   3-6
rag_factory/models/evaluation/ab_testing.py                           98     98     0%   3-281
rag_factory/models/evaluation/models.py                               29     29     0%   3-74
rag_factory/observability/__init__.py                                  3      3     0%   3-10
rag_factory/observability/integrations/__init__.py                     1      1     0%   3
rag_factory/observability/integrations/prometheus.py                  47     47     0%   3-229
rag_factory/observability/logging/__init__.py                          2      2     0%   3-5
rag_factory/observability/logging/config.py                           16     16     0%   3-37
rag_factory/observability/logging/filters.py                          41     41     0%   3-103
rag_factory/observability/logging/logger.py                           86     86     0%   3-396
rag_factory/observability/metrics/__init__.py                          2      2     0%   3-9
rag_factory/observability/metrics/collector.py                       143    143     0%   3-459
rag_factory/observability/metrics/cost.py                             39     39     0%   3-221
rag_factory/observability/metrics/performance.py                      64     64     0%   3-196
rag_factory/observability/monitoring/__init__.py                       1      1     0%   3
rag_factory/observability/monitoring/api.py                          101    101     0%   3-328
rag_factory/pipeline.py                                              156    156     0%   34-507
rag_factory/registry/__init__.py                                       4      0   100%
rag_factory/registry/exceptions.py                                     6      0   100%
rag_factory/registry/service_factory.py                               62     49    21%   45-52, 60-63, 68-71, 76-78, 87-105, 118-147, 158-197
rag_factory/registry/service_registry.py                              91     71    22%   43-51, 55-85, 103-156, 164, 178-203, 210-223, 227, 231-232
rag_factory/repositories/__init__.py                                   5      0   100%
rag_factory/repositories/base.py                                      42     21    50%   38, 53, 70, 87, 103, 114-118, 125, 136-139, 159-164
rag_factory/repositories/chunk.py                                    206    180    13%   40-45, 68-76, 90-95, 120-133, 155-171, 187-202, 221-231, 250-261, 287-328, 355-396, 422-474, 489-499, 513-521, 537-543, 557-563, 577-587, 604-626, 639-670
rag_factory/repositories/document.py                                  93     74    20%   41-46, 63-68, 98-118, 140-156, 172-187, 206, 223-233, 247-255, 272-279, 290-293, 307-312
rag_factory/repositories/exceptions.py                                18      7    61%   34-36, 56-59
rag_factory/services/__init__.py                                       4      0   100%
rag_factory/services/api/__init__.py                                   4      4     0%   7-11
rag_factory/services/api/anthropic.py                                 27     27     0%   7-141
rag_factory/services/api/cohere.py                                    30     30     0%   7-113
rag_factory/services/api/openai.py                                    46     46     0%   7-235
rag_factory/services/consistency.py                                   32     24    25%   80-113, 144-170
rag_factory/services/database/__init__.py                              5      0   100%
rag_factory/services/database/database_context.py                    181    159    12%   58-64, 88-106, 123, 140-150, 179-200, 223-240, 256-266, 305-330, 338-339, 343-406, 414-415, 430-510, 516-517, 521-546
rag_factory/services/database/migration_validator.py                  85     63    26%   29-30, 61-69, 81-88, 108-120, 137-141, 152-181, 192-204, 215-222, 234, 246-256, 268-271
rag_factory/services/database/neo4j.py                                54     36    33%   14, 60-71, 79-84, 103-114, 134-150, 172-181, 188-191, 195, 199
rag_factory/services/database/postgres.py                            195     83    57%   17-18, 26-27, 85, 91-103, 187-189, 215, 239, 241, 245-246, 327-330, 360-375, 390-414, 425-440, 472-490, 504-527, 532, 543, 580-598, 614-616, 623, 627
rag_factory/services/dependencies.py                                  41     12    71%   99, 101, 103, 105, 107, 134-140, 179-180
rag_factory/services/embedding/__init__.py                             4      0   100%
rag_factory/services/embedding/base.py                                31      6    81%   44, 59, 68, 77, 86, 98
rag_factory/services/embedding/cache.py                               43     33    23%   26-32, 43-58, 67-76, 80-82, 90-94
rag_factory/services/embedding/config.py                              29     13    55%   37-43, 50-68, 80
rag_factory/services/embedding/providers/__init__.py                   5      0   100%
rag_factory/services/embedding/providers/cohere.py                    53     36    32%   10-20, 55-75, 90-117, 125, 133, 141, 152-153
rag_factory/services/embedding/providers/local.py                     43     27    37%   8-9, 46-67, 83-109, 117, 125, 133, 146
rag_factory/services/embedding/providers/onnx_local.py               114     37    68%   18-19, 90, 114, 139-147, 158, 170-175, 194, 248-250, 273, 289-308, 327, 335, 348
rag_factory/services/embedding/providers/openai.py                    51     34    33%   10-20, 53-72, 87-108, 116, 124, 132, 143-144
rag_factory/services/embedding/rate_limiter.py                        20      3    85%   32, 50-51
rag_factory/services/embedding/service.py                            100     83    17%   50-62, 79-93, 117-198, 220-222, 233-239, 248-266, 273-275
rag_factory/services/interfaces.py                                    35      0   100%
rag_factory/services/llm/__init__.py                                   5      0   100%
rag_factory/services/llm/base.py                                      43      0   100%
rag_factory/services/llm/config.py                                    23      7    70%   46-48, 51-53, 57
rag_factory/services/llm/prompt_template.py                           43     24    44%   47-82, 97-100, 111-115
rag_factory/services/llm/providers/__init__.py                        12     10    17%   10-22
rag_factory/services/llm/providers/anthropic.py                       57     42    26%   44-50, 64-97, 120-147, 171-172, 184-189, 197, 205
rag_factory/services/llm/providers/ollama.py                          55     42    24%   22-24, 37-67, 93-122, 142-143, 157, 165, 174, 185-196
rag_factory/services/llm/providers/openai.py                          57     20    65%   56, 62, 101-105, 133-158, 189-194, 202, 210
rag_factory/services/llm/service.py                                   66     25    62%   68, 94, 127-129, 154-175, 186, 194-198, 215-216
rag_factory/services/llm/token_counter.py                             27      4    85%   56-57, 71-72
rag_factory/services/local/__init__.py                                 2      2     0%   7-9
rag_factory/services/local/reranker.py                                32     32     0%   7-116
rag_factory/services/onnx/__init__.py                                  2      2     0%   7-9
rag_factory/services/onnx/embedding.py                                29     29     0%   7-129
rag_factory/services/utils/__init__.py                                 2      0   100%
rag_factory/services/utils/model_converter.py                        104    104     0%   8-291
rag_factory/services/utils/onnx_utils.py                             133     42    68%   22-23, 37, 77-84, 96-97, 117-123, 142-146, 175, 189-190, 194, 198-201, 216-218, 258, 265-268, 361-362, 376, 392-404
rag_factory/services/utils/reranker_selector.py                       68     68     0%   8-249
rag_factory/strategies/__init__.py                                     2      0   100%
rag_factory/strategies/agentic/__init__.py                             7      7     0%   8-20
rag_factory/strategies/agentic/agent.py                              151    151     0%   8-420
rag_factory/strategies/agentic/config.py                              12     12     0%   5-31
rag_factory/strategies/agentic/frameworks/__init__.py                  1      1     0%   9
rag_factory/strategies/agentic/query_analyzer.py                      89     89     0%   8-278
rag_factory/strategies/agentic/strategy.py                            87     87     0%   8-284
rag_factory/strategies/agentic/tool_implementations.py               135    135     0%   11-511
rag_factory/strategies/agentic/tools.py                               40     40     0%   8-142
rag_factory/strategies/base.py                                        55     10    82%   81, 83, 85, 169-178
rag_factory/strategies/chunking/__init__.py                           15     15     0%   28-67
rag_factory/strategies/chunking/base.py                               91     91     0%   3-256
rag_factory/strategies/chunking/docling_chunker.py                    45     45     0%   13-188
rag_factory/strategies/chunking/fixed_size_chunker.py                 74     74     0%   3-204
rag_factory/strategies/chunking/hybrid_chunker.py                     70     70     0%   3-192
rag_factory/strategies/chunking/semantic_chunker.py                  195    195     0%   3-534
rag_factory/strategies/chunking/structural_chunker.py                147    147     0%   3-425
rag_factory/strategies/chunking/utils.py                              89     89     0%   3-264
rag_factory/strategies/contextual/__init__.py                          7      7     0%   8-19
rag_factory/strategies/contextual/batch_processor.py                  91     91     0%   8-266
rag_factory/strategies/contextual/config.py                           46     46     0%   8-195
rag_factory/strategies/contextual/context_generator.py                84     84     0%   8-234
rag_factory/strategies/contextual/cost_tracker.py                     32     32     0%   8-126
rag_factory/strategies/contextual/prompts.py                          25     25     0%   8-119
rag_factory/strategies/contextual/storage.py                          44     44     0%   8-118
rag_factory/strategies/contextual/strategy.py                         78     78     0%   8-298
rag_factory/strategies/fine_tuned/__init__.py                          4      4     0%   1-5
rag_factory/strategies/fine_tuned/ab_testing.py                       78     78     0%   1-134
rag_factory/strategies/fine_tuned/config.py                           11     11     0%   1-30
rag_factory/strategies/fine_tuned/custom_loader.py                    40     40     0%   1-88
rag_factory/strategies/fine_tuned/model_registry.py                  126    126     0%   1-303
rag_factory/strategies/hierarchical/__init__.py                        5      5     0%   8-21
rag_factory/strategies/hierarchical/hierarchy_builder.py              79     79     0%   8-335
rag_factory/strategies/hierarchical/models.py                         58     58     0%   8-137
rag_factory/strategies/hierarchical/parent_retriever.py               89     89     0%   8-278
rag_factory/strategies/hierarchical/strategy.py                       76     76     0%   9-301
rag_factory/strategies/indexing/__init__.py                            6      0   100%
rag_factory/strategies/indexing/context_aware.py                     108     88    19%   31, 43, 63-134, 155-156, 160-170, 182-188, 192-198, 208-237, 241-242
rag_factory/strategies/indexing/hierarchical.py                       90     73    19%   50, 63, 88-134, 156-185, 201-236, 253-278, 292-293
rag_factory/strategies/indexing/in_memory.py                          41     25    39%   54, 65, 93-121, 143, 160, 173, 194-200
rag_factory/strategies/indexing/keyword_indexing.py                   42     42     0%   8-157
rag_factory/strategies/indexing/knowledge_graph_indexing.py           46     33    28%   18, 27, 48-89, 108-129
rag_factory/strategies/indexing/vector_embedding.py                   98     34    65%   64-65, 76-77, 149-150, 167-218
rag_factory/strategies/knowledge_graph/__init__.py                     4      4     0%   8-19
rag_factory/strategies/knowledge_graph/config.py                      21     21     0%   8-129
rag_factory/strategies/knowledge_graph/entity_extractor.py            70     70     0%   7-205
rag_factory/strategies/knowledge_graph/graph_store.py                 31     31     0%   8-124
rag_factory/strategies/knowledge_graph/hybrid_retriever.py            56     56     0%   8-164
rag_factory/strategies/knowledge_graph/memory_graph_store.py          97     97     0%   8-196
rag_factory/strategies/knowledge_graph/models.py                      53     53     0%   8-122
rag_factory/strategies/knowledge_graph/relationship_extractor.py      62     62     0%   8-189
rag_factory/strategies/knowledge_graph/strategy.py                    80     80     0%   8-256
rag_factory/strategies/late_chunking/__init__.py                       6      6     0%   8-21
rag_factory/strategies/late_chunking/coherence_analyzer.py            27     27     0%   7-96
rag_factory/strategies/late_chunking/document_embedder.py            104    104     0%   8-318
rag_factory/strategies/late_chunking/embedding_chunker.py            117    117     0%   7-300
rag_factory/strategies/late_chunking/models.py                        62     62     0%   8-100
rag_factory/strategies/late_chunking/strategy.py                      70     70     0%   8-215
rag_factory/strategies/multi_query/__init__.py                         7      7     0%   8-19
rag_factory/strategies/multi_query/config.py                          36     36     0%   3-138
rag_factory/strategies/multi_query/deduplicator.py                    87     87     0%   3-197
rag_factory/strategies/multi_query/parallel_executor.py               52     52     0%   3-163
rag_factory/strategies/multi_query/prompts.py                         10     10     0%   3-84
rag_factory/strategies/multi_query/ranker.py                          80     80     0%   3-201
rag_factory/strategies/multi_query/strategy.py                        68     68     0%   3-184
rag_factory/strategies/multi_query/variant_generator.py               62     62     0%   3-187
rag_factory/strategies/query_expansion/__init__.py                     8      0   100%
rag_factory/strategies/query_expansion/base.py                        66     10    85%   90, 103, 114-118, 130-133
rag_factory/strategies/query_expansion/cache.py                       52     40    23%   20-22, 38-55, 64-66, 70-73, 81-97, 105-112
rag_factory/strategies/query_expansion/expander_service.py            92     71    23%   36-40, 55-58, 75-148, 159-180, 204-215, 233-234, 242-246, 254, 265-273, 283-284
rag_factory/strategies/query_expansion/hyde_expander.py               19     12    37%   25-27, 41-65
rag_factory/strategies/query_expansion/llm_expander.py                25     16    36%   23-25, 39-71, 99
rag_factory/strategies/query_expansion/metrics.py                     66     30    55%   44-45, 53-57, 76-109, 130, 134, 146-152, 160
rag_factory/strategies/query_expansion/prompts.py                     13      7    46%   16, 28-56, 68-107
rag_factory/strategies/reranking/__init__.py                           7      7     0%   8-21
rag_factory/strategies/reranking/base.py                              68     68     0%   8-130
rag_factory/strategies/reranking/bge_reranker.py                      51     51     0%   8-151
rag_factory/strategies/reranking/cache.py                             48     48     0%   8-138
rag_factory/strategies/reranking/cohere_reranker.py                   34     34     0%   8-119
rag_factory/strategies/reranking/cosine_reranker.py                   62     62     0%   8-249
rag_factory/strategies/reranking/cross_encoder_reranker.py            40     40     0%   8-124
rag_factory/strategies/reranking/metrics.py                           67     67     0%   8-264
rag_factory/strategies/reranking/reranker_service.py                 115    115     0%   8-310
rag_factory/strategies/retrieval/__init__.py                           6      0   100%
rag_factory/strategies/retrieval/keyword_retriever.py                 56     41    27%   41, 52, 71-113, 126-133, 153-172
rag_factory/strategies/retrieval/knowledge_graph_retriever.py         22      9    59%   19, 27, 48-70
rag_factory/strategies/retrieval/multi_query_retriever.py             22      9    59%   19, 26, 44-66
rag_factory/strategies/retrieval/query_expansion_retriever.py         24      9    62%   20, 27, 45-67
rag_factory/strategies/retrieval/semantic_retriever.py                23      2    91%   23, 77
rag_factory/strategies/self_reflective/__init__.py                     6      6     0%   3-16
rag_factory/strategies/self_reflective/config.py                      20     20     0%   3-42
rag_factory/strategies/self_reflective/grader.py                      80     80     0%   3-266
rag_factory/strategies/self_reflective/models.py                      48     48     0%   3-109
rag_factory/strategies/self_reflective/refiner.py                     69     69     0%   3-228
rag_factory/strategies/self_reflective/strategy.py                    92     92     0%   3-269
rag_factory/utils/__init__.py                                          3      0   100%
rag_factory/utils/token_counter.py                                    45     30    33%   36-40, 52-62, 74-82, 86, 90, 105-106, 124-135, 153-158
rag_factory/utils/tokenization.py                                     95     73    23%   36-40, 53-64, 87-114, 126-129, 141-144, 156, 175-190, 209-228, 241-243, 251-252, 270-271, 292-293, 314-315
------------------------------------------------------------------------------------------------
TOTAL                                                              12325  10388    16%
=========================== short test summary info ============================
FAILED tests/integration_real/test_end_to_end_real.py::test_full_rag_pipeline - AssertionError: assert '1889' in 'The given context does not contain information about when the Eiffel Tower was constructed; it only provides details on Python, which has no relation to historical events or structures like the Eiffel Tower. The answer can be found independently of this text but is unrelated to its content regarding programming languages.'
 +  where 'The given context does not contain information about when the Eiffel Tower was constructed; it only provides details on Python, which has no relation to historical events or structures like the Eiffel Tower. The answer can be found independently of this text but is unrelated to its content regarding programming languages.' = LLMResponse(content='The given context does not contain information about when the Eiffel Tower was constructed; it only provides details on Python, which has no relation to historical events or structures like the Eiffel Tower. The answer can be found independently of this text but is unrelated to its content regarding programming languages.', model='phi-4-mini-instruct', provider='openai', prompt_tokens=56, completion_tokens=56, total_tokens=112, cost=0.0, latency=3.963956117630005, metadata={'finish_reason': 'stop'}).content
=================== 1 failed, 1 passed, 2 warnings in 30.69s ===================
--- Logging error ---
Traceback (most recent call last):
  File "/usr/lib/python3.12/logging/__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
ValueError: I/O operation on closed file.
Call stack:
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/openai/_base_client.py", line 815, in __del__
    self.close()
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpx/_client.py", line 1270, in close
    self._transport.close()
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 262, in close
    self._pool.close()
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 353, in close
    self._close_connections(closing_connections)
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 345, in _close_connections
    connection.close()
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py", line 172, in close
    with Trace("close", logger, None, {}):
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_trace.py", line 52, in __enter__
    self.trace(f"{self.name}.started", info)
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_trace.py", line 47, in trace
    self.logger.debug(message)
Message: 'close.started'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/usr/lib/python3.12/logging/__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
ValueError: I/O operation on closed file.
Call stack:
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/openai/_base_client.py", line 815, in __del__
    self.close()
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpx/_client.py", line 1270, in close
    self._transport.close()
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 262, in close
    self._pool.close()
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 353, in close
    self._close_connections(closing_connections)
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 345, in _close_connections
    connection.close()
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py", line 172, in close
    with Trace("close", logger, None, {}):
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_trace.py", line 64, in __exit__
    self.trace(f"{self.name}.complete", info)
  File "/mnt/MCPProyects/ragTools/venv/lib/python3.12/site-packages/httpcore/_trace.py", line 47, in trace
    self.logger.debug(message)
Message: 'close.complete'
Arguments: ()

========================================
Test run completed at 2025-12-18 23:23:18
========================================
